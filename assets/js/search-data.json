{
  
    
        "post0": {
            "title": "Paper Reproduction - "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue" in python",
            "content": "Introduction . This is my replication of the empirical results, tables, and figures produced in a paper by Dr. Ergete Ferede, published by the University of Calgary school of public policy in Volume 11:24, September 2018. . The original paper is here: https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf . I chose this paper to reproduce for two reasons. The first is pragmatic; the data it uses is all publicly available, so I actually can. The second is that it describes a topic of importance in the province of Alberta, where I live. . You can read the details of what the paper sets out to show in the paper itself, but in brief the idea is to show that provincial government spending increases in the year following an increase in non-renewable resource revenue, but it does not decrease accordingly in the year following declines in the same revenue source. This has a ratcheting effect on public finance that is a contributor to the &quot;royalty rollercoaster&quot; that is Alberta&#39;s public finance. . In the following sections I&#39;ll go through the code necessary to extract and transform the data set used in the paper, as well as reproduce its key empirical results. Since most economists don&#39;t use python, and they make up a key part of my intended audience for this, I&#39;ll be adding comments to my code that explicitly describe what some of the functions and methods I&#39;m calling do. . I&#39;m including all of the code necessary to produce this reproduction, since that&#39;s a big part of why I&#39;m doing this exercise, but if you&#39;re just interested in seeing how my reproduced results compare to the original paper you can skip all the code blocks. You can find the code for this notebook on my github . A surprising result of this reproduction is that I&#39;ve identified a single data point error in the original paper that negates its results. Read on to find out what the error was and the impact it had on the results. . Setup and data acquisition . This section of the code loads required modules, downloads the required data sets, and reads them into DataFrames. . import datetime as dt from itertools import chain from pathlib import Path import altair as alt from arch.unitroot import DFGLS, ADF, PhillipsPerron from IPython.display import Image import pandas as pd import pandas_datareader as pdr import requests import seaborn as sns import stats_can import statsmodels from statsmodels.tsa.api import VAR %matplotlib inline alt.renderers.enable(&quot;jupyterlab&quot;); . We start by loading the required libraries that will be used to support the analysis. For reference here are links to the libraries that are being used: . Pathlib | datetime | requests | pandas | pandas_datareader | numpy | stats_can | altair | seaborn | arch | statsmodels | matplotlib | . Historical budget data . Functions in this section are concerned with acquiring historical Alberta budget data and reading it into a DataFrame . def download_budget_data() -&gt; Path: &quot;&quot;&quot;Download the excel file for the analysis from the policy school page. Note the readme sheet on the first file. Credit to Kneebone and Wilkins for assembling it, and policy school for hosting it. Originally used this URL, but found it was missing some later heritage contributions. After discussion with Dr. Kneebone an updated set has been provided https://www.policyschool.ca/wp-content/uploads/2019/01/Provincial-Government-Budget-Data-January-2019FINAL-USE.xlsx Returns - pathlib.Path A path object with the location and name of the data &quot;&quot;&quot; print(&#39;Downloading data set&#39;) url = &#39;https://www.policyschool.ca/wp-content/uploads/2019/03/Provincial-Government-Budget-Data-March-2019.xlsx&#39; # send a request to the url for the file response = requests.get( url, stream=True, headers={&#39;user-agent&#39;: None} ) # create a path object for the file in the data folder above # where this notebook is saved with the file named # budget.xlsx for easy later access. fname = Path(&#39;.&#39;).joinpath(&#39;data&#39;).joinpath(&#39;budgets.xlsx&#39;) # write the response from the request to the file in the path specified above with open (fname, &#39;wb&#39;) as outfile: for chunk in response.iter_content(chunk_size=512): if chunk: # filter out keep-alive new chunks outfile.write(chunk) # Return the location of the file so we can load it later easily return fname def get_budget_file(force_update: bool=False) -&gt; Path: &quot;&quot;&quot;Get the budget file, downloading if required. Parameters - force_update: bool Download the data file even if you already have it Returns - pathlib.Path A path object with the location and name of the data &quot;&quot;&quot; # This is where we&#39;re expecting the file to be saved if it exists fname = Path(&#39;.&#39;).joinpath(&#39;data&#39;).joinpath(&#39;budgets.xlsx&#39;) if not fname.exists() or force_update: download_budget_data() return fname def get_date_index(df: pd.DataFrame) -&gt; pd.DatetimeIndex: &quot;&quot;&quot;Helper function to turn budget year strings into datetimes. The Fiscal year columns span across years, e.g. 1965-66. In order to use all the date indexed functionality I want to convert them into an actual datetime format. This function accomplishes that Parameters - df: pd.DataFrame The budget dataframe with the fiscal year style columns Returns - pd.DatetimeIndex A datetime index showing January 1 of the beginning of each fiscal year for each period. &quot;&quot;&quot; date_index = pd.to_datetime( df .assign(year=lambda df: df[&#39;budget_yr&#39;].str[0:4].astype(int)) .assign(month=1) .assign(day=1) [[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]] ) return date_index def read_ab_budget() -&gt; pd.DataFrame: &quot;&quot;&quot;Read Alberta budget data. Downloads the data if necessary, reads it in and gives the variables easier to work with names Returns - pd.DataFrame Alberta&#39;s revenue and expenditure tables &quot;&quot;&quot; # Get the budget file, download if necessary using functions # defined above fname = get_budget_file() df = ( pd.read_excel( fname, sheet_name=&#39;Alberta&#39;, # column titles are spaced over 3 rows header=3, # first column of data is B index_col=1, # there&#39;s a big footnote at the bottom we want to skip skipfooter=21 ) # Because of the merged cells we get an empty first row .loc[lambda x: x.index.notnull()] # Not sure where the empty first column comes from but drop it .drop(columns=&#39;Unnamed: 0&#39;) .reset_index() .rename(columns={ &#39;index&#39;: &#39;budget_yr&#39;, &#39;Personal Income Tax&#39;: &#39;personal_income_tax&#39;, &#39;Corporation Income Tax&#39;: &#39;corporate_income_tax&#39;, &#39;Retail Sales Tax&#39;: &#39;retail_sales_tax&#39;, &#39;Federal Cash Transfers&#39;: &#39;federal_cash_transfers&#39;, &#39;Natural Resource Revenue&#39;: &#39;natural_resource_revenue&#39;, &#39;Other Own-Source Revenue&#39;: &#39;other_own_source_revenue&#39;, &#39;Total Revenue&#39;: &#39;total_revenue&#39;, &#39;Health&#39;: &#39;health_exp&#39;, &#39;Social Services&#39;: &#39;social_services_exp&#39;, &#39;Education&#39;: &#39;education_exp&#39;, &#39;Other Program Expenditures&#39;: &#39;other_program_exp&#39;, &#39;Total Program Expenditures&#39;: &#39;total_prog_exp&#39;, &#39;Debt Service&#39;: &#39;debt_service&#39;, &#39;Total Expenditures&#39;: &#39;total_exp&#39;, &#39;Unnamed: 16&#39;: &#39;annual_deficit&#39; }) # Turn the fiscal year string into a datetime object .assign(budget_dt=lambda df: get_date_index(df)) .set_index(&#39;budget_dt&#39;) ) return df def read_heritage() -&gt; pd.DataFrame: &quot;&quot;&quot;Read deposits to the heritage trust fund from a separate table. The paper nets out contributions to the heritage trust fund when they are made, so we have to read them in to be able to net them out of resource revenue. They&#39;re stored in the same sheet of the workbook, just down below the big table we read in with the function above. &quot;&quot;&quot; fname = get_budget_file() df = ( pd.read_excel( fname, sheet_name=&#39;Alberta&#39;, # Have to manually specify column names because of # how the table is laid out header=None, usecols=&#39;D:G&#39;, names=[&#39;budget_yr&#39;, &#39;resource_allocation&#39;, &#39;deposits&#39;, &#39;advance_edu&#39;], skiprows=71, skipfooter=1 ) # more fiddly cleaning because of how the table is set up # there&#39;s a blank row between 1986-87 and when # contributions resume in 2005-06 .loc[lambda df: ~df[&#39;budget_yr&#39;].isna()] .set_index(&#39;budget_yr&#39;) # missing entries have 0 contributions for that # category in that year .fillna(0) # The three columns are all counted the same # for the purposes of this analysis, they just have # different labels/classifications depending on the year .assign(total_heritage=lambda df: df.sum(axis=&#39;columns&#39;)) # Add a dummy variable to indicate heritage fund deposit years .assign(heritage_dummy=1) .reset_index() # convert the fiscal year column to a datetime index .assign(budget_dt=lambda df: get_date_index(df)) .drop(columns=&#39;budget_yr&#39;) .set_index(&#39;budget_dt&#39;) ) return df def clean_budget() -&gt; pd.DataFrame: &quot;&quot;&quot;Combine base budget with heritage deposits. Pull all the logic together to create one dataframe with all the fiscal data for the period of interest. Returns - pd.DataFrame The full nominal budget data set. &quot;&quot;&quot; budg = read_ab_budget() heritage = read_heritage() budg_clean = ( # Start with the budget dataframe budg # consolidate some revenue categories .assign(other_revenue=lambda df: df[[&#39;retail_sales_tax&#39;, &#39;federal_cash_transfers&#39;, &#39;other_own_source_revenue&#39;]].sum(axis=&#39;columns&#39;)) # Just keep the columns we still need .reindex(columns=[&#39;personal_income_tax&#39;, &#39;corporate_income_tax&#39;, &#39;natural_resource_revenue&#39;, &#39;other_revenue&#39;, &#39;total_prog_exp&#39;, &#39;debt_service&#39;]) # add in the heritage contributions data .merge(heritage[[&#39;total_heritage&#39;, &#39;heritage_dummy&#39;]], how=&#39;left&#39;, left_index=True, right_index=True) # Set contributions and the heritage dummy to 0 for years where there were no contributions .fillna(0) # Net out heritage contributions from natural resources revenue .assign(natural_resource_revenue_before_heritage=lambda df: df[&#39;natural_resource_revenue&#39;]) .assign(natural_resource_revenue=lambda df: df[&#39;natural_resource_revenue&#39;] - df[&#39;total_heritage&#39;]) # consolidate revenue .assign(total_revenue=lambda df: df[[&#39;personal_income_tax&#39;, &#39;corporate_income_tax&#39;, &#39;natural_resource_revenue&#39;, &#39;other_revenue&#39;]].sum(axis=&#39;columns&#39;)) # consolidate expenditure .assign(total_expenditure=lambda df: df[[&#39;total_prog_exp&#39;, &#39;debt_service&#39;]].sum(axis=&#39;columns&#39;)) # calculate the deficit .assign(deficit=lambda df: df[&#39;total_expenditure&#39;] - df[&#39;total_revenue&#39;]) # make all the budget numbers floating point .astype(&#39;float64&#39;) ) return budg_clean . Real Per Capita budget . All of the analysis in the paper is done in terms of real per-capita data. Functions in this section transform the nominal total budget numbers acquired in the previous section into real per-capita figures. . def periodic_to_budget_annual(df: pd.DataFrame, index_name: str, year_periods: int = 4) -&gt; pd.DataFrame: &quot;&quot;&quot;Take a monthly or quarterly indexed dataframe and annualize it by budget period. The inflation and population data we need to convert the budget into real per-capita figures are monthly series. We need to get the average population and price level for each fiscal year in the data set. Rolling mean indexed on January year N+1 is the March to March average population for fiscal year N Applying a date offset of -1 year and taking only January data of these rolling means gives us an average on the same basis as the budget dates. Parameters - df: pandas.DataFrame DataFrame to be piped into this function index_name: str The name of the date index year_periods: int, default 4 4 for quarterly data (population), 12 for monthly (inflation) Returns - pd.DataFrame An annualized dataframe on a fiscal year basis for comparison to annual budget figures. &quot;&quot;&quot; df = ( df .copy() .rolling(year_periods, closed=&#39;left&#39;) .mean() .reset_index() .assign(budget_dt=lambda df: df[index_name] - pd.DateOffset(years=1)) .loc[lambda x: x[&#39;budget_dt&#39;].dt.year &gt;= 1965] .loc[lambda x: x[&#39;budget_dt&#39;].dt.month == 1] .drop(columns=index_name) .set_index(&#39;budget_dt&#39;) .copy() ) return df def per_capita_data() -&gt; pd.DataFrame: &quot;&quot;&quot;Read in population data to calculate per capita estimates. Quarterly population estimates for Alberta from Statistics Canada Returns - pd.DataFrame Fiscal year annualized population estimates for Alberta over the reference period. &quot;&quot;&quot; table = &#39;17-10-0009-01&#39; df = ( stats_can.table_to_df(table, path=&#39;data&#39;) .loc[lambda x: x[&#39;GEO&#39;] == &#39;Alberta&#39;] .loc[lambda x: x[&#39;REF_DATE&#39;] &gt;= &#39;1965&#39;] .set_index(&#39;REF_DATE&#39;) [[&#39;VALUE&#39;]] .rename(columns={&#39;VALUE&#39; : &#39;population&#39;}) .pipe(periodic_to_budget_annual, &#39;REF_DATE&#39;, 4) ) return df def inflation_data() -&gt; pd.DataFrame: &quot;&quot;&quot;Read in inflation data to calculate real dollar estimates. The whole series is scaled so 2017 budget year is = 1 Returns - pd.DataFrame Fiscal year annualized inflation data for Alberta over the reference period. Normalized to 2017 = 1 &quot;&quot;&quot; # Alberta inflation doesn&#39;t go back far enough, use Canada for earlier dates vecs = (&#39;v41692327&#39;, &#39;v41690973&#39;) df = ( stats_can.vectors_to_df_local(vecs, path=&#39;data&#39;, start_date=dt.date(1965, 1, 1)) .rename(columns={&#39;v41692327&#39;: &#39;ab_inflation&#39;, &#39;v41690973&#39;: &#39;ca_inflation&#39;}) ) # fill in with Canadian inflation data where (early) Alberta inflation data is missing. mask = df[&#39;ab_inflation&#39;].isna() # Could probably do some interpolation or scaling before this, but I looked # at the raw series and they were pretty comparable df.loc[mask, &#39;ab_inflation&#39;] = df.loc[mask, &#39;ca_inflation&#39;] df = ( df .drop(columns=&#39;ca_inflation&#39;) .pipe(periodic_to_budget_annual, &#39;REF_DATE&#39;, 12) ) # Rescale to 2017 = 100 (this is fiscal year 2017, # original may have done calendar year) inf_2017 = float(df.loc[&#39;2017&#39;, &#39;ab_inflation&#39;]) df = df / inf_2017 return df def budget_real_per_capita() -&gt; pd.DataFrame: &quot;&quot;&quot;Get budget data in real per-capita terms. Returns - pd.DataFrame Budget data in real per-capita terms. &quot;&quot;&quot; # Read in budget data using the function defined in the # previous section clean_budget_df = clean_budget() # Everything except the dummy variable gets turned into # real per-capita terms scale_cols = clean_budget_df.columns.drop(&#39;heritage_dummy&#39;).tolist() # Get population per_capita = per_capita_data() # Get inflation inflation = inflation_data() # Combine the datasets, can just use assign because they all # have a datetime index dfpc = ( clean_budget_df .assign(pop=per_capita) .assign(cpi=inflation) ) # rescale to real per capita dfpc[scale_cols] = ( dfpc[scale_cols] # original data was in millions of dollars .mul(1_000_000) # divide by population and inflation for # real per-capita .div(dfpc[&#39;pop&#39;], axis=&#39;index&#39;) .div(dfpc[&#39;cpi&#39;], axis=&#39;index&#39;) ) return dfpc . Exogenous factors . The paper lists the Alberta employment rate, the Alberta unemployment rate, and the CAD/USD exchange rate as exogenous factors included in the model. Functions in this section acquire that data. I had to do some fiddling to get long enough historical series for some of the factors as you&#39;ll note in the code. It&#39;s hard to say for sure how the original author sourced this data. I&#39;ll just have to compare my tables and charts to his to see if I got close enough. . def download_historical_cad_usd() -&gt; pd.DataFrame: &quot;&quot;&quot;Get exchange rates from before 1971. FRED live data only goes back to 1971, I need a longer series This was what I could find. It&#39;s annual only, so I can&#39;t do it on a budget year basis, but hopefully it will be close enough This whole function is just some gross munging to read in a table from a web page. Once it&#39;s called we save it to the data folder so I don&#39;t have to re-call it every time I run this notebook. &quot;&quot;&quot; url = &#39;https://fxtop.com/en/historical-exchange-rates.php?YA=1&amp;C1=USD&amp;C2=CAD&amp;A=1&amp;YYYY1=1953&amp;MM1=01&amp;DD1=01&amp;YYYY2=2019&amp;MM2=04&amp;DD2=01&amp;LANG=en&#39; df = pd.read_html(url)[29] headers = df.iloc[0] new_df = ( pd.DataFrame(df.values[1:], columns=headers) .rename(columns={&#39;Year&#39;: &#39;year&#39;, &#39;Average USD/CAD&#39;: &#39;EXCAUS&#39;}) .assign(month=1) .assign(day=1) .assign(budget_dt=lambda df: pd.to_datetime(df[[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]])) .set_index(&#39;budget_dt&#39;) .reindex(columns=[&#39;EXCAUS&#39;]) ) new_df.to_csv(&#39;./data/early_cad_usd.csv&#39;) return new_df def read_historical_cad_usd(force_update: bool = False) -&gt; pd.DataFrame: &quot;&quot;&quot;Get exchange rates before 1971. This wraps the above function to read in the downloaded data if it&#39;s available and download and then read it if required. Parameters - force_update: bool Download the data set even if you already have it Returns - pd.DataFrame Exchange rates from 1965 to 1971 &quot;&quot;&quot; fname = Path(&#39;.&#39;).joinpath(&#39;data&#39;).joinpath(&#39;early_cad_usd.csv&#39;) if not fname.exists() or force_update: return download_historical_cad_usd() else: return pd.read_csv(fname).set_index(&#39;budget_dt&#39;) def download_cad_usd() -&gt; pd.DataFrame: &quot;&quot;&quot;Download monthly exchange data from FRED. For most of the period of interest I can get monthly data from FRED, so I&#39;ll do that where possible. Returns - pd.DataFrame Most of the CAD/USD exchange data I need for this analysis. &quot;&quot;&quot; df = pdr.get_data_fred(&#39;EXCAUS&#39;, start=dt.date(1970, 1, 1)) df.to_csv(&#39;./data/cad_usd.csv&#39;) return df def read_cad_usd(force_update=False): &quot;&quot;&quot;Get monthly exchange data from FRED. This wraps the above function to read in the downloaded data if it&#39;s available and download and then read it if required. Parameters - force_update: bool Download the data set even if you already have it Returns - pd.DataFrame Exchange rate data &quot;&quot;&quot; fname = Path(&#39;.&#39;).joinpath(&#39;data&#39;).joinpath(&#39;cad_usd.csv&#39;) if not fname.exists() or force_update: return download_cad_usd() else: return pd.read_csv(fname, parse_dates=[&#39;DATE&#39;]).set_index(&#39;DATE&#39;) def annual_cad_usd() -&gt; pd.DataFrame: &quot;&quot;&quot;Full series of CAD/USD in fiscal year format. Get FRED data and turn the monthly values into annualized on a budget basis for as much as possible. Fill in the remainder with calendar annual data from fxtop Returns - pd.DataFrame Exchange data on an annualized basis. &quot;&quot;&quot; # Create a datetime index of all the points we need annual_date_range = pd.date_range(&#39;1964-01-01&#39;, &#39;2018-01-01&#39;, freq=&#39;AS&#39;, name=&#39;budget_dt&#39;) # Get the old annual stuff to fill in later old_df = read_historical_cad_usd() df = ( # get the monthly series read_cad_usd() # annualize it .pipe(periodic_to_budget_annual, &#39;DATE&#39;, 12) # add in all the missing dates we need .reindex(annual_date_range) # fill those missing dates from the old annual data set. .fillna(old_df) ) return df def stats_can_exog() -&gt; pd.DataFrame: &quot;&quot;&quot;Bring in exogenous StatsCan data. Employment and Unemployment rates. Returns - pd.DataFrame Exogenous data required from StatsCan &quot;&quot;&quot; # Vectors for monthly series where available ur_vec = &quot;v2064516&quot; er_vec = &quot;v2064518&quot; annual_date_range = pd.date_range(&#39;1964-01-01&#39;, &#39;2018-01-01&#39;, freq=&#39;AS&#39;, name=&#39;budget_dt&#39;) # for the earlier periods we only have annual data old_df = ( stats_can.table_to_df(&#39;36-10-0345-01&#39;, path=&#39;data&#39;) # Get Alberta data only .loc[lambda x: x[&#39;GEO&#39;] == &#39;Alberta&#39;] # Keep only the categories we care about .loc[lambda x: x[&#39;Economic indicators&#39;].isin([&#39;Population&#39;, &#39;Total employment&#39;, &#39;Unemployment rate&#39;])] # pivot so the year is the row and the variables are the columns .pivot_table(index=&#39;REF_DATE&#39;, columns=&#39;Economic indicators&#39;, values=&#39;VALUE&#39;) .rename(columns={&#39;Unemployment rate&#39;: &#39;unemployment_rate&#39;}) # calculate the employment rate .assign(employment_rate=lambda x: (x[&#39;Total employment&#39;] / x[&#39;Population&#39;]) * 100) # drop the population, just used for calculating employment rate .reindex(columns=[&#39;unemployment_rate&#39;, &#39;employment_rate&#39;]) .rename_axis(&#39;budget_dt&#39;, axis=&#39;index&#39;) .rename_axis(None, axis=&#39;columns&#39;) ) # Get monthly data where available df = ( stats_can.vectors_to_df_local([ur_vec, er_vec], path=&#39;data&#39;, start_date=dt.date(1964, 1, 1)) .rename(columns={ur_vec: &#39;unemployment_rate&#39;, er_vec: &#39;employment_rate&#39;}) # annualize .pipe(periodic_to_budget_annual, &#39;REF_DATE&#39;, 12) # get the full range of data we want .reindex(annual_date_range) # fill in the gaps with the old annual series .fillna(old_df) # Not ideal but even the annual series doesn&#39;t go quite back # far enough so we have to backfill the earliest available # data point .fillna(method=&#39;bfill&#39;) ) return df def exogenous_variables() -&gt; pd.DataFrame: &quot;&quot;&quot;Bring in exogenous parameters together. From the paper: We also include other exogenous variables that are likely to affect the province’s budget. It is known that the various components of the provincial budget can be influenced by the business cycle. Thus, following Buettner and Wildsain (2006), we account for the potential effects of the business cycle by including one-period lagged changes in the provincial employment and unemployment rates. Another important exogenous factor that is often cited in provincial budget documents as being important in influencing the provincial government’s oil royalty revenue is the Canadian-U.S. dollar exchange rate. For this reason, we control for this factor by including one period lagged changes in the Canadian-U.S. dollar exchange rate Returns - pd.DataFrame All the necessary exogenous factors for reproducing the paper. &quot;&quot;&quot; cadusd = annual_cad_usd() ur_er = stats_can_exog() df = pd.concat([cadusd, ur_er], axis=&#39;columns&#39;) return df . Exploratory Figures . Figure 1 . Page 5 of the report charts Non-renewable Resource Revenue, Total Expenditure, and Total Revenue. All are in per-capita 2017 dollars. Reproducing this chart will be a good starting check that my data extraction and transformation matches the original author&#39;s strategy. . def fig1(df: pd.DataFrame) -&gt; alt.Chart: &quot;&quot;&quot;Reproduce Figure 1 from the paper. Parameters - df: pd.DataFrame A dataframe with non-renewable resource revenue, total expenditure, and total revenue time series &quot;&quot;&quot; chart_df = ( df .loc[&#39;1970&#39;:&#39;2016&#39;, [&#39;natural_resource_revenue&#39;, &#39;total_revenue&#39;, &#39;total_expenditure&#39;]] .rename(columns={ &#39;natural_resource_revenue&#39;: &#39;Non-renewable Resource Revenue&#39;, &#39;total_revenue&#39;: &#39;Total Revenue&#39;, &#39;total_expenditure&#39;: &#39;Total Expenditure&#39; }) .reset_index() .melt(id_vars=&#39;budget_dt&#39;) ) c_domain = [&quot;Non-renewable Resource Revenue&quot;, &quot;Total Expenditure&quot;, &quot;Total Revenue&quot;] c_range = [&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;] chart = ( alt.Chart(chart_df) .mark_line() .encode( x=alt.X(&#39;budget_dt:T&#39;, axis=alt.Axis(title=None)), y=alt.Y(&#39;value:Q&#39;, axis=alt.Axis(title=&#39;Per capita in 2017 dollars&#39;), scale=alt.Scale(domain=(0, 14_000))), color=alt.Color(&#39;variable:N&#39;, legend=alt.Legend(title=None, orient=&#39;bottom&#39;), scale=alt.Scale(domain=c_domain, range=c_range)) ) .properties(width=1250, height=500) ) return chart . Here&#39;s the original chart from the paper: . Image(filename=&quot;img/ferede_fig_1.png&quot;) . And here&#39;s mine: . df = budget_real_per_capita() fig1(df) . This graph looks very similar to the chart in the paper, with a notable exception of the 1976/1977 budget year. My chart shows Non-renewable Resource Revenue as slightly negative, whereas the original chart has it largely in line with 1975/1976 and 1977/1978. NRR is negative in my chart because I have netted out contributions to the Alberta Heritage Savings Trust Fund (AHSTF). To the best of my understanding, the original paper does the same, and the consistent values between the two in all other years supports that. Quoting the original paper: . The part of resource revenue that is saved in the AHSTF is not expected to influence the provincial government’s spending and revenue-raising choices. For this reason, in our analysis, we exclude the part of the resource revenue that is saved in the AHSTF from the non-renewable-resource revenue data. . For comparison, here is the same chart, but without netting AHSTF contributions from revenue revenue: . df.head() . personal_income_tax corporate_income_tax natural_resource_revenue other_revenue total_prog_exp debt_service total_heritage heritage_dummy natural_resource_revenue_before_heritage total_revenue total_expenditure deficit pop cpi . budget_dt . 1965-01-01 231.093501 | 174.729232 | 1397.833860 | 1442.925275 | 2327.844291 | 11.272854 | 0.0 | 0.0 | 1397.833860 | 3246.581868 | 2339.117145 | -907.464724 | 1448000.0 | 0.122526 | . 1966-01-01 301.024417 | 145.136773 | 1290.104645 | 1542.750138 | 2891.984579 | 10.750872 | 0.0 | 0.0 | 1290.104645 | 3279.015972 | 2902.735451 | -376.280521 | 1462500.0 | 0.127201 | . 1967-01-01 407.988537 | 203.994268 | 1116.868620 | 2351.033944 | 4436.875339 | 10.199713 | 0.0 | 0.0 | 1116.868620 | 4079.885369 | 4447.075053 | 367.189683 | 1488250.0 | 0.131755 | . 1968-01-01 469.425988 | 239.503055 | 1360.377353 | 2639.323668 | 4368.535726 | 19.160244 | 0.0 | 0.0 | 1360.377353 | 4708.630064 | 4387.695970 | -320.934094 | 1522750.0 | 0.137098 | . 1969-01-01 599.977490 | 286.556413 | 1141.748209 | 2444.684401 | 4396.849967 | 17.909776 | 0.0 | 0.0 | 1141.748209 | 4472.966515 | 4414.759743 | -58.206771 | 1558000.0 | 0.143352 | . no_net_df = ( df .assign(total_revenue=lambda df: df[&quot;total_revenue&quot;] + df[&quot;total_heritage&quot;]) .assign(natural_resource_revenue=lambda df: df[&quot;natural_resource_revenue&quot;] + df[&quot;total_heritage&quot;]) .assign(deficit=lambda df: df[&quot;total_expenditure&quot;] - df[&quot;total_revenue&quot;]) ) fig1(no_net_df) . 1976/1977 more closely matches the original chart in the paper, but the remaining years in the period of mid 70s to mid 80s when there were significant contributions clearly do not match. Let&#39;s try one more where I just substitute that one year. . error_df = df.copy() heritage_76 = error_df.loc[&quot;1976&quot;, &quot;total_heritage&quot;] error_df.loc[&quot;1976&quot;, &quot;natural_resource_revenue&quot;] += heritage_76 error_df.loc[&quot;1976&quot;, &quot;total_revenue&quot;] += heritage_76 error_df.loc[&quot;1976&quot;, &quot;deficit&quot;] -= heritage_76 fig1(error_df) . Here&#39;s the original figure again for easier comparison . Image(filename=&quot;img/ferede_fig_1.png&quot;) . From eyeballing it that looks exactly like Figure 1 in the paper. It appears there&#39;s a data error in the original paper. For the rest of this analysis I&#39;ll compare both my base implementation of the data, as well as the one with the data error. . Figure 2 . Page 6 of the paper produces a scatter plot of Real per capita non-renewable resource revenue on the X axis vs. Real per capita budget balance on the Y, along with a linear trend fit. . def fig2(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot;Reproduce Figure 2 from the paper. Parameters - df: pd.DataFrame The table with historical revenue and expenditure data. &quot;&quot;&quot; sns.set(rc={&#39;figure.figsize&#39;:(11.7,8.27)}) chart_df = ( df .loc[&#39;1970&#39;:&#39;2016&#39;, [&#39;natural_resource_revenue&#39;, &#39;deficit&#39;]] .rename(columns={ &#39;natural_resource_revenue&#39;: &#39;Non-renewable Resource Revenue&#39;, &#39;deficit&#39;: &#39;Deficit&#39; }) .assign(balance=lambda df: df[&#39;Deficit&#39;] * -1) .rename(columns={&#39;balance&#39;: &#39;Budget Balance&#39;}) .copy() ) sns.regplot(x=&#39;Non-renewable Resource Revenue&#39;, y=&#39;Budget Balance&#39;, data=chart_df) . Here&#39;s the original figure . Image(filename=&quot;img/ferede_fig_2.png&quot;) . Here&#39;s the figure using my original data: . fig2(df) . And here&#39;s the figure using the version with a data error: . fig2(error_df) . Again, this chart is more consistent with the dataframe where I don&#39;t net heritage fund contributions out of 1976 but do for all other years . Model Specification and estimation . This section combines the previously specified data extraction with transformations necessary to produce summary statistics, statistical tests, and the VAR model itself. . def model_df_levels(budg: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot;Combine real per capita budget data to get model data in levels. lag exogenous variables (unemployment and employment rates, CAD/USD exchange) Parameters - budg: pd.DataFrame Budget data, either with or without data error Returns - pd.DataFrame Budget data combined with exogenous factors &quot;&quot;&quot; exog = exogenous_variables() df = ( pd.concat([budg, exog], axis=&#39;columns&#39;) .rename(columns={&#39;total_prog_exp&#39;: &#39;program_expenditure&#39;, &#39;EXCAUS&#39;: &#39;cad_usd&#39;}) .assign(ur_lag=lambda df: df[&#39;unemployment_rate&#39;].shift(periods=1)) .assign(er_lag=lambda df: df[&#39;employment_rate&#39;].shift(periods=1)) .assign(cad_usd_lag=lambda df: df[&#39;cad_usd&#39;].shift(periods=1)) .reindex(columns=[ &#39;program_expenditure&#39;, &#39;debt_service&#39;, &#39;corporate_income_tax&#39;, &#39;personal_income_tax&#39;, &#39;other_revenue&#39;, &#39;natural_resource_revenue&#39;, &#39;deficit&#39;, &#39;heritage_dummy&#39;, &#39;ur_lag&#39;, &#39;er_lag&#39;, &#39;cad_usd_lag&#39; ]) ) return df . mdfl = model_df_levels(df) mdfl_err = model_df_levels(error_df) . Sumary statistics for key variables, 1970-71, 2016-17 in levels . Prior to any modeling, let&#39;s compare the summary statistics for the data sets I&#39;ve created against those in the paper: . number = &quot;{:0&lt;4,.1f}&quot; percent = &#39;{:.1%}&#39; count = &quot;{:0.0f}&quot; def tbl1_level(model_df: pd.DataFrame): &quot;&quot;&quot;Produce summary statistics of the input data in levels. Parameters - model_df: pd.DataFrame Input data set Returns - pd.io.formats.style.Styler: Nicely formatted summary statistics &quot;&quot;&quot; df = ( model_df .loc[&#39;1970&#39;:&#39;2016&#39;] .copy() .drop(columns=[&#39;heritage_dummy&#39;]) .reindex(columns=[ &#39;natural_resource_revenue&#39;, &#39;corporate_income_tax&#39;, &#39;personal_income_tax&#39;, &#39;other_revenue&#39;, &#39;debt_service&#39;, &#39;program_expenditure&#39;, &#39;deficit&#39;, &#39;ur_lag&#39;, &#39;er_lag&#39;, &#39;cad_usd_lag&#39; ]) .describe() .T .style.format({ &#39;count&#39;: count, &#39;mean&#39;: number, &#39;std&#39;: number, &#39;min&#39;: number, &#39;25%&#39;: number, &#39;50%&#39;: number, &#39;75%&#39;: number, &#39;max&#39;: number }) ) return df . Here&#39;s Table 1 from the paper: . Image(filename=&quot;img/ferede_tbl_1.png&quot;) . Here&#39;s my summary of the top half . tbl1_level(mdfl) . count mean std min 25% 50% 75% max . natural_resource_revenue 47 | 2,737.2 | 1,358.9 | -125.3 | 1,726.4 | 2,371.7 | 3,941.6 | 5,181.6 | . corporate_income_tax 47 | 792.1 | 349.6 | 245.8 | 544.6 | 723.6 | 1,010.9 | 1,560.6 | . personal_income_tax 47 | 1,862.1 | 607.8 | 767.1 | 1,403.8 | 1,889.6 | 2,359.8 | 2,830.0 | . other_revenue 47 | 4,394.6 | 1,084.9 | 2,352.8 | 3,632.2 | 4,699.2 | 5,209.2 | 5,818.2 | . debt_service 47 | 322.8 | 334.9 | 31.4 | 82.7 | 158.5 | 486.9 | 1,075.6 | . program_expenditure 47 | 9,399.1 | 2,161.2 | 4,745.8 | 7,623.0 | 10,060.9 | 11,142.6 | 12,869.2 | . deficit 47 | -64.0 | 1,618.0 | -3,184.0 | -1,320.5 | -140.6 | 1,092.3 | 3,775.8 | . ur_lag 47 | 6.10 | 2.20 | 3.40 | 4.50 | 5.40 | 7.50 | 11.4 | . er_lag 47 | 63.7 | 9.90 | 38.5 | 65.1 | 67.3 | 69.0 | 71.8 | . cad_usd_lag 45 | 1.20 | 0.20 | 1.00 | 1.10 | 1.20 | 1.40 | 1.60 | . And the same summary on the data with the introduced error . tbl1_level(mdfl_err) . count mean std min 25% 50% 75% max . natural_resource_revenue 47 | 2,843.8 | 1,325.6 | 691.9 | 1,760.7 | 2,532.1 | 3,972.6 | 5,181.6 | . corporate_income_tax 47 | 792.1 | 349.6 | 245.8 | 544.6 | 723.6 | 1,010.9 | 1,560.6 | . personal_income_tax 47 | 1,862.1 | 607.8 | 767.1 | 1,403.8 | 1,889.6 | 2,359.8 | 2,830.0 | . other_revenue 47 | 4,394.6 | 1,084.9 | 2,352.8 | 3,632.2 | 4,699.2 | 5,209.2 | 5,818.2 | . debt_service 47 | 322.8 | 334.9 | 31.4 | 82.7 | 158.5 | 486.9 | 1,075.6 | . program_expenditure 47 | 9,399.1 | 2,161.2 | 4,745.8 | 7,623.0 | 10,060.9 | 11,142.6 | 12,869.2 | . deficit 47 | -170.7 | 1,587.7 | -3,184.0 | -1,425.7 | -202.8 | 945.9 | 3,775.8 | . ur_lag 47 | 6.10 | 2.20 | 3.40 | 4.50 | 5.40 | 7.50 | 11.4 | . er_lag 47 | 63.7 | 9.90 | 38.5 | 65.1 | 67.3 | 69.0 | 71.8 | . cad_usd_lag 45 | 1.20 | 0.20 | 1.00 | 1.10 | 1.20 | 1.40 | 1.60 | . All the figures that I can validate against (exogenous variables aren&#39;t reported in the paper) are reasonably close. The one noted difference is the previously described outlier in natural resource revenue which leads to my minimum for that variable being significantly lower than in the paper. That large one goes away again if I introduce the same data error described above. My guess for the remaining small discrepancies are differences in calculating population or CPI. . Sumary statistics for key variables, 1970-71, 2016-17, first difference . Reproduce the bottom half of table 1 from the paper . def model_df_first_diff(mdfl: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot;Produce the first difference of the level model df. Parameters - mdfl: pd.DataFrame The model dataframe in levels Returns - pd.DataFrame The first differenced model dataframe &quot;&quot;&quot; df = ( mdfl .diff() .loc[&#39;1970&#39;:&#39;2016&#39;] .copy() .assign(heritage_dummy=mdfl[&#39;heritage_dummy&#39;]) # don&#39;t want to lag diff this .assign(constant=1) .assign(zero=0) .assign(nrrd=lambda df: df[[&#39;natural_resource_revenue&#39;, &#39;zero&#39;]].min(axis=&#39;columns&#39;)) .assign(nrri=lambda df: df[[&#39;natural_resource_revenue&#39;, &#39;zero&#39;]].max(axis=&#39;columns&#39;)) .reindex(columns=[ &#39;natural_resource_revenue&#39;, &#39;nrri&#39;, &#39;nrrd&#39;, &#39;corporate_income_tax&#39;, &#39;personal_income_tax&#39;, &#39;other_revenue&#39;, &#39;debt_service&#39;, &#39;program_expenditure&#39;, &#39;deficit&#39;, &#39;ur_lag&#39;, &#39;er_lag&#39;, &#39;cad_usd_lag&#39;, &#39;heritage_dummy&#39;, &#39;constant&#39; ]) ) return df . def tbl1_diff(model_df: pd.DataFrame) -&gt; pd.io.formats.style.Styler: &quot;&quot;&quot;Produce summary statistics of the first differenced data set. Parameters - model_df: pd.DataFrame Input data set Returns - pd.io.formats.style.Styler: Nicely styled summary statistics &quot;&quot;&quot; df = ( model_df_first_diff(model_df) .drop(columns=[&#39;heritage_dummy&#39;, &#39;constant&#39;]) .describe() .T .style.format({ &#39;count&#39;: count, &#39;mean&#39;: number, &#39;std&#39;: number, &#39;min&#39;: number, &#39;25%&#39;: number, &#39;50%&#39;: number, &#39;75%&#39;: number, &#39;max&#39;: number }) ) return df . Here&#39;s Table 1 from the paper again: . Image(filename=&quot;img/ferede_tbl_1.png&quot;) . Here&#39;s mine: . tbl1_diff(mdfl) . count mean std min 25% 50% 75% max . natural_resource_revenue 47 | -8.3 | 1,365.3 | -4,543.2 | -551.3 | 64.2 | 463.5 | 4,638.7 | . nrri 47 | 437.8 | 852.9 | 0.00 | 0.00 | 64.2 | 463.5 | 4,638.7 | . nrrd 47 | -446.1 | 858.9 | -4,543.2 | -551.3 | 0.00 | 0.00 | 0.00 | . corporate_income_tax 47 | 13.3 | 201.2 | -463.0 | -108.5 | 25.9 | 137.8 | 447.1 | . personal_income_tax 47 | 42.7 | 227.2 | -690.7 | -28.7 | 40.0 | 154.2 | 936.9 | . other_revenue 47 | 41.8 | 577.2 | -1,268.7 | -288.9 | -28.0 | 162.8 | 2,417.6 | . debt_service 47 | 4.50 | 86.2 | -247.5 | -21.6 | 2.00 | 36.0 | 222.3 | . program_expenditure 47 | 147.4 | 734.5 | -1,263.2 | -245.9 | 202.7 | 552.7 | 2,477.9 | . deficit 47 | 62.3 | 1,578.2 | -4,722.8 | -806.6 | -39.0 | 836.0 | 4,431.3 | . ur_lag 47 | 0.10 | 1.10 | -1.8 | -0.7 | -0.1 | 0.30 | 3.90 | . er_lag 47 | 0.60 | 3.30 | -3.2 | -0.2 | 0.40 | 0.90 | 21.4 | . cad_usd_lag 44 | 0.00 | 0.10 | -0.2 | -0.0 | 0.00 | 0.10 | 0.20 | . And with the data error: . tbl1_diff(mdfl_err) . count mean std min 25% 50% 75% max . natural_resource_revenue 47 | -8.3 | 977.5 | -2,451.6 | -471.1 | 64.2 | 458.3 | 2,757.8 | . nrri 47 | 349.1 | 579.4 | 0.00 | 0.00 | 64.2 | 458.3 | 2,757.8 | . nrrd 47 | -357.4 | 604.0 | -2,451.6 | -471.1 | 0.00 | 0.00 | 0.00 | . corporate_income_tax 47 | 13.3 | 201.2 | -463.0 | -108.5 | 25.9 | 137.8 | 447.1 | . personal_income_tax 47 | 42.7 | 227.2 | -690.7 | -28.7 | 40.0 | 154.2 | 936.9 | . other_revenue 47 | 41.8 | 577.2 | -1,268.7 | -288.9 | -28.0 | 162.8 | 2,417.6 | . debt_service 47 | 4.50 | 86.2 | -247.5 | -21.6 | 2.00 | 36.0 | 222.3 | . program_expenditure 47 | 147.4 | 734.5 | -1,263.2 | -245.9 | 202.7 | 552.7 | 2,477.9 | . deficit 47 | 62.3 | 1,260.2 | -2,615.7 | -758.1 | -39.0 | 747.4 | 3,117.0 | . ur_lag 47 | 0.10 | 1.10 | -1.8 | -0.7 | -0.1 | 0.30 | 3.90 | . er_lag 47 | 0.60 | 3.30 | -3.2 | -0.2 | 0.40 | 0.90 | 21.4 | . cad_usd_lag 44 | 0.00 | 0.10 | -0.2 | -0.0 | 0.00 | 0.10 | 0.20 | . As with everything so far, the data overall matches, and Natural Resource Revenue matches a lot better if I neglect to net out the heritage fund in 1976. . Unit-Root Tests . Table A1 in the paper shows the results of unit root tests for both the level and first differenced variables in the model. This section will reproduce those tables . def stationarity_tests(df: pd.DataFrame, first_diff: bool = False) -&gt; pd.DataFrame: &quot;&quot;&quot;Compute stationarity test statistics. Parameters - df: pd.DataFrame The model input data first_diff: bool, default False Perform tests on first differenced version of the data &quot;&quot;&quot; if first_diff: df = ( model_df_first_diff(df) .drop(columns=[&quot;heritage_dummy&quot;, &quot;constant&quot;]) ) else: df = ( df .loc[&#39;1970&#39;:&#39;2016&#39;] .copy() .drop(columns=[&#39;heritage_dummy&#39;]) .reindex( columns=[ &#39;natural_resource_revenue&#39;, &#39;corporate_income_tax&#39;, &#39;personal_income_tax&#39;, &#39;other_revenue&#39;, &#39;debt_service&#39;, &#39;program_expenditure&#39;, &#39;deficit&#39;, &#39;ur_lag&#39;, &#39;er_lag&#39;, &#39;cad_usd_lag&#39;, ] ) ) tests_dict = {&#39;ADF&#39;: ADF, &#39;Phillips-Perron&#39;: PhillipsPerron, &#39;DF-GLS&#39;: DFGLS} cols = df.columns tests_df = pd.DataFrame() for test_label, test in tests_dict.items(): for col in cols: if test_label != &#39;Phillips-Perron&#39;: col_test = test(df[col].dropna(), method=&#39;BIC&#39;) else: col_test = test(df[col].dropna()) test_val = col_test.stat test_p = col_test.pvalue test_summary = f&#39;val: {test_val:0.3f}, p: {test_p:.1%}&#39; tests_df.loc[col, test_label] = test_summary return tests_df . Here&#39;s the table from the paper: . Image(filename=&quot;img/ferede_tbl_a1.png&quot;) . Here is mine with my data set . stationarity_tests(mdfl, first_diff=False) . ADF Phillips-Perron DF-GLS . natural_resource_revenue val: -3.927, p: 0.2% | val: -4.056, p: 0.1% | val: -3.125, p: 0.2% | . corporate_income_tax val: -2.340, p: 15.9% | val: -2.285, p: 17.7% | val: -1.676, p: 9.2% | . personal_income_tax val: -1.650, p: 45.7% | val: -1.440, p: 56.3% | val: -0.688, p: 43.2% | . other_revenue val: -2.196, p: 20.8% | val: -2.057, p: 26.2% | val: -1.585, p: 11.0% | . debt_service val: -1.548, p: 51.0% | val: -1.697, p: 43.3% | val: -1.377, p: 16.2% | . program_expenditure val: -2.966, p: 3.8% | val: -2.137, p: 23.0% | val: -0.682, p: 43.4% | . deficit val: -3.510, p: 0.8% | val: -3.924, p: 0.2% | val: -3.537, p: 0.0% | . ur_lag val: -2.709, p: 7.3% | val: -2.071, p: 25.6% | val: -2.319, p: 2.1% | . er_lag val: -10.652, p: 0.0% | val: -2.931, p: 4.2% | val: -0.763, p: 39.7% | . cad_usd_lag val: -2.454, p: 12.7% | val: -1.796, p: 38.2% | val: -1.870, p: 6.1% | . stationarity_tests(mdfl, first_diff=True) . ADF Phillips-Perron DF-GLS . natural_resource_revenue val: -7.403, p: 0.0% | val: -10.300, p: 0.0% | val: -7.430, p: 0.0% | . nrri val: -6.885, p: 0.0% | val: -6.896, p: 0.0% | val: -6.533, p: 0.0% | . nrrd val: -7.835, p: 0.0% | val: -9.218, p: 0.0% | val: -7.634, p: 0.0% | . corporate_income_tax val: -7.228, p: 0.0% | val: -8.484, p: 0.0% | val: -7.103, p: 0.0% | . personal_income_tax val: -5.560, p: 0.0% | val: -8.831, p: 0.0% | val: -6.703, p: 0.0% | . other_revenue val: -7.914, p: 0.0% | val: -8.169, p: 0.0% | val: -7.717, p: 0.0% | . debt_service val: -3.918, p: 0.2% | val: -4.252, p: 0.1% | val: -3.931, p: 0.0% | . program_expenditure val: -2.545, p: 10.5% | val: -6.105, p: 0.0% | val: -2.461, p: 1.4% | . deficit val: -2.485, p: 11.9% | val: -10.105, p: 0.0% | val: -2.509, p: 1.2% | . ur_lag val: -4.924, p: 0.0% | val: -4.056, p: 0.1% | val: -4.901, p: 0.0% | . er_lag val: -2.697, p: 7.5% | val: -6.507, p: 0.0% | val: -2.742, p: 0.6% | . cad_usd_lag val: -3.318, p: 1.4% | val: -3.042, p: 3.1% | val: -3.384, p: 0.1% | . And with the error data set: . stationarity_tests(mdfl_err, first_diff=False) . ADF Phillips-Perron DF-GLS . natural_resource_revenue val: -2.652, p: 8.3% | val: -2.875, p: 4.8% | val: -2.113, p: 3.5% | . corporate_income_tax val: -2.340, p: 15.9% | val: -2.285, p: 17.7% | val: -1.676, p: 9.2% | . personal_income_tax val: -1.650, p: 45.7% | val: -1.440, p: 56.3% | val: -0.688, p: 43.2% | . other_revenue val: -2.196, p: 20.8% | val: -2.057, p: 26.2% | val: -1.585, p: 11.0% | . debt_service val: -1.548, p: 51.0% | val: -1.697, p: 43.3% | val: -1.377, p: 16.2% | . program_expenditure val: -2.966, p: 3.8% | val: -2.137, p: 23.0% | val: -0.682, p: 43.4% | . deficit val: -2.566, p: 10.0% | val: -2.763, p: 6.4% | val: -2.606, p: 0.9% | . ur_lag val: -2.709, p: 7.3% | val: -2.071, p: 25.6% | val: -2.319, p: 2.1% | . er_lag val: -10.652, p: 0.0% | val: -2.931, p: 4.2% | val: -0.763, p: 39.7% | . cad_usd_lag val: -2.454, p: 12.7% | val: -1.796, p: 38.2% | val: -1.870, p: 6.1% | . stationarity_tests(mdfl_err, first_diff=True) . ADF Phillips-Perron DF-GLS . natural_resource_revenue val: -5.913, p: 0.0% | val: -7.791, p: 0.0% | val: -5.925, p: 0.0% | . nrri val: -6.119, p: 0.0% | val: -6.149, p: 0.0% | val: -5.731, p: 0.0% | . nrrd val: -7.846, p: 0.0% | val: -8.008, p: 0.0% | val: -7.640, p: 0.0% | . corporate_income_tax val: -7.228, p: 0.0% | val: -8.484, p: 0.0% | val: -7.103, p: 0.0% | . personal_income_tax val: -5.560, p: 0.0% | val: -8.831, p: 0.0% | val: -6.703, p: 0.0% | . other_revenue val: -7.914, p: 0.0% | val: -8.169, p: 0.0% | val: -7.717, p: 0.0% | . debt_service val: -3.918, p: 0.2% | val: -4.252, p: 0.1% | val: -3.931, p: 0.0% | . program_expenditure val: -2.545, p: 10.5% | val: -6.105, p: 0.0% | val: -2.461, p: 1.4% | . deficit val: -6.642, p: 0.0% | val: -8.574, p: 0.0% | val: -6.609, p: 0.0% | . ur_lag val: -4.924, p: 0.0% | val: -4.056, p: 0.1% | val: -4.901, p: 0.0% | . er_lag val: -2.697, p: 7.5% | val: -6.507, p: 0.0% | val: -2.742, p: 0.6% | . cad_usd_lag val: -3.318, p: 1.4% | val: -3.042, p: 3.1% | val: -3.384, p: 0.1% | . Documentation on the test tools I used can be found here . There are some interesting differences. Most notable is that on the levels of the deficit series I reject the null hypothesis of a unit root using all three tests at a significance level &lt; 1%. The paper specifically notes that if the deficit is stationary in levels then a Vector Error Correction model can be applied. As the original author&#39;s fails to reject the null he implements a Vector AutoRegression model on the first differenced data. In levels the only other series that I find to be stationary is natural resource revenue. ADF on program expenditure would also reject the null at 5% significance, but would fail to reject it using the other two tests. . Looking at the first differenced series, since that&#39;s what the paper ultimately ends up using, I also reject the null hypothesis of a unit root for all variables using all tests at a 1% significant except program expenditure and deficit using Augmented Dickey Fuller. Those last two tests differ from what&#39;s reported in the paper. . The paper notes that it uses the Schwarz Information Criterion (SIC) for determining optimal lags in the DF-GLS test. It doesn&#39;t specify what it&#39;s using in the other two tests. For ADF and DF-GLS I used the Schwarz/Bayesian IC (BIC), which is just another name for SIC. Phillips-Perron only uses 1 lag and then Newey-West for a long run variance estimator. I also ran these tests using Akaike IC (AIC) for optimal lags for ADF and DF-GLS, with similar results. . Again we can see that using the results with the data error more closely matches the table in the paper, specifically around resource revenue and deficits. . The Model . The conclusions from the paper are based on fitting a VAR to the first differenced data set we&#39;ve been analyzing above. Let&#39;s do that now and compare the results to the paper. . def fit_var(mdfl: pd.DataFrame) -&gt; statsmodels.tsa.vector_ar.var_model.VARResults: &quot;&quot;&quot;Fit a VAR to the model data. Parameters - mdfl: pd.DataFrame Input model data Returns - statsmodels.tsa.vector_ar.var_model.VarResults The fitted model &quot;&quot;&quot; vec_df = ( model_df_first_diff(mdfl) .drop(columns=&#39;natural_resource_revenue&#39;) .dropna() ) endog_df = vec_df[[ &#39;nrri&#39;, &#39;nrrd&#39;, &#39;program_expenditure&#39;, &#39;debt_service&#39;, &#39;corporate_income_tax&#39;, &#39;personal_income_tax&#39;, &#39;other_revenue&#39; ]] exog_df = vec_df[[&#39;ur_lag&#39;, &#39;er_lag&#39;, &#39;cad_usd_lag&#39;, &#39;heritage_dummy&#39;]] model = VAR(endog=endog_df, exog=exog_df, freq=&#39;AS&#39;) # Fit the model with 2 lags results = model.fit(2) return results . def highlight_significance(val): &quot;&quot;&quot;Colour code statistical significance. Takes a scalar and returns a string with the css property `&#39;color: &lt;color&gt;&#39;` where color is maroon for 1% significance, red for 5% significance, orange for 10%, and black otherwise Parameters - val: float The p value of a test Returns - str: A formatted colour coded p value &quot;&quot;&quot; if val &lt;= 0.01: color = &#39;maroon&#39; elif val &lt;= 0.05: color = &#39;red&#39; elif val &lt;= 0.1: color = &#39;orange&#39; else: color = &#39;black&#39; return f&#39;color: {color}&#39; . results = fit_var(mdfl) results_err = fit_var(mdfl_err) summary = results.summary() . reindex_cols = [&quot;program_expenditure&quot;, &quot;debt_service&quot;, &quot;corporate_income_tax&quot;, &quot;personal_income_tax&quot;, &quot;other_revenue&quot;, &quot;nrri&quot;, &quot;nrrd&quot;] index_order = [&quot;nrri&quot;, &quot;nrrd&quot;, &quot;program_expenditure&quot;, &quot;debt_service&quot;, &quot;corporate_income_tax&quot;, &quot;personal_income_tax&quot;, &quot;other_revenue&quot;] reindex_rows = list(chain.from_iterable((f&quot;L1.{row}&quot;, f&quot;L2.{row}&quot;) for row in index_order)) . Here&#39;s the table from the paper: . Image(filename=&quot;img/ferede_tbl_a2.png&quot;) . results.params.reindex(index=reindex_rows, columns=reindex_cols) . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue nrri nrrd . L1.nrri 0.307014 | -0.028267 | 0.081846 | -0.052581 | -0.197417 | 0.157775 | 0.002814 | . L2.nrri 0.328607 | 0.006210 | -0.044256 | -0.031283 | -0.073461 | 0.059988 | -0.052033 | . L1.nrrd 0.131802 | -0.034380 | 0.026578 | -0.144894 | 0.193682 | -0.067386 | -0.333322 | . L2.nrrd 0.353466 | -0.020362 | 0.089569 | -0.037287 | -0.147460 | 0.110970 | -0.055394 | . L1.program_expenditure -0.089949 | 0.024739 | -0.000802 | -0.053868 | 0.070080 | -0.065026 | -0.105625 | . L2.program_expenditure -0.262484 | 0.009716 | -0.031223 | 0.042743 | 0.410439 | 0.025392 | -0.456895 | . L1.debt_service 0.794685 | 0.401266 | -0.108643 | 0.336733 | 0.400447 | -2.370450 | -0.278888 | . L2.debt_service -2.693856 | 0.228409 | -0.088742 | -0.179124 | -0.041884 | 0.290135 | -0.215989 | . L1.corporate_income_tax -0.972853 | -0.092308 | -0.210123 | 0.169059 | -0.071714 | -0.914462 | 0.730905 | . L2.corporate_income_tax 0.088413 | -0.199031 | -0.206852 | 0.245321 | -0.509617 | -0.683630 | -0.578293 | . L1.personal_income_tax 0.959002 | -0.041882 | 0.292569 | -0.378646 | -0.250158 | 0.347750 | 0.862515 | . L2.personal_income_tax 0.814557 | 0.084507 | -0.112748 | -0.099020 | -0.528812 | 0.385575 | -0.226230 | . L1.other_revenue 0.347131 | -0.015834 | -0.042866 | -0.022883 | -0.296401 | 0.224576 | -0.102018 | . L2.other_revenue -0.140592 | -0.010601 | -0.009822 | -0.010749 | -0.339955 | 0.385613 | 0.043835 | . results.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(&quot;{:.2%}&quot;) . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue nrri nrrd . L1.nrri 17.85% | 28.23% | 19.05% | 46.54% | 31.40% | 35.35% | 99.27% | . L2.nrri 3.41% | 72.82% | 29.76% | 52.28% | 58.14% | 60.37% | 80.42% | . L1.nrrd 54.73% | 17.31% | 65.78% | 3.61% | 30.34% | 67.97% | 26.07% | . L2.nrrd 8.86% | 39.46% | 11.53% | 56.93% | 40.83% | 47.32% | 84.37% | . L1.program_expenditure 63.72% | 26.03% | 98.78% | 37.09% | 66.89% | 64.73% | 68.24% | . L2.program_expenditure 16.67% | 65.69% | 54.83% | 47.56% | 1.18% | 85.75% | 7.53% | . L1.debt_service 62.80% | 3.37% | 80.89% | 51.54% | 77.63% | 5.24% | 90.00% | . L2.debt_service 9.99% | 22.60% | 84.32% | 72.89% | 97.63% | 81.21% | 92.24% | . L1.corporate_income_tax 17.66% | 26.58% | 28.68% | 45.69% | 90.77% | 8.83% | 45.31% | . L2.corporate_income_tax 90.24% | 1.66% | 29.52% | 28.12% | 41.09% | 20.34% | 55.35% | . L1.personal_income_tax 11.91% | 55.47% | 8.27% | 5.12% | 63.61% | 44.83% | 30.03% | . L2.personal_income_tax 12.28% | 16.47% | 43.56% | 55.23% | 24.36% | 32.70% | 75.15% | . L1.other_revenue 11.25% | 52.98% | 47.45% | 74.03% | 11.48% | 16.83% | 73.04% | . L2.other_revenue 54.20% | 68.98% | 87.64% | 88.26% | 8.61% | 2.48% | 88.83% | . results_err.params.reindex(index=reindex_rows, columns=reindex_cols) . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue nrri nrrd . L1.nrri 0.531582 | -0.024542 | 0.107731 | -0.123072 | -0.150617 | 0.208281 | -0.080868 | . L2.nrri -0.184916 | -0.003392 | -0.079855 | -0.031832 | -0.134564 | 0.023580 | 0.076075 | . L1.nrrd -0.164689 | -0.033454 | 0.002593 | -0.085661 | 0.110608 | -0.058941 | -0.105753 | . L2.nrrd 0.179398 | -0.020179 | 0.072508 | 0.027891 | -0.178763 | 0.081864 | -0.032297 | . L1.program_expenditure -0.148569 | 0.024375 | 0.010583 | -0.035676 | 0.095480 | -0.051153 | -0.027252 | . L2.program_expenditure -0.274168 | 0.012469 | -0.028204 | 0.060072 | 0.419366 | 0.043190 | -0.266589 | . L1.debt_service -0.138827 | 0.391381 | -0.192682 | 0.456206 | 0.250624 | -2.379002 | 0.182134 | . L2.debt_service -1.839879 | 0.244616 | 0.030729 | -0.321701 | 0.208719 | 0.387627 | 0.026273 | . L1.corporate_income_tax -0.591167 | -0.087419 | -0.166213 | 0.193455 | -0.009423 | -1.069105 | 0.084121 | . L2.corporate_income_tax 0.986182 | -0.171169 | -0.180347 | 0.219469 | -0.439124 | -0.441616 | 0.337464 | . L1.personal_income_tax 0.446547 | -0.050872 | 0.281996 | -0.364826 | -0.291701 | 0.365623 | 0.499642 | . L2.personal_income_tax 0.537147 | 0.068406 | -0.161794 | -0.159843 | -0.598672 | 0.542697 | -0.054906 | . L1.other_revenue 0.319686 | -0.014068 | -0.036482 | -0.021636 | -0.280981 | 0.235076 | -0.052564 | . L2.other_revenue -0.328932 | -0.009975 | -0.017606 | 0.013375 | -0.362590 | 0.255418 | -0.122366 | . results_err.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(&quot;{:.2%}&quot;) . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue nrri nrrd . L1.nrri 6.09% | 42.34% | 12.95% | 12.90% | 50.73% | 26.95% | 75.32% | . L2.nrri 53.56% | 91.63% | 28.56% | 70.91% | 57.36% | 90.54% | 77.87% | . L1.nrrd 54.19% | 25.17% | 96.94% | 26.71% | 60.91% | 74.28% | 66.59% | . L2.nrrd 49.68% | 47.94% | 27.30% | 71.17% | 39.79% | 64.10% | 89.27% | . L1.program_expenditure 47.97% | 28.33% | 84.07% | 55.26% | 57.06% | 71.44% | 88.63% | . L2.program_expenditure 18.15% | 57.39% | 58.33% | 30.58% | 1.07% | 75.16% | 15.20% | . L1.debt_service 93.96% | 4.82% | 67.48% | 38.39% | 86.44% | 5.10% | 91.27% | . L2.debt_service 31.50% | 21.64% | 94.66% | 53.88% | 88.68% | 75.02% | 98.74% | . L1.corporate_income_tax 47.78% | 33.14% | 42.56% | 41.64% | 98.87% | 5.35% | 91.13% | . L2.corporate_income_tax 17.90% | 3.09% | 32.66% | 29.54% | 45.50% | 36.55% | 61.21% | . L1.personal_income_tax 51.65% | 49.41% | 10.20% | 6.37% | 59.67% | 42.44% | 42.34% | . L2.personal_income_tax 39.82% | 31.95% | 30.98% | 37.91% | 23.97% | 19.93% | 92.41% | . L1.other_revenue 16.84% | 57.49% | 53.04% | 74.43% | 13.07% | 12.78% | 80.28% | . L2.other_revenue 18.72% | 71.13% | 77.81% | 85.12% | 6.95% | 12.35% | 58.85% | . Once again, my results with the data error included are much closer to the original paper results. Note that without the data error, program spending is shown to rise in response to an increase or decrease in natural resource revenue, but only on the second lag (at least at a statistically significant level. That&#39;s completely contrary to the main thesis of the paper. Now, given that I&#39;ve shown with the corrected data set that budget deficits are stationary in levels, maybe a more appropriate form of analysis would have been to use a VECM as the paper states it would have been, but from this I can say that the data error has led to a significant change in the outcome of the analysis. . Impulse Response Functions . The actual results of the paper involve taking the estimated impulse response functions derived from the VAR model and examining their implications. Given the results above I&#39;m not sure there&#39;s a lot of value in reproducing all of the other results of this model, but I do want to at least reproduce the IRFs. . def irf_tbl(result: statsmodels.tsa.vector_ar.var_model.VARResults, impulse: str) -&gt; pd.DataFrame: &quot;&quot;&quot;Show the IRF as in table 3 of the paper. Parameters - result: statsmodels.tsa.vector_ar.var_model.VARResults The fitted VAR impulse: str The impulse function, either an increase or decrease in natural resource revenue Returns - pd.DataFrame A summary table &quot;&quot;&quot; irf = result.irf() irf_stderr = irf.stderr() irfs = irf.irfs params = list(results.params.columns) def _impulse_response(impulse: str, response: str) -&gt; pd.DataFrame: &quot;&quot;&quot;Get a specific IRF out of the big array. Parameters - impulse: str The impulse function response: str The response function Returns - pd.DataFrame The 3 period IRF &quot;&quot;&quot; imp_ind = params.index(impulse) res_ind = params.index(response) ir = irfs[:, res_ind, imp_ind] se = irf_stderr[:, res_ind, imp_ind] imp_name = response + &#39;_impulse&#39; se_name = response + &#39;_se&#39; df = pd.DataFrame({imp_name: ir, se_name: se}) return df.loc[1:3].T responses = params[2:] return pd.concat([_impulse_response(impulse, response) for response in responses]) . Table 3 . IMPACTS ON ALBERTA’S BUDGET OF A ONE-DOLLAR INNOVATION IN NON-RENEWABLE-RESOURCE REVENUE (ASYMMETRIC CASE), 1970/71–2016/17 . Image(filename=&quot;img/ferede_tbl_3.png&quot;) . irf_tbl(results, &quot;nrrd&quot;) . 1 2 3 . program_expenditure_impulse 0.131802 | 0.152091 | -0.263057 | . program_expenditure_se 0.219009 | 0.213435 | 0.189561 | . debt_service_impulse -0.034380 | -0.016984 | -0.025572 | . debt_service_se 0.025235 | 0.026153 | 0.023777 | . corporate_income_tax_impulse 0.026578 | 0.022545 | 0.014045 | . corporate_income_tax_se 0.060010 | 0.062050 | 0.049948 | . personal_income_tax_impulse -0.144894 | 0.050801 | 0.023045 | . personal_income_tax_se 0.069135 | 0.090279 | 0.082072 | . other_revenue_impulse 0.193682 | -0.226313 | 0.116648 | . other_revenue_se 0.188179 | 0.190130 | 0.169276 | . irf_tbl(results, &quot;nrri&quot;) . 1 2 3 . program_expenditure_impulse 0.307014 | 0.128759 | 0.032942 | . program_expenditure_se 0.228184 | 0.200129 | 0.172849 | . debt_service_impulse -0.028267 | -0.004321 | -0.011658 | . debt_service_se 0.026292 | 0.026157 | 0.025746 | . corporate_income_tax_impulse 0.081846 | -0.052562 | -0.019982 | . corporate_income_tax_se 0.062524 | 0.057369 | 0.041195 | . personal_income_tax_impulse -0.052581 | -0.027779 | 0.040890 | . personal_income_tax_se 0.072031 | 0.085436 | 0.068562 | . other_revenue_impulse -0.197417 | -0.028068 | 0.187637 | . other_revenue_se 0.196063 | 0.172347 | 0.147472 | . irf_tbl(results_err, &quot;nrrd&quot;) . 1 2 3 . program_expenditure_impulse -0.164689 | 0.190170 | 0.024520 | . program_expenditure_se 0.270039 | 0.264733 | 0.231589 | . debt_service_impulse -0.033454 | -0.029727 | -0.027865 | . debt_service_se 0.029185 | 0.033489 | 0.028892 | . corporate_income_tax_impulse 0.002593 | 0.041965 | 0.055297 | . corporate_income_tax_se 0.067657 | 0.068952 | 0.054178 | . personal_income_tax_impulse -0.085661 | 0.064177 | -0.029006 | . personal_income_tax_se 0.077188 | 0.085444 | 0.072486 | . other_revenue_impulse 0.110608 | -0.211807 | -0.018863 | . other_revenue_se 0.216285 | 0.206303 | 0.197149 | . irf_tbl(results_err, &quot;nrri&quot;) . 1 2 3 . program_expenditure_impulse 0.531582 | -0.303243 | -0.034754 | . program_expenditure_se 0.283645 | 0.282106 | 0.247443 | . debt_service_impulse -0.024542 | -0.003484 | -0.020839 | . debt_service_se 0.030655 | 0.033327 | 0.030126 | . corporate_income_tax_impulse 0.107731 | -0.094389 | -0.033291 | . corporate_income_tax_se 0.071066 | 0.074045 | 0.058701 | . personal_income_tax_impulse -0.123072 | -0.011699 | 0.079664 | . personal_income_tax_se 0.081077 | 0.093111 | 0.078465 | . other_revenue_impulse -0.150617 | -0.053069 | 0.289039 | . other_revenue_se 0.227183 | 0.225741 | 0.202924 | . Same patters as we&#39;ve seen above, the results with the data error included are pretty close to what the paper reports. . Plot the IRFs . Ok, this is the last bit I want to reproduce, just because IRF charts look cool so it would be a shame to leave them out. . Image(filename=&quot;img/ferede_fig_3.png&quot;) . results.irf().plot(impulse=&#39;nrri&#39;, response=&#39;program_expenditure&#39;); . results.irf().plot(impulse=&#39;nrrd&#39;, response=&#39;program_expenditure&#39;); . results_err.irf().plot(impulse=&#39;nrri&#39;, response=&#39;program_expenditure&#39;); . results_err.irf().plot(impulse=&#39;nrrd&#39;, response=&#39;program_expenditure&#39;); . These are harder to eyeball because I&#39;m showing the individual responses and the paper is showing the cumulative ones. But again, if I think about what adding up the points in my charts would look like, they end up closer to the data set with the error. . Conclusion . The initial goal of this exercise was to practice my python and econometrics. I&#39;ve certainly done that over the course of working on it, but as a bonus I&#39;ve also demonstrated how sensitive results can be to even a single data point (at least when you have relatively few samples. . Appendix: Raw tables . For the purposes of validation, here are the full tables I used to produce both the summary statistics above, as well as all statistical models and tests below . mdfl . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue natural_resource_revenue deficit heritage_dummy ur_lag er_lag cad_usd_lag . budget_dt . 1964-01-01 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1965-01-01 2327.844291 | 11.272854 | 174.729232 | 231.093501 | 1442.925275 | 1397.833860 | -907.464724 | 0.0 | 2.500000 | 37.935748 | NaN | . 1966-01-01 2891.984579 | 10.750872 | 145.136773 | 301.024417 | 1542.750138 | 1290.104645 | -376.280521 | 0.0 | 2.500000 | 37.935748 | NaN | . 1967-01-01 4436.875339 | 10.199713 | 203.994268 | 407.988537 | 2351.033944 | 1116.868620 | 367.189683 | 0.0 | 2.500000 | 37.935748 | NaN | . 1968-01-01 4368.535726 | 19.160244 | 239.503055 | 469.425988 | 2639.323668 | 1360.377353 | -320.934094 | 0.0 | 2.700000 | 38.322148 | NaN | . 1969-01-01 4396.849967 | 17.909776 | 286.556413 | 599.977490 | 2444.684401 | 1141.748209 | -58.206771 | 0.0 | 3.300000 | 39.041995 | NaN | . 1970-01-01 4745.762712 | 38.135593 | 245.762712 | 771.186441 | 2737.288136 | 978.813559 | 50.847458 | 0.0 | 3.400000 | 39.833226 | NaN | . 1971-01-01 4993.823780 | 67.916003 | 271.664014 | 767.051333 | 2848.477084 | 1090.651114 | 83.896240 | 0.0 | 5.200000 | 39.686520 | NaN | . 1972-01-01 4988.477844 | 85.114978 | 362.663820 | 858.551083 | 2768.087112 | 1224.915554 | -140.624746 | 0.0 | 5.700000 | 38.456938 | 1.009883 | . 1973-01-01 5004.356719 | 81.151731 | 382.089398 | 977.202089 | 2387.213408 | 2160.664826 | -821.661272 | 0.0 | 5.700000 | 39.470588 | 0.990792 | . 1974-01-01 6124.378806 | 92.793618 | 829.155879 | 1038.689856 | 2352.767225 | 4549.880638 | -2553.321174 | 0.0 | 5.300000 | 40.843443 | 1.000233 | . 1975-01-01 7071.218716 | 84.149869 | 686.347373 | 925.648564 | 2721.722340 | 4417.868145 | -1596.217836 | 0.0 | 3.500000 | 42.840909 | 0.978133 | . 1976-01-01 6845.363213 | 59.113672 | 326.307469 | 1038.036080 | 2830.362613 | -125.320985 | 2835.091707 | 1.0 | 4.200000 | 43.439912 | 1.017267 | . 1977-01-01 7061.151803 | 46.247496 | 554.969954 | 1206.639219 | 2720.193639 | 4513.335196 | -1887.738708 | 1.0 | 3.891667 | 64.800000 | 0.986075 | . 1978-01-01 6825.084687 | 48.260049 | 545.709790 | 1225.062794 | 2643.165786 | 4701.642511 | -2242.236144 | 1.0 | 4.475000 | 64.633333 | 1.063525 | . 1979-01-01 9302.990086 | 32.774318 | 342.491620 | 1265.088664 | 2525.261181 | 5181.619632 | 21.303307 | 1.0 | 4.750000 | 65.341667 | 1.140767 | . 1980-01-01 9937.742745 | 31.376340 | 620.395823 | 1337.773062 | 2676.972321 | 4797.727697 | 536.250183 | 1.0 | 3.966667 | 67.125000 | 1.171558 | . 1981-01-01 10441.551444 | 109.885646 | 701.577586 | 1597.568239 | 5094.588359 | 4001.769572 | -844.066666 | 1.0 | 3.866667 | 68.050000 | 1.169408 | . 1982-01-01 12281.969722 | 57.716023 | 639.073783 | 1731.480694 | 4808.269418 | 2887.899921 | 2272.961929 | 1.0 | 3.883333 | 69.500000 | 1.198892 | . 1983-01-01 11489.190564 | 168.596206 | 771.007210 | 1488.773513 | 4813.372379 | 4001.941517 | 582.692150 | 1.0 | 7.750000 | 66.275000 | 1.233858 | . 1984-01-01 11320.287262 | 218.545766 | 793.666203 | 1396.584127 | 5529.782999 | 4306.693537 | -487.893837 | 1.0 | 11.008333 | 63.816667 | 1.232583 | . 1985-01-01 12869.198063 | 168.831114 | 723.561918 | 1410.945740 | 5622.447161 | 3939.701880 | 1341.372479 | 1.0 | 11.408333 | 63.933333 | 1.295150 | . 1986-01-01 11863.166376 | 263.862833 | 351.817111 | 1570.739021 | 4940.542816 | 1488.115306 | 3775.814955 | 1.0 | 9.775000 | 65.491667 | 1.365900 | . 1987-01-01 10791.884860 | 486.136253 | 505.683690 | 1900.350807 | 5480.081397 | 2231.807343 | 1160.097876 | 0.0 | 10.008333 | 65.350000 | 1.389767 | . 1988-01-01 10774.989850 | 655.047057 | 572.857965 | 1675.835569 | 5818.165763 | 1713.642551 | 1649.535059 | 0.0 | 9.525000 | 65.450000 | 1.326175 | . 1989-01-01 10745.455671 | 865.692419 | 543.484030 | 1968.964999 | 5716.675586 | 1739.148895 | 1642.874581 | 0.0 | 7.983333 | 66.925000 | 1.230942 | . 1990-01-01 10614.752643 | 923.523101 | 578.462597 | 2014.173627 | 5689.536231 | 1936.372929 | 1319.730359 | 0.0 | 7.158333 | 67.516667 | 1.184108 | . 1991-01-01 10060.915107 | 878.291420 | 488.608088 | 2043.330951 | 5298.490171 | 1351.526066 | 1757.251250 | 0.0 | 6.916667 | 67.500000 | 1.167017 | . 1992-01-01 10490.255898 | 920.232018 | 413.099221 | 1811.929709 | 5614.128666 | 1415.691681 | 2155.638638 | 0.0 | 8.250000 | 66.591667 | 1.145975 | . 1993-01-01 9567.673764 | 1046.414892 | 540.289188 | 1820.154561 | 5604.076850 | 1782.195133 | 867.372924 | 0.0 | 9.458333 | 65.383333 | 1.208808 | . 1994-01-01 8304.447335 | 1075.635389 | 661.029079 | 1886.982358 | 5328.892392 | 2081.040289 | -577.861395 | 0.0 | 9.583333 | 64.875000 | 1.290167 | . 1995-01-01 7542.256294 | 1000.994980 | 792.231321 | 1889.578759 | 4888.995090 | 1657.024370 | -684.578266 | 0.0 | 8.783333 | 65.725000 | 1.365892 | . 1996-01-01 7286.510699 | 838.743299 | 807.190029 | 1976.382124 | 4455.321793 | 2314.288966 | -1427.928914 | 0.0 | 7.841667 | 66.650000 | 1.372650 | . 1997-01-01 7598.400009 | 729.331650 | 1020.071271 | 2138.894710 | 4551.426710 | 2084.277589 | -1466.938621 | 0.0 | 6.883333 | 67.316667 | 1.363700 | . 1998-01-01 7647.600932 | 735.120709 | 884.383797 | 2452.712386 | 4366.478407 | 1262.339259 | -583.192208 | 0.0 | 5.875000 | 67.925000 | 1.384867 | . 1999-01-01 8342.393922 | 487.608742 | 640.113987 | 2601.260027 | 4640.443868 | 2371.737084 | -1423.552301 | 0.0 | 5.583333 | 68.566667 | 1.483633 | . 2000-01-01 8710.352822 | 474.863472 | 980.253881 | 1910.598641 | 4348.877202 | 5129.494603 | -3184.008033 | 0.0 | 5.666667 | 68.500000 | 1.485825 | . 2001-01-01 9338.287124 | 360.113309 | 1037.070500 | 1946.193764 | 4320.894451 | 2897.190669 | -502.948950 | 0.0 | 4.966667 | 68.675000 | 1.485517 | . 2002-01-01 8826.496403 | 209.515399 | 888.679810 | 2127.725707 | 3820.134757 | 3138.329395 | -938.857868 | 0.0 | 4.725000 | 68.975000 | 1.549017 | . 2003-01-01 8889.845744 | 112.157737 | 701.917057 | 1909.164731 | 4925.835384 | 3176.836868 | -1711.750559 | 0.0 | 5.341667 | 68.975000 | 1.570592 | . 2004-01-01 9563.476337 | 121.092191 | 947.887219 | 1864.098004 | 5040.562703 | 3907.027522 | -2075.006920 | 0.0 | 5.116667 | 69.741667 | 1.401167 | . 2005-01-01 10252.960247 | 95.080363 | 1118.344428 | 1793.108293 | 5214.467798 | 4829.545684 | -2607.425593 | 1.0 | 4.591667 | 70.125000 | 1.301575 | . 2006-01-01 10491.526962 | 77.006633 | 1291.562414 | 2729.974686 | 5203.857546 | 3943.455955 | -2600.317006 | 1.0 | 3.941667 | 69.850000 | 1.211483 | . 2007-01-01 11093.119793 | 71.131049 | 1560.562037 | 2749.181812 | 4712.930591 | 3359.113940 | -1217.537538 | 1.0 | 3.458333 | 70.791667 | 1.134375 | . 2008-01-01 11481.223206 | 65.508008 | 1339.134853 | 2742.517945 | 3444.209491 | 3752.538047 | 268.330878 | 0.0 | 3.566667 | 71.383333 | 1.074183 | . 2009-01-01 11192.177199 | 111.838586 | 1464.684956 | 2426.866512 | 5009.321141 | 2085.188848 | 317.954328 | 0.0 | 3.691667 | 71.800000 | 1.066767 | . 2010-01-01 11408.370379 | 141.808459 | 1001.672465 | 2292.670240 | 4699.207866 | 2532.122236 | 1024.506031 | 0.0 | 6.575000 | 69.416667 | 1.141442 | . 2011-01-01 11288.615235 | 144.188676 | 1062.777455 | 2474.323912 | 4526.773140 | 3362.283433 | 6.645971 | 0.0 | 6.516667 | 68.133333 | 1.030125 | . 2012-01-01 11491.360391 | 143.201492 | 1330.207614 | 2690.901483 | 4642.860889 | 2175.711738 | 794.880159 | 0.0 | 5.433333 | 69.508333 | 0.989025 | . 2013-01-01 11803.343431 | 158.459375 | 1473.940765 | 2829.977012 | 5288.245930 | 2572.413384 | -202.774285 | 0.0 | 4.700000 | 69.958333 | 0.999408 | . 2014-01-01 11006.721496 | 182.199224 | 1479.028999 | 2817.708454 | 4975.008515 | 2283.359468 | -366.184716 | 0.0 | 4.658333 | 69.591667 | 1.029992 | . 2015-01-01 10664.784806 | 176.142114 | 1040.727000 | 2817.529567 | 4492.864354 | 691.915996 | 1797.890004 | 0.0 | 4.775000 | 69.058333 | 1.104683 | . 2016-01-01 11322.810385 | 230.029051 | 913.571647 | 2608.854241 | 4407.637789 | 750.684901 | 2872.090858 | 0.0 | 6.091667 | 68.408333 | 1.278808 | . 2017-01-01 11433.853017 | 319.856119 | 813.921696 | 2543.505301 | 5094.564399 | 1175.559759 | 2126.157981 | 0.0 | 8.133333 | 66.250000 | 1.325583 | . 2018-01-01 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 7.883333 | 66.316667 | 1.297858 | . model_df_first_diff(mdfl) . natural_resource_revenue nrri nrrd corporate_income_tax personal_income_tax other_revenue debt_service program_expenditure deficit ur_lag er_lag cad_usd_lag heritage_dummy constant . budget_dt . 1970-01-01 -162.934650 | 0.000000 | -162.934650 | -40.793701 | 171.208950 | 292.603734 | 20.225817 | 348.912745 | 109.054229 | 0.100000 | 0.791232 | NaN | 0.0 | 1 | . 1971-01-01 111.837554 | 111.837554 | 0.000000 | 25.901302 | -4.135108 | 111.188949 | 29.780410 | 248.061068 | 33.048782 | 1.800000 | -0.146706 | NaN | 0.0 | 1 | . 1972-01-01 134.264440 | 134.264440 | 0.000000 | 90.999806 | 91.499750 | -80.389972 | 17.198975 | -5.345936 | -224.520986 | 0.500000 | -1.229583 | NaN | 0.0 | 1 | . 1973-01-01 935.749273 | 935.749273 | 0.000000 | 19.425579 | 118.651006 | -380.873704 | -3.963247 | 15.878875 | -681.036526 | 0.000000 | 1.013650 | -0.019092 | 0.0 | 1 | . 1974-01-01 2389.215812 | 2389.215812 | 0.000000 | 447.066481 | 61.487767 | -34.446183 | 11.641888 | 1120.022088 | -1731.659902 | -0.400000 | 1.372855 | 0.009442 | 0.0 | 1 | . 1975-01-01 -132.012493 | 0.000000 | -132.012493 | -142.808507 | -113.041292 | 368.955115 | -8.643749 | 946.839909 | 957.103338 | -1.800000 | 1.997466 | -0.022100 | 0.0 | 1 | . 1976-01-01 -4543.189130 | 0.000000 | -4543.189130 | -360.039903 | 112.387516 | 108.640274 | -25.036197 | -225.855503 | 4431.309543 | 0.700000 | 0.599003 | 0.039133 | 1.0 | 1 | . 1977-01-01 4638.656180 | 4638.656180 | 0.000000 | 228.662485 | 168.603139 | -110.168974 | -12.866176 | 215.788591 | -4722.830415 | -0.308333 | 21.360088 | -0.031192 | 1.0 | 1 | . 1978-01-01 188.307315 | 188.307315 | 0.000000 | -9.260164 | 18.423575 | -77.027853 | 2.012553 | -236.067116 | -354.497436 | 0.583333 | -0.166667 | 0.077450 | 1.0 | 1 | . 1979-01-01 479.977121 | 479.977121 | 0.000000 | -203.218170 | 40.025870 | -117.904605 | -15.485732 | 2477.905398 | 2263.539451 | 0.275000 | 0.708333 | 0.077242 | 1.0 | 1 | . 1980-01-01 -383.891935 | 0.000000 | -383.891935 | 277.904203 | 72.684398 | 151.711140 | -1.397977 | 634.752660 | 514.946876 | -0.783333 | 1.783333 | 0.030792 | 1.0 | 1 | . 1981-01-01 -795.958126 | 0.000000 | -795.958126 | 81.181763 | 259.795177 | 2417.616038 | 78.509306 | 503.808699 | -1380.316848 | -0.100000 | 0.925000 | -0.002150 | 1.0 | 1 | . 1982-01-01 -1113.869651 | 0.000000 | -1113.869651 | -62.503803 | 133.912455 | -286.318941 | -52.169623 | 1840.418278 | 3117.028595 | 0.016667 | 1.450000 | 0.029483 | 1.0 | 1 | . 1983-01-01 1114.041596 | 1114.041596 | 0.000000 | 131.933427 | -242.707181 | 5.102961 | 110.880183 | -792.779158 | -1690.269779 | 3.866667 | -3.225000 | 0.034967 | 1.0 | 1 | . 1984-01-01 304.752020 | 304.752020 | 0.000000 | 22.658992 | -92.189386 | 716.410620 | 49.949560 | -168.903301 | -1070.585987 | 3.258333 | -2.458333 | -0.001275 | 1.0 | 1 | . 1985-01-01 -366.991657 | 0.000000 | -366.991657 | -70.104285 | 14.361613 | 92.664162 | -49.714652 | 1548.910801 | 1829.266316 | 0.400000 | 0.116667 | 0.062567 | 1.0 | 1 | . 1986-01-01 -2451.586574 | 0.000000 | -2451.586574 | -371.744807 | 159.793281 | -681.904345 | 95.031719 | -1006.031688 | 2434.442476 | -1.633333 | 1.558333 | 0.070750 | 1.0 | 1 | . 1987-01-01 743.692037 | 743.692037 | 0.000000 | 153.866579 | 329.611786 | 539.538581 | 222.273420 | -1071.281516 | -2615.717079 | 0.233333 | -0.141667 | 0.023867 | 0.0 | 1 | . 1988-01-01 -518.164793 | 0.000000 | -518.164793 | 67.174276 | -224.515238 | 338.084366 | 168.910804 | -16.895010 | 489.437183 | -0.483333 | 0.100000 | -0.063592 | 0.0 | 1 | . 1989-01-01 25.506344 | 25.506344 | 0.000000 | -29.373936 | 293.129430 | -101.490177 | 210.645362 | -29.534180 | -6.660479 | -1.541667 | 1.475000 | -0.095233 | 0.0 | 1 | . 1990-01-01 197.224034 | 197.224034 | 0.000000 | 34.978568 | 45.208628 | -27.139354 | 57.830682 | -130.703028 | -323.144222 | -0.825000 | 0.591667 | -0.046833 | 0.0 | 1 | . 1991-01-01 -584.846863 | 0.000000 | -584.846863 | -89.854509 | 29.157325 | -391.046061 | -45.231681 | -553.837536 | 437.520891 | -0.241667 | -0.016667 | -0.017092 | 0.0 | 1 | . 1992-01-01 64.165615 | 64.165615 | 0.000000 | -75.508867 | -231.401242 | 315.638496 | 41.940598 | 429.340791 | 398.387388 | 1.333333 | -0.908333 | -0.021042 | 0.0 | 1 | . 1993-01-01 366.503452 | 366.503452 | 0.000000 | 127.189967 | 8.224851 | -10.051816 | 126.182874 | -922.582134 | -1288.265714 | 1.208333 | -1.208333 | 0.062833 | 0.0 | 1 | . 1994-01-01 298.845156 | 298.845156 | 0.000000 | 120.739891 | 66.827798 | -275.184458 | 29.220498 | -1263.226429 | -1445.234319 | 0.125000 | -0.508333 | 0.081358 | 0.0 | 1 | . 1995-01-01 -424.015919 | 0.000000 | -424.015919 | 131.202242 | 2.596401 | -439.897302 | -74.640409 | -762.191041 | -106.716872 | -0.800000 | 0.850000 | 0.075725 | 0.0 | 1 | . 1996-01-01 657.264596 | 657.264596 | 0.000000 | 14.958707 | 86.803365 | -433.673297 | -162.251681 | -255.745595 | -743.350647 | -0.941667 | 0.925000 | 0.006758 | 0.0 | 1 | . 1997-01-01 -230.011377 | 0.000000 | -230.011377 | 212.881242 | 162.512585 | 96.104917 | -109.411649 | 311.889310 | -39.009707 | -0.958333 | 0.666667 | -0.008950 | 0.0 | 1 | . 1998-01-01 -821.938330 | 0.000000 | -821.938330 | -135.687474 | 313.817676 | -184.948302 | 5.789059 | 49.200923 | 883.746412 | -1.008333 | 0.608333 | 0.021167 | 0.0 | 1 | . 1999-01-01 1109.397825 | 1109.397825 | 0.000000 | -244.269810 | 148.547641 | 273.965461 | -247.511966 | 694.792990 | -840.360093 | -0.291667 | 0.641667 | 0.098767 | 0.0 | 1 | . 2000-01-01 2757.757519 | 2757.757519 | 0.000000 | 340.139894 | -690.661386 | -291.566666 | -12.745271 | 367.958900 | -1760.455732 | 0.083333 | -0.066667 | 0.002192 | 0.0 | 1 | . 2001-01-01 -2232.303933 | 0.000000 | -2232.303933 | 56.816619 | 35.595123 | -27.982751 | -114.750162 | 627.934302 | 2681.059082 | -0.700000 | 0.175000 | -0.000308 | 0.0 | 1 | . 2002-01-01 241.138725 | 241.138725 | 0.000000 | -148.390689 | 181.531943 | -500.759694 | -150.597911 | -511.790721 | -435.908918 | -0.241667 | 0.300000 | 0.063500 | 0.0 | 1 | . 2003-01-01 38.507474 | 38.507474 | 0.000000 | -186.762753 | -218.560977 | 1105.700627 | -97.357661 | 63.349341 | -772.892691 | 0.616667 | 0.000000 | 0.021575 | 0.0 | 1 | . 2004-01-01 730.190654 | 730.190654 | 0.000000 | 245.970162 | -45.066727 | 114.727320 | 8.934454 | 673.630593 | -363.256361 | -0.225000 | 0.766667 | -0.169425 | 0.0 | 1 | . 2005-01-01 922.518162 | 922.518162 | 0.000000 | 170.457209 | -70.989711 | 173.905095 | -26.011829 | 689.483910 | -532.418673 | -0.525000 | 0.383333 | -0.099592 | 1.0 | 1 | . 2006-01-01 -886.089729 | 0.000000 | -886.089729 | 173.217986 | 936.866393 | -10.610252 | -18.073730 | 238.566715 | 7.108587 | -0.650000 | -0.275000 | -0.090092 | 1.0 | 1 | . 2007-01-01 -584.342016 | 0.000000 | -584.342016 | 268.999623 | 19.207126 | -490.926955 | -5.875584 | 601.592831 | 1382.779468 | -0.483333 | 0.941667 | -0.077108 | 1.0 | 1 | . 2008-01-01 393.424107 | 393.424107 | 0.000000 | -221.427184 | -6.663868 | -1268.721100 | -5.623041 | 388.103413 | 1485.868417 | 0.108333 | 0.591667 | -0.060192 | 0.0 | 1 | . 2009-01-01 -1667.349199 | 0.000000 | -1667.349199 | 125.550103 | -315.651432 | 1565.111650 | 46.330578 | -289.046007 | 49.623450 | 0.125000 | 0.416667 | -0.007417 | 0.0 | 1 | . 2010-01-01 446.933388 | 446.933388 | 0.000000 | -463.012491 | -134.196272 | -310.113274 | 29.969873 | 216.193180 | 706.551703 | 2.883333 | -2.383333 | 0.074675 | 0.0 | 1 | . 2011-01-01 830.161197 | 830.161197 | 0.000000 | 61.104990 | 181.653671 | -172.434726 | 2.380217 | -119.755144 | -1017.860060 | -0.058333 | -1.283333 | -0.111317 | 0.0 | 1 | . 2012-01-01 -1186.571695 | 0.000000 | -1186.571695 | 267.430159 | 216.577571 | 116.087749 | -0.987183 | 202.745156 | 788.234188 | -1.083333 | 1.375000 | -0.041100 | 0.0 | 1 | . 2013-01-01 396.701645 | 396.701645 | 0.000000 | 143.733151 | 139.075529 | 645.385041 | 15.257883 | 311.983040 | -997.654445 | -0.733333 | 0.450000 | 0.010383 | 0.0 | 1 | . 2014-01-01 -289.053915 | 0.000000 | -289.053915 | 5.088233 | -12.268558 | -313.237415 | 23.739849 | -796.621934 | -163.410431 | -0.041667 | -0.366667 | 0.030583 | 0.0 | 1 | . 2015-01-01 -1591.443473 | 0.000000 | -1591.443473 | -438.301999 | -0.178887 | -482.144162 | -6.057110 | -341.936691 | 2164.074720 | 0.116667 | -0.533333 | 0.074692 | 0.0 | 1 | . 2016-01-01 58.768905 | 58.768905 | 0.000000 | -127.155353 | -208.675326 | -85.226565 | 53.886937 | 658.025579 | 1074.200854 | 1.316667 | -0.650000 | 0.174125 | 0.0 | 1 | . mdfl_err . program_expenditure debt_service corporate_income_tax personal_income_tax other_revenue natural_resource_revenue deficit heritage_dummy ur_lag er_lag cad_usd_lag . budget_dt . 1964-01-01 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1965-01-01 2327.844291 | 11.272854 | 174.729232 | 231.093501 | 1442.925275 | 1397.833860 | -907.464724 | 0.0 | 2.500000 | 37.935748 | NaN | . 1966-01-01 2891.984579 | 10.750872 | 145.136773 | 301.024417 | 1542.750138 | 1290.104645 | -376.280521 | 0.0 | 2.500000 | 37.935748 | NaN | . 1967-01-01 4436.875339 | 10.199713 | 203.994268 | 407.988537 | 2351.033944 | 1116.868620 | 367.189683 | 0.0 | 2.500000 | 37.935748 | NaN | . 1968-01-01 4368.535726 | 19.160244 | 239.503055 | 469.425988 | 2639.323668 | 1360.377353 | -320.934094 | 0.0 | 2.700000 | 38.322148 | NaN | . 1969-01-01 4396.849967 | 17.909776 | 286.556413 | 599.977490 | 2444.684401 | 1141.748209 | -58.206771 | 0.0 | 3.300000 | 39.041995 | NaN | . 1970-01-01 4745.762712 | 38.135593 | 245.762712 | 771.186441 | 2737.288136 | 978.813559 | 50.847458 | 0.0 | 3.400000 | 39.833226 | NaN | . 1971-01-01 4993.823780 | 67.916003 | 271.664014 | 767.051333 | 2848.477084 | 1090.651114 | 83.896240 | 0.0 | 5.200000 | 39.686520 | NaN | . 1972-01-01 4988.477844 | 85.114978 | 362.663820 | 858.551083 | 2768.087112 | 1224.915554 | -140.624746 | 0.0 | 5.700000 | 38.456938 | 1.009883 | . 1973-01-01 5004.356719 | 81.151731 | 382.089398 | 977.202089 | 2387.213408 | 2160.664826 | -821.661272 | 0.0 | 5.700000 | 39.470588 | 0.990792 | . 1974-01-01 6124.378806 | 92.793618 | 829.155879 | 1038.689856 | 2352.767225 | 4549.880638 | -2553.321174 | 0.0 | 5.300000 | 40.843443 | 1.000233 | . 1975-01-01 7071.218716 | 84.149869 | 686.347373 | 925.648564 | 2721.722340 | 4417.868145 | -1596.217836 | 0.0 | 3.500000 | 42.840909 | 0.978133 | . 1976-01-01 6845.363213 | 59.113672 | 326.307469 | 1038.036080 | 2830.362613 | 4887.518397 | -2177.747675 | 1.0 | 4.200000 | 43.439912 | 1.017267 | . 1977-01-01 7061.151803 | 46.247496 | 554.969954 | 1206.639219 | 2720.193639 | 4513.335196 | -1887.738708 | 1.0 | 3.891667 | 64.800000 | 0.986075 | . 1978-01-01 6825.084687 | 48.260049 | 545.709790 | 1225.062794 | 2643.165786 | 4701.642511 | -2242.236144 | 1.0 | 4.475000 | 64.633333 | 1.063525 | . 1979-01-01 9302.990086 | 32.774318 | 342.491620 | 1265.088664 | 2525.261181 | 5181.619632 | 21.303307 | 1.0 | 4.750000 | 65.341667 | 1.140767 | . 1980-01-01 9937.742745 | 31.376340 | 620.395823 | 1337.773062 | 2676.972321 | 4797.727697 | 536.250183 | 1.0 | 3.966667 | 67.125000 | 1.171558 | . 1981-01-01 10441.551444 | 109.885646 | 701.577586 | 1597.568239 | 5094.588359 | 4001.769572 | -844.066666 | 1.0 | 3.866667 | 68.050000 | 1.169408 | . 1982-01-01 12281.969722 | 57.716023 | 639.073783 | 1731.480694 | 4808.269418 | 2887.899921 | 2272.961929 | 1.0 | 3.883333 | 69.500000 | 1.198892 | . 1983-01-01 11489.190564 | 168.596206 | 771.007210 | 1488.773513 | 4813.372379 | 4001.941517 | 582.692150 | 1.0 | 7.750000 | 66.275000 | 1.233858 | . 1984-01-01 11320.287262 | 218.545766 | 793.666203 | 1396.584127 | 5529.782999 | 4306.693537 | -487.893837 | 1.0 | 11.008333 | 63.816667 | 1.232583 | . 1985-01-01 12869.198063 | 168.831114 | 723.561918 | 1410.945740 | 5622.447161 | 3939.701880 | 1341.372479 | 1.0 | 11.408333 | 63.933333 | 1.295150 | . 1986-01-01 11863.166376 | 263.862833 | 351.817111 | 1570.739021 | 4940.542816 | 1488.115306 | 3775.814955 | 1.0 | 9.775000 | 65.491667 | 1.365900 | . 1987-01-01 10791.884860 | 486.136253 | 505.683690 | 1900.350807 | 5480.081397 | 2231.807343 | 1160.097876 | 0.0 | 10.008333 | 65.350000 | 1.389767 | . 1988-01-01 10774.989850 | 655.047057 | 572.857965 | 1675.835569 | 5818.165763 | 1713.642551 | 1649.535059 | 0.0 | 9.525000 | 65.450000 | 1.326175 | . 1989-01-01 10745.455671 | 865.692419 | 543.484030 | 1968.964999 | 5716.675586 | 1739.148895 | 1642.874581 | 0.0 | 7.983333 | 66.925000 | 1.230942 | . 1990-01-01 10614.752643 | 923.523101 | 578.462597 | 2014.173627 | 5689.536231 | 1936.372929 | 1319.730359 | 0.0 | 7.158333 | 67.516667 | 1.184108 | . 1991-01-01 10060.915107 | 878.291420 | 488.608088 | 2043.330951 | 5298.490171 | 1351.526066 | 1757.251250 | 0.0 | 6.916667 | 67.500000 | 1.167017 | . 1992-01-01 10490.255898 | 920.232018 | 413.099221 | 1811.929709 | 5614.128666 | 1415.691681 | 2155.638638 | 0.0 | 8.250000 | 66.591667 | 1.145975 | . 1993-01-01 9567.673764 | 1046.414892 | 540.289188 | 1820.154561 | 5604.076850 | 1782.195133 | 867.372924 | 0.0 | 9.458333 | 65.383333 | 1.208808 | . 1994-01-01 8304.447335 | 1075.635389 | 661.029079 | 1886.982358 | 5328.892392 | 2081.040289 | -577.861395 | 0.0 | 9.583333 | 64.875000 | 1.290167 | . 1995-01-01 7542.256294 | 1000.994980 | 792.231321 | 1889.578759 | 4888.995090 | 1657.024370 | -684.578266 | 0.0 | 8.783333 | 65.725000 | 1.365892 | . 1996-01-01 7286.510699 | 838.743299 | 807.190029 | 1976.382124 | 4455.321793 | 2314.288966 | -1427.928914 | 0.0 | 7.841667 | 66.650000 | 1.372650 | . 1997-01-01 7598.400009 | 729.331650 | 1020.071271 | 2138.894710 | 4551.426710 | 2084.277589 | -1466.938621 | 0.0 | 6.883333 | 67.316667 | 1.363700 | . 1998-01-01 7647.600932 | 735.120709 | 884.383797 | 2452.712386 | 4366.478407 | 1262.339259 | -583.192208 | 0.0 | 5.875000 | 67.925000 | 1.384867 | . 1999-01-01 8342.393922 | 487.608742 | 640.113987 | 2601.260027 | 4640.443868 | 2371.737084 | -1423.552301 | 0.0 | 5.583333 | 68.566667 | 1.483633 | . 2000-01-01 8710.352822 | 474.863472 | 980.253881 | 1910.598641 | 4348.877202 | 5129.494603 | -3184.008033 | 0.0 | 5.666667 | 68.500000 | 1.485825 | . 2001-01-01 9338.287124 | 360.113309 | 1037.070500 | 1946.193764 | 4320.894451 | 2897.190669 | -502.948950 | 0.0 | 4.966667 | 68.675000 | 1.485517 | . 2002-01-01 8826.496403 | 209.515399 | 888.679810 | 2127.725707 | 3820.134757 | 3138.329395 | -938.857868 | 0.0 | 4.725000 | 68.975000 | 1.549017 | . 2003-01-01 8889.845744 | 112.157737 | 701.917057 | 1909.164731 | 4925.835384 | 3176.836868 | -1711.750559 | 0.0 | 5.341667 | 68.975000 | 1.570592 | . 2004-01-01 9563.476337 | 121.092191 | 947.887219 | 1864.098004 | 5040.562703 | 3907.027522 | -2075.006920 | 0.0 | 5.116667 | 69.741667 | 1.401167 | . 2005-01-01 10252.960247 | 95.080363 | 1118.344428 | 1793.108293 | 5214.467798 | 4829.545684 | -2607.425593 | 1.0 | 4.591667 | 70.125000 | 1.301575 | . 2006-01-01 10491.526962 | 77.006633 | 1291.562414 | 2729.974686 | 5203.857546 | 3943.455955 | -2600.317006 | 1.0 | 3.941667 | 69.850000 | 1.211483 | . 2007-01-01 11093.119793 | 71.131049 | 1560.562037 | 2749.181812 | 4712.930591 | 3359.113940 | -1217.537538 | 1.0 | 3.458333 | 70.791667 | 1.134375 | . 2008-01-01 11481.223206 | 65.508008 | 1339.134853 | 2742.517945 | 3444.209491 | 3752.538047 | 268.330878 | 0.0 | 3.566667 | 71.383333 | 1.074183 | . 2009-01-01 11192.177199 | 111.838586 | 1464.684956 | 2426.866512 | 5009.321141 | 2085.188848 | 317.954328 | 0.0 | 3.691667 | 71.800000 | 1.066767 | . 2010-01-01 11408.370379 | 141.808459 | 1001.672465 | 2292.670240 | 4699.207866 | 2532.122236 | 1024.506031 | 0.0 | 6.575000 | 69.416667 | 1.141442 | . 2011-01-01 11288.615235 | 144.188676 | 1062.777455 | 2474.323912 | 4526.773140 | 3362.283433 | 6.645971 | 0.0 | 6.516667 | 68.133333 | 1.030125 | . 2012-01-01 11491.360391 | 143.201492 | 1330.207614 | 2690.901483 | 4642.860889 | 2175.711738 | 794.880159 | 0.0 | 5.433333 | 69.508333 | 0.989025 | . 2013-01-01 11803.343431 | 158.459375 | 1473.940765 | 2829.977012 | 5288.245930 | 2572.413384 | -202.774285 | 0.0 | 4.700000 | 69.958333 | 0.999408 | . 2014-01-01 11006.721496 | 182.199224 | 1479.028999 | 2817.708454 | 4975.008515 | 2283.359468 | -366.184716 | 0.0 | 4.658333 | 69.591667 | 1.029992 | . 2015-01-01 10664.784806 | 176.142114 | 1040.727000 | 2817.529567 | 4492.864354 | 691.915996 | 1797.890004 | 0.0 | 4.775000 | 69.058333 | 1.104683 | . 2016-01-01 11322.810385 | 230.029051 | 913.571647 | 2608.854241 | 4407.637789 | 750.684901 | 2872.090858 | 0.0 | 6.091667 | 68.408333 | 1.278808 | . 2017-01-01 11433.853017 | 319.856119 | 813.921696 | 2543.505301 | 5094.564399 | 1175.559759 | 2126.157981 | 0.0 | 8.133333 | 66.250000 | 1.325583 | . 2018-01-01 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 7.883333 | 66.316667 | 1.297858 | . model_df_first_diff(mdfl_err) . natural_resource_revenue nrri nrrd corporate_income_tax personal_income_tax other_revenue debt_service program_expenditure deficit ur_lag er_lag cad_usd_lag heritage_dummy constant . budget_dt . 1970-01-01 -162.934650 | 0.000000 | -162.934650 | -40.793701 | 171.208950 | 292.603734 | 20.225817 | 348.912745 | 109.054229 | 0.100000 | 0.791232 | NaN | 0.0 | 1 | . 1971-01-01 111.837554 | 111.837554 | 0.000000 | 25.901302 | -4.135108 | 111.188949 | 29.780410 | 248.061068 | 33.048782 | 1.800000 | -0.146706 | NaN | 0.0 | 1 | . 1972-01-01 134.264440 | 134.264440 | 0.000000 | 90.999806 | 91.499750 | -80.389972 | 17.198975 | -5.345936 | -224.520986 | 0.500000 | -1.229583 | NaN | 0.0 | 1 | . 1973-01-01 935.749273 | 935.749273 | 0.000000 | 19.425579 | 118.651006 | -380.873704 | -3.963247 | 15.878875 | -681.036526 | 0.000000 | 1.013650 | -0.019092 | 0.0 | 1 | . 1974-01-01 2389.215812 | 2389.215812 | 0.000000 | 447.066481 | 61.487767 | -34.446183 | 11.641888 | 1120.022088 | -1731.659902 | -0.400000 | 1.372855 | 0.009442 | 0.0 | 1 | . 1975-01-01 -132.012493 | 0.000000 | -132.012493 | -142.808507 | -113.041292 | 368.955115 | -8.643749 | 946.839909 | 957.103338 | -1.800000 | 1.997466 | -0.022100 | 0.0 | 1 | . 1976-01-01 469.650252 | 469.650252 | 0.000000 | -360.039903 | 112.387516 | 108.640274 | -25.036197 | -225.855503 | -581.529839 | 0.700000 | 0.599003 | 0.039133 | 1.0 | 1 | . 1977-01-01 -374.183202 | 0.000000 | -374.183202 | 228.662485 | 168.603139 | -110.168974 | -12.866176 | 215.788591 | 290.008967 | -0.308333 | 21.360088 | -0.031192 | 1.0 | 1 | . 1978-01-01 188.307315 | 188.307315 | 0.000000 | -9.260164 | 18.423575 | -77.027853 | 2.012553 | -236.067116 | -354.497436 | 0.583333 | -0.166667 | 0.077450 | 1.0 | 1 | . 1979-01-01 479.977121 | 479.977121 | 0.000000 | -203.218170 | 40.025870 | -117.904605 | -15.485732 | 2477.905398 | 2263.539451 | 0.275000 | 0.708333 | 0.077242 | 1.0 | 1 | . 1980-01-01 -383.891935 | 0.000000 | -383.891935 | 277.904203 | 72.684398 | 151.711140 | -1.397977 | 634.752660 | 514.946876 | -0.783333 | 1.783333 | 0.030792 | 1.0 | 1 | . 1981-01-01 -795.958126 | 0.000000 | -795.958126 | 81.181763 | 259.795177 | 2417.616038 | 78.509306 | 503.808699 | -1380.316848 | -0.100000 | 0.925000 | -0.002150 | 1.0 | 1 | . 1982-01-01 -1113.869651 | 0.000000 | -1113.869651 | -62.503803 | 133.912455 | -286.318941 | -52.169623 | 1840.418278 | 3117.028595 | 0.016667 | 1.450000 | 0.029483 | 1.0 | 1 | . 1983-01-01 1114.041596 | 1114.041596 | 0.000000 | 131.933427 | -242.707181 | 5.102961 | 110.880183 | -792.779158 | -1690.269779 | 3.866667 | -3.225000 | 0.034967 | 1.0 | 1 | . 1984-01-01 304.752020 | 304.752020 | 0.000000 | 22.658992 | -92.189386 | 716.410620 | 49.949560 | -168.903301 | -1070.585987 | 3.258333 | -2.458333 | -0.001275 | 1.0 | 1 | . 1985-01-01 -366.991657 | 0.000000 | -366.991657 | -70.104285 | 14.361613 | 92.664162 | -49.714652 | 1548.910801 | 1829.266316 | 0.400000 | 0.116667 | 0.062567 | 1.0 | 1 | . 1986-01-01 -2451.586574 | 0.000000 | -2451.586574 | -371.744807 | 159.793281 | -681.904345 | 95.031719 | -1006.031688 | 2434.442476 | -1.633333 | 1.558333 | 0.070750 | 1.0 | 1 | . 1987-01-01 743.692037 | 743.692037 | 0.000000 | 153.866579 | 329.611786 | 539.538581 | 222.273420 | -1071.281516 | -2615.717079 | 0.233333 | -0.141667 | 0.023867 | 0.0 | 1 | . 1988-01-01 -518.164793 | 0.000000 | -518.164793 | 67.174276 | -224.515238 | 338.084366 | 168.910804 | -16.895010 | 489.437183 | -0.483333 | 0.100000 | -0.063592 | 0.0 | 1 | . 1989-01-01 25.506344 | 25.506344 | 0.000000 | -29.373936 | 293.129430 | -101.490177 | 210.645362 | -29.534180 | -6.660479 | -1.541667 | 1.475000 | -0.095233 | 0.0 | 1 | . 1990-01-01 197.224034 | 197.224034 | 0.000000 | 34.978568 | 45.208628 | -27.139354 | 57.830682 | -130.703028 | -323.144222 | -0.825000 | 0.591667 | -0.046833 | 0.0 | 1 | . 1991-01-01 -584.846863 | 0.000000 | -584.846863 | -89.854509 | 29.157325 | -391.046061 | -45.231681 | -553.837536 | 437.520891 | -0.241667 | -0.016667 | -0.017092 | 0.0 | 1 | . 1992-01-01 64.165615 | 64.165615 | 0.000000 | -75.508867 | -231.401242 | 315.638496 | 41.940598 | 429.340791 | 398.387388 | 1.333333 | -0.908333 | -0.021042 | 0.0 | 1 | . 1993-01-01 366.503452 | 366.503452 | 0.000000 | 127.189967 | 8.224851 | -10.051816 | 126.182874 | -922.582134 | -1288.265714 | 1.208333 | -1.208333 | 0.062833 | 0.0 | 1 | . 1994-01-01 298.845156 | 298.845156 | 0.000000 | 120.739891 | 66.827798 | -275.184458 | 29.220498 | -1263.226429 | -1445.234319 | 0.125000 | -0.508333 | 0.081358 | 0.0 | 1 | . 1995-01-01 -424.015919 | 0.000000 | -424.015919 | 131.202242 | 2.596401 | -439.897302 | -74.640409 | -762.191041 | -106.716872 | -0.800000 | 0.850000 | 0.075725 | 0.0 | 1 | . 1996-01-01 657.264596 | 657.264596 | 0.000000 | 14.958707 | 86.803365 | -433.673297 | -162.251681 | -255.745595 | -743.350647 | -0.941667 | 0.925000 | 0.006758 | 0.0 | 1 | . 1997-01-01 -230.011377 | 0.000000 | -230.011377 | 212.881242 | 162.512585 | 96.104917 | -109.411649 | 311.889310 | -39.009707 | -0.958333 | 0.666667 | -0.008950 | 0.0 | 1 | . 1998-01-01 -821.938330 | 0.000000 | -821.938330 | -135.687474 | 313.817676 | -184.948302 | 5.789059 | 49.200923 | 883.746412 | -1.008333 | 0.608333 | 0.021167 | 0.0 | 1 | . 1999-01-01 1109.397825 | 1109.397825 | 0.000000 | -244.269810 | 148.547641 | 273.965461 | -247.511966 | 694.792990 | -840.360093 | -0.291667 | 0.641667 | 0.098767 | 0.0 | 1 | . 2000-01-01 2757.757519 | 2757.757519 | 0.000000 | 340.139894 | -690.661386 | -291.566666 | -12.745271 | 367.958900 | -1760.455732 | 0.083333 | -0.066667 | 0.002192 | 0.0 | 1 | . 2001-01-01 -2232.303933 | 0.000000 | -2232.303933 | 56.816619 | 35.595123 | -27.982751 | -114.750162 | 627.934302 | 2681.059082 | -0.700000 | 0.175000 | -0.000308 | 0.0 | 1 | . 2002-01-01 241.138725 | 241.138725 | 0.000000 | -148.390689 | 181.531943 | -500.759694 | -150.597911 | -511.790721 | -435.908918 | -0.241667 | 0.300000 | 0.063500 | 0.0 | 1 | . 2003-01-01 38.507474 | 38.507474 | 0.000000 | -186.762753 | -218.560977 | 1105.700627 | -97.357661 | 63.349341 | -772.892691 | 0.616667 | 0.000000 | 0.021575 | 0.0 | 1 | . 2004-01-01 730.190654 | 730.190654 | 0.000000 | 245.970162 | -45.066727 | 114.727320 | 8.934454 | 673.630593 | -363.256361 | -0.225000 | 0.766667 | -0.169425 | 0.0 | 1 | . 2005-01-01 922.518162 | 922.518162 | 0.000000 | 170.457209 | -70.989711 | 173.905095 | -26.011829 | 689.483910 | -532.418673 | -0.525000 | 0.383333 | -0.099592 | 1.0 | 1 | . 2006-01-01 -886.089729 | 0.000000 | -886.089729 | 173.217986 | 936.866393 | -10.610252 | -18.073730 | 238.566715 | 7.108587 | -0.650000 | -0.275000 | -0.090092 | 1.0 | 1 | . 2007-01-01 -584.342016 | 0.000000 | -584.342016 | 268.999623 | 19.207126 | -490.926955 | -5.875584 | 601.592831 | 1382.779468 | -0.483333 | 0.941667 | -0.077108 | 1.0 | 1 | . 2008-01-01 393.424107 | 393.424107 | 0.000000 | -221.427184 | -6.663868 | -1268.721100 | -5.623041 | 388.103413 | 1485.868417 | 0.108333 | 0.591667 | -0.060192 | 0.0 | 1 | . 2009-01-01 -1667.349199 | 0.000000 | -1667.349199 | 125.550103 | -315.651432 | 1565.111650 | 46.330578 | -289.046007 | 49.623450 | 0.125000 | 0.416667 | -0.007417 | 0.0 | 1 | . 2010-01-01 446.933388 | 446.933388 | 0.000000 | -463.012491 | -134.196272 | -310.113274 | 29.969873 | 216.193180 | 706.551703 | 2.883333 | -2.383333 | 0.074675 | 0.0 | 1 | . 2011-01-01 830.161197 | 830.161197 | 0.000000 | 61.104990 | 181.653671 | -172.434726 | 2.380217 | -119.755144 | -1017.860060 | -0.058333 | -1.283333 | -0.111317 | 0.0 | 1 | . 2012-01-01 -1186.571695 | 0.000000 | -1186.571695 | 267.430159 | 216.577571 | 116.087749 | -0.987183 | 202.745156 | 788.234188 | -1.083333 | 1.375000 | -0.041100 | 0.0 | 1 | . 2013-01-01 396.701645 | 396.701645 | 0.000000 | 143.733151 | 139.075529 | 645.385041 | 15.257883 | 311.983040 | -997.654445 | -0.733333 | 0.450000 | 0.010383 | 0.0 | 1 | . 2014-01-01 -289.053915 | 0.000000 | -289.053915 | 5.088233 | -12.268558 | -313.237415 | 23.739849 | -796.621934 | -163.410431 | -0.041667 | -0.366667 | 0.030583 | 0.0 | 1 | . 2015-01-01 -1591.443473 | 0.000000 | -1591.443473 | -438.301999 | -0.178887 | -482.144162 | -6.057110 | -341.936691 | 2164.074720 | 0.116667 | -0.533333 | 0.074692 | 0.0 | 1 | . 2016-01-01 58.768905 | 58.768905 | 0.000000 | -127.155353 | -208.675326 | -85.226565 | 53.886937 | 658.025579 | 1074.200854 | 1.316667 | -0.650000 | 0.174125 | 0.0 | 1 | .",
            "url": "https://ianepreston.github.io/iblog/econometrics/jupyter/python/alberta/2021/02/26/ferede.html",
            "relUrl": "/econometrics/jupyter/python/alberta/2021/02/26/ferede.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Traefik Lan",
            "content": "Using traefik for internal services . Using traefik for internal services Introduction | The key configuration | Setting up Traefik | Conclusion | | Introduction . I have a small server at home that I use to run various services in docker containers. I don’t expose any of them to the internet, I can tunnel in through my vpn running on my pfsense router. Because of that up until recently it was enough to just publish the ports of my various services and make a bookmark pointing to my server at that port. Recently I wanted to set up a couple services that both expected to be exposed on port 80, the default http port. Just remapping the port would be insufficient, because their clients did not have options to specify a port to connect on. To get around this issue, I decided to suck it up and learn to use a reverse proxy. I looked into both nginx and traefik and settled on traefik. Unfortunately for me, pretty much all the tutorials online expect you to be exposing traefik to the web and have all sorts of stuff about TLS and letsencrypt that don’t matter to me, and don’t really explain how to do the routing if you only want it to work on your internal network. This guide is for me in the future if I ever have to set this up again, and anyone else that’s interested in a similar setup. . This guide isn’t going to cover what a reverse proxy is, or many of the details of traefik, there are lots of good guides for that online. It’s specifically going to cover the configuration specific to a LAN only connection. . The key configuration . My pfsense box manages DNS and allows me to resolve machines on my network by their name. For instance, my server is named mars and if I run ping mars it will correctly resolve to that machine’s IP. Traefik doesn’t like to play nice with that naming convetion though, at least I couldn’t get it to work with all sorts of combinations of mars and mars.localdomain. Fortunately, I found a forum post that addressed this issue. In my DNS settings I picked a nice sounding domain that I was sure I wouldn’t actually want to connect to (in my case I used ian.ca) and set my DNS to redirect any requests to that domain to go to my server. To do this, from pfsense I went to services -&gt; DNS resolver, and added two lines to the “custom options” section near the bottom . local-zone: &quot;ian.ca&quot; redirect local-data: &quot;ian.ca A &lt;my server IP&gt;&quot; . Setting up Traefik . Most of the rest of the traefik setup could be borrowed from other basic guides. I configure traefik and all my other docker containers in an ansible role. You can find that on my GitHub. A couple small gotchas that I ran into were making sure Traefik was on the same docker network as the containers I wanted it to route, and to remember the difference between exposing ports (available within the docker network) and publishing ports (mapping them to a port on the host). . Conclusion . This was one of those setups that is straightforward in retrospect, but I had to spend a lot of time googling and banging my head against the wall before I could get the correct combination of configurations to do what I want. Hopefully this is useful to others, or at least me next time I try and do this. .",
            "url": "https://ianepreston.github.io/iblog/2021/01/09/traefik-lan.html",
            "relUrl": "/2021/01/09/traefik-lan.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Arch Tldr",
            "content": "Automating provisioning Arch continued - tldr . Automating provisioning Arch continued - tldr Introduction | Arch USB boot | From the live boot Setup WiFi | Make sure partitions are set up | Run the script | | Set up ssh keys | Set up WiFi again | Run Ansible | Setup dotfiles | Conclusion | | Introduction . This note is a reference for me to put together my full provisioning pipeline. . Arch USB boot . Follow The Arch guide . From the live boot . Setup WiFi . In the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue. . iwctl station wlan0 connect &lt;your SSID&gt; # You can enclose it in quotes if it has spaces &lt;enter passphrase&gt; exit dhcpcd wlan0 . Make sure partitions are set up . If you’re not just going to wipe the whole disk you can run lsblk to determine what partitions you have. cfdisk has a nice interface for creating and modifying partitions if necessary. To format the boot partition run: . mkfs.vfat -F32 /dev/&lt;partition&gt; . mkfs.ext4 /dev/&lt;partition&gt; will work for the root partition. . Run the script . bash &lt;(curl -fsSL http://bootstrap.ianpreston.ca) . After that power off, remove the USB and power back on. . Set up ssh keys . Plug in the USB with ssh keys on it. Guide for reference . lsblk # find where the partition with the keys is stored mkdir ssh # make a mount point sudo mount /dev/sd&lt;something&gt; ssh cp -R ssh ssh_local # Have to set permissions on keys (stupid NTFS) cd ssh_local/CA chmod 600 host_ca chmod 600 user_ca cd ../ chmod +x setup_host.sh chmod +x setup_user.sh sudo ./setup_host.sh ./setup_user.sh . Set up WiFi again . nmcli device wifi connect &lt;SSID&gt; password &lt;password&gt; . Run Ansible . The bootstrap script cloned the repository into /srv/recipes. Modify the hosts file in /srv/recipes/ansible/inventor/hosts to include the hostname of the machine you’re setting up in the appropriate categories if you haven’t already. . Run provision_desktop.sh in the ansible folder. It will fail part way through as you won’t have the keys set up for GitHub for your local user. Go through the ssh key generation process again for the newly created user, this will also make a GitHub specific key. Manually add that key to GitHub’s authorized keys and re-run the recipe. I’ve also seen it flake out a few times on particular application installs. Often I can just get past it by running yay -S &lt;application&gt; to manually install the problematic app. At the time of this writing there’s an additional fix required for spotify that’s mentioned on the AUR page for it. I’m not putting that in the recipe as I’m hoping it will be fixed soon. . Setup dotfiles . Log in as your regular user. . cd ~/.dotfiles ./setup.sh rcup -v . Conclusion . Sure, you could just install Ubuntu and be done with it, but where’s the fun in that? Why not spend weeks yak shaving your setup until you’re perfectly happy with it? .",
            "url": "https://ianepreston.github.io/iblog/2020/11/26/arch-tldr.html",
            "relUrl": "/2020/11/26/arch-tldr.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Dotfiles",
            "content": "Automating provisioning Arch continued - dotfiles . Automating provisioning Arch continued - dotfiles Introduction | Repository setup | Repository root files setup.sh | base-rcrc | other root files | | bash completions | distros/manjaro/aliases | linux | macos | windows-wsl | nerdfonts | shared aliases | exports | functions | options | other | third party | | | bin | files/mac and iterm2 | rcs bash_logout | bash_profile | bashrc | other files | config folder | host-* folders | tag-* folders | vifm and vim | | hooks | conclusion | | Introduction . In what is hopefully the last in this series on configuring my computer I’ll go through setting up configuration files (also called dotfiles or rcs). . As with the earlier posts in this series I’ll be copying liberally from Brennan Fee. Since I’m building off his guide I will also use RCM to manage my dotfiles. At the time of this writing I don’t think I need a lot of the tag or host specific tools it offers, and I could probably stick with using my old approach of a bare git repository right in my home. This seems a bit cleaner though, and maybe I’ll want to extend it eventually. The idea of being able to extend things to WSL or MacOS is cool and I could see using host or tag specific features to be able to use these on work machines as well. . I have a hard time remembering why I structure my dotfiles the way I do, or what some features do, so hopefully by writing all this out I will make it easier to update them in the future. As a bonus maybe this will be useful to others looking to configure their own environments. I’m not going to dissect the files in this document, it’s too easy to let things get out of sync between this guide and my actual dotfiles. Instead this will focus on explaining the directory structure and what files do what, since that’s not easily captured by comments. I’ll try to heavily comment the actual files in the repository. That will have a slightly better chance of staying relevant. . Repository setup . Following Brennan’s example, I have two repositories to contain my configs. The first is for configuration I don’t mind sharing and it’s available here. In addition to that I have a second private repository for configurations that contain personal information, which I will not be sharing a link to for obvious reasons. . Just for reference in this guide the public dotfiles repository is cloned to ~/.dotfiles, which is the default location for RCM. The private ones are cloned to ~/.private_dotfiles. That one I’ll have to manually specify, and if I want to add a file to it I’ll have to move it over manually. . Repository root files . setup.sh . This file either links in or generates the config file for RCM (rcrc). This is the file that identifies which tags are applicable for the machine so it has to be configured properly before the rest of the dotfiles can be brought up. The base implementation checks what Operating System you’re running and adds tags for that. At work I have it generate additional tags for which user is running it so I can create user specific tagged files (for things like email addresses). . After running this script, if we didn’t link in an already existing .rcrc file then you’ll have a host specific one generated, but it won’t be saved in the repository, it will just be a reglar file. If as prompted you run mkrc -o ~/.rcrc it will add a host specific rcrc file to the repository. . base-rcrc . This file is used by setup.sh to generate the host specific ~/.rcrc. The script adds tags to this file based on the operating system you’re running. You can add additional tags if you’d like. . other root files . The other files in the root of the repository are generic repository management files. README.md will show on the base of the page on GitHub and should point back to this blog post for more details. I picked GPL V3 for the license somewhat arbitrarily. I think I used the GitHub license picker helper for it. .gitignore and .gitattributes handle files for git to ignore and enforce consistent line break characters. .editorconfig tells a variety of text editors things like whether to use tabs or spaces for indentation. . bash . This folder contains all the stuff that gets loaded into my profile at login. It’s where things like custom functions and the layout of my command prompt are defined. . completions . These scripts let you tab complete commands for certain applications. At the time of this writing I have completions for git, pipx and poetry installed. . distros/manjaro/aliases . I don’t actually use manjaro, but I wanted to keep this in as an example for myself of how to set distribution specific functionality. . linux . This has a few commands to set start or open to run xdg-open in linux. Makes the syntax compatible against platforms. That would be for opening a file in a gui rather than with a command line app. . macos . I don’t have any mac machines to test this stuff out on right now. It’s got a few files that presumably help make behaviour consistent on macs. . windows-wsl . Similar to the mac and linux entries above. Lets you use the same commands regardless of your specific platform. . nerdfonts . This maps a bunch of nerd fonts to environment variables so they can be included in shell scripts. It lets you do things like put a check mark in your command prompt. Very important stuff. . shared . This is where the bulk of the content is in the bash directory. All of these files are cross platform and should work the same on linux, mac or WSL. . aliases . Basically these are all the command shortcuts. For example alias grep=&quot;grep --color&quot; means you can just type grep but get nicely coloured results. . exports . This is where environment variables are set. For example EDITOR=vim is set here. . functions . This is where user defined functions/tools live. For example extract is defined here to call the appropriate underlying app to extract a file based on its extension. . options . Sets a bunch of shell options. Things like turning on vi mode for the command line. . other . A catch all. Code to set up conda, manage the file path, and actually set the appearance of my command prompt all live here. . third party . A place to dump cool code snippets you found on the internet that you want to be able to manage in your shell. . bin . As opposed to the functions in the bash folder that get added to your environment, these are scripts that are supposed to be called directly, and are therefore on the path but not parsed until they’re called. At least I think that’s the distinction. I’m not super good at bash namespaces yet so this might need to be edited. . files/mac and iterm2 . I don’t have a mac, not totally sure what this stuff does. But maybe some day I will! Then it’ll be super nice to have this stuff enabled… I assume. . rcs . This is where the actual config files live . bash_logout . Clear the screen when you log out. I’m not sure if I actually need this, doesn’t seem to hurt . bash_profile . I’m sure in theory there’s a difference between this file and .bashrc but in practice they seem to be the same. Just map this one to load ~/.bashrc so whichever one your terminal expects you get the same result. . bashrc . bashrc configures your shell on login. Brennan has a nice modular design that I’m going to emulate. Basically nothing goes in bashrc itself, rather it walks through all the folders in the previously described bash folder and adds them in (at least those relevant to your Operating System). A snippet of what that looks like is below. . # We want to walk &quot;outside&quot; in... which is to say run all options files first, then all # exports, then all functions, etc. for folder in &quot;options&quot; &quot;exports&quot; &quot;functions&quot; &quot;third-party&quot; &quot;other&quot; &quot;aliases&quot;; do for base in &quot;shared&quot; &quot;$OS_PRIMARY&quot; &quot;distros/$OS_SECONDARY&quot;; do for root in &quot;$DOTFILES/bash&quot; &quot;$DOTFILES_PRIVATE/bash&quot;; do if [[ -d &quot;$root/$base/$folder&quot; ]]; then for file in $root/$base/$folder/*.bash; do # shellcheck source=/dev/null source &quot;$file&quot; done fi done done done . All the actual functionality lives in the bash folders of the dotfiles repositories and only this file itself needs to be linked in by RCM. Distribution and OS specific functionality can be managed by just placing the script in the appropriate folder. Because of the order of execution the more granular files will overwrite more general settings if there’s a conflict. . other files . dircolors: make ls show pretty colours. | gitignore: files and patterns to ignore in all git repositories | inputrc: manage basic keyboard mappings for the shell (home to go to the beginning of the line for example) | prettierrc: configurations for the code formatter prettier. Kind of like black for other languages | tmux.conf: configuration for tmux. I don’t use tmux enough to have strong opinions about these commands so the commenting is pretty sparse at the time of this writing | . config folder . Polite applications store their configuration files here rather than your home directory. The Arch Wiki has a good list of polite applications and how to override some of the impolite ones. The folders all correspond to the name of the application they configure (e.g. git) so they layout is pretty self explanatory. . host-* folders . Host specific configs. Everything within here will have the same layout as the rcs folder above it, but will have machine specific configs. For my setup that’s just the ~/.rcrc file that sets the tags for everything else on the machine. . tag-* folders . The same idea as hosts, except each host can have multiple tags. In general this is used for OS specific configurations. At work I also add tags for each user on the system for things like configuring e-mail addresses. . vifm and vim . These folder should really be under config. They’re just the settings for vim and vifm. Rude of them to demand their own space in ~. . hooks . These just live in the rcs folder, but they’re special so I want to give them their own place. Some configurations need to have setup steps run, either before they’re installed or after. For instance, you can specify which plugins you want vim to use, but they won’t actually be installed until you run vim -N -u &quot;$HOME/.vim/vimrc.bundles&quot; +PlugUpdate +PlugClean! +qa -. You can put a script that does that in hooks/post-up and it will automatically run that after loading in your configuration files. I’m pushing this feature a little beyond what it’s intended to install some user level things like miniconda and pyenv. Doing that with this tool doesn’t quite fit its intended use, but it seems to work so I’ll stick with it. . conclusion . This guide gave an overview of the structure of my dotfiles. For more details on the tool used to set them up check out RCM and for the specifics of the configurations check the files themselves in my repository. .",
            "url": "https://ianepreston.github.io/iblog/2020/11/25/dotfiles.html",
            "relUrl": "/2020/11/25/dotfiles.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Ansible",
            "content": "Automating provisioning Arch continued - Ansible . Automating provisioning Arch continued - Ansible Introduction | Hashed passwords | git clone | dconf | Other resources | Conclusion | | Introduction . My previous post described how to automate a base installation of Arch. This follow up post will give an overview of the next step of configuration. . After getting a base system setup there is still a ton of administrative tasks to do, like creating a user account and installing software. I accomplished this using Ansible. As with the previous post, I borrowed heavily from Brennan Fee for the configuration. My copy is here. This post won’t be as in depth as the previous one, as the ansible syntax is a lot more directly readable, so in most cases it should be enough to look at the code and maybe consult the ansible docs to figure out what’s going on. The sections below will outline a few of the parts that were a little tricky. . Hashed passwords . Ansible lets you create a user and include the hash of their password, which means you can have the data available publicly without a security concern. In order to generate a hash of a password refer to this section of the ansible FAQ . git clone . I had a tricky time with this task. I wanted to clone some repositories I controlled using ssh and save them in my home directory. After a lot of googling I determined that trying to become my user and do the clone directly wouldn’t work because ansible wouldn’t know which key to use (I have a separate key for GitHub than for my local network). This task splits it up by cloning into the ansible user directory and then using the copy task to move them over to my home directory and set the correct permissions. A little hacky, but it worked. . dconf . You can use ansible to configure your GNOME desktop with the dconf module. The trickiest part of that is figuring out what key you have to change. This blog has the solution I used. . dconf dump / &gt; before.txt | make changes in settings or tweak tool | dconf dump / &gt; after.txt | diff before.txt after.txt | Figure out what changed and create a dconf task for it. | . Other resources . Beyond the links previously mentioned I want to highlight a tutorial series from Jeff Geerling which was excellent and informative. He also wrote a book on ansible that I haven’t read yet but imagine is quite good, given the quality of his video guide, and the fact that I found posts from him a few times when I was googling how to do something. . Conclusion . Ansible is a pretty rad way to reproducibly get your desktop environment set up just the way you like it. It’s a bit overkill given what it’s actually designed for, but it’s a handy skill to learn and it saves rebuilding your environment from scratch. .",
            "url": "https://ianepreston.github.io/iblog/2020/11/21/ansible.html",
            "relUrl": "/2020/11/21/ansible.html",
            "date": " • Nov 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Arch Bootstrap",
            "content": "Automating provisioning Arch . Automating provisioning Arch Introduction edit 2020-10-20 | Actual Introduction | | Inspiration | TLDR Setting up WiFi | Make sure partitions are set up | Run the script | Post install | | Setting up for testing Prepping the VM | Getting the bootstrap script to the machine | | Bootstrapping the install Strict mode | Text formatting | Setup paths | Flags and variables | User provided variables | Common helper functions | Verification functions | Prompts / User interaction | Installation/configuration options Partitioning | | Installation | | Conclusion and next steps | | Introduction . edit 2020-10-20 . As I use this script to provision machines I’m going to end up making edits to it. I’m not going to edit this post every time I do that. The latest version of the provision script is always available here. . I’ve also updated the TLDR section a bit based on some experience from the install. That part I will update if I make changes since I use it for reference when building systems. . Actual Introduction . I’ve installed a lot of operating systems a lot of times. The goal of writing out this post is to force me to really think about and clearly document a reproducible workflow for building my workstation. . A secondary goal is to get better at bash. . Inspiration . There are a lot of very smart people out there doing similar things. In the past I’ve used Luke Smith’s LARBS as a base that I forked and modified. There was also this recent discussion on Reddit which pointed me to a couple of very interesting repositories to get inspiration from: Brennan Fee’s provision-arch and Morten Linderud’s PKGBUILDS . TLDR . For future me when I want to actually just install Arch on a system: . Setting up WiFi . In the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue. . systemctl start iwd.service iwctl device list #magically changed my device name to wlan0 here somehow station wlan0 connect &lt;your SSID&gt; # You can enclose it in quotes if it has spaces &lt;enter passphrase&gt; exit dhcpcd wlan0 . That should work, try pinging something just to be safe. . Make sure partitions are set up . I’m a wuss and don’t trust a script to actually create partitions. lsblk will tell you what disks you have. If you need to create/delete partitions before proceeding use cfdisk /dev/sd&lt;letter&gt; to create them. If it’s a completely blank hard drive and you need to create a boot partition make one at the beginning of the disk with 500M of space in cfdisk and then run mkfs.vfat -F32 /dev/sd&lt;letter&gt;1 to format it. There’s probably a cleaner way to clean out the LVMs this script creates but for now it’s easier for me to just blow them away in cfdisk and create a fresh partition to install over. edit: I got braver. The new script has an option to just wipe the whole disk if you want. . Run the script . bash &lt;(curl -fsSL http://bootstrap.ianpreston.ca) . Post install . Get the Wifi going again. It’s a different command than you use from the installer: | . nmcli device wifi connect &lt;SSID&gt; password &lt;password&gt; . Set up ssh keys - plug in the USB | . lsblk # find where the partition with the keys is stored mkdir ssh # make a mount point sudo mount /dev/sd&lt;something&gt; ssh cp -R ssh ssh_local # Have to set permissions on keys (stupid NTFS) cd ssh_local/CA chmod 600 host_ca chmod 600 user_ca cd ../ chmod +x setup_host.sh chmod +x setup_user.sh sudo ./setup_host.sh ./setup_user.sh . After this point you should be able to run ansible to complete the setup. . Setting up for testing . I ended up working on this project over a period of time. Initially I was using a VM in Virtualbox. I document the steps for setting that up below. After a while I ended up putting docker on my Windows machine that I was using for testing, and found it conflicted with Virtualbox, so I switched to Hyper-V. Finally I got my hands on a beater notebook and ended up finishing up on that. . Prepping the VM . First thing to do for any install is download the ISO. I’m going to use Virtualbox as my hypervisor. No particular reason, I’ve just used it in the past and am comfortable with it. . Then I create a base image to work off of. . . . Fire up the new VM, and select the Arch ISO at the prompt: . . After that we’re at the boot prompt. Now comes the fun task of developing a bootstrap script that will automate the install process. . . Getting the bootstrap script to the machine . I’ve created a repository on GitHub to host code like this. I think I could probably just install git on the boot machine, clone the whole repo, navigate to the bootstrap script, and run it. That’s no fun though. Let’s see if I can find a more complicated approach just to save a few keystrokes. . The full URL to the raw script is at this page. That’s pointing to the branch I’m using while I develop the script. When I’ve got it working I’ll hopefully remember to come back here and point it to the master reference. From there I can go to my domain registrar and add a URL redirect record to point an easy to remember subdomain to that path: . . So now bootstrap.ianpreston.ca redirects directly to my shell script. . To actually get the script onto my machine and run it I’ll use curl. The exact syntax will be . bash &lt;(curl -fsSL http://bootstrap.ianpreston.ca) . -f Specifies that the script should fail silently. Otherwise if there’s an http error it will return a 404 page, which I’d then try and run. I’d rather it just not return anything and fail that way. . -L Specifies that if the server reports that the page has moved then curl will redo the request. . -sS Means the script should run silently, unless there’s a failure, in which case it will show the output. -S means show error and -s means silent. . As I’m writing this the shell script doesn’t really do anything, it just prints something out so I know it worked. Here’s where it’s at at this point: . #!/usr/bin/env bash # To install: bash &lt;(curl -fsSL http://bootstrap.ianpreston.ca) echo &quot;We got this far!&quot; . As a quick aside, I don’t write bash scripts often, so I often forget how exactly to set up the shebang. For bash scripts I’ve seen #!/bin/bash and #!/usr/bin/env bash. It seems like in most circumstances they’re interchangeable. This StackOverflow post suggests that the latter is slightly more flexible/portable so I’m going to try and make a habit of using it in my scripts going forward. . Back at the VM I test my overly elaborate bootstrapping setup and… . . Sweet! . Bootstrapping the install . Borrowing liberally from the Arch Install Guide and the previously mentioned arch bootstrap script by Brennan Fee let’s build up a script to auto install Arch. The goal isn’t to do everything with the script, we just need to get to a minimal environment with an SSH server so that Ansible can take over. . Strict mode . The first couple lines of Brennan’s script include a bunch of things I don’t really understand. Since part of the goal of this is learning more bash I’m going to dissect them before moving on. The lines in question are: . SOURCED=false &amp;&amp; [ &quot;${0}&quot; = &quot;${BASH_SOURCE[0]}&quot; ] || SOURCED=true if ! $SOURCED; then set -eEu shopt -s extdebug trap &#39;s=$?; echo &quot;$0: Error on line &quot;$LINENO&quot;: $BASH_COMMAND&quot;; exit $s&#39; ERR IFS=$&#39; n t&#39; fi . Let’s break this up into tiny chunks. SOURCED=false is used to set a shell variable to false. Next up &amp;&amp; is a list operator which will only run the next command if the previous one succeeded. So if we set SOURCED to false successfully then the next command will be executed. . Putting something inside square brackets means to evaluate the expression inside and return success or failure based on that. You can use it for if statements, or use it to only execute a subsequent command based on the result of the conditional. Luke Smith has a good video explaining how to avoid if statements by writing code like the line we’re evaluating. . So what are we actually evaluating in the square brackets? This StackOverflow post explains the difference between ${0} and ${BASH_SOURCE[0]}. I’ll outline the difference below with an example script called a few different ways: . I’ve got a script called experiment.sh which I’ll be running to check out these smaller components of the script . #!/usr/bin/env bash echo &quot;0: [$0] vs bash_source: [${BASH_SOURCE[0]}]&quot; . Here’s the output of running that script a few different ways: . root@archiso ~ # bash ./experiment.sh 0: [./experiment.sh] vs bash_source: [./experiment.sh] root@archiso ~ # ./experiment.sh 0: [./experiment.sh] vs bash_source: [./experiment.sh] root@archiso ~ # . ./experiment.sh 0: [./experiment.sh] vs bash_source: [] root@archiso ~ # source ./experiment.sh 0: [./experiment.sh] vs bash_source: [] . If I execute the script in a subshell, as with the first two examples, then they are equivalent and the expression will evaluate to true. If I source the script - that is I tell it to execute the commands in my current shell, then they will not be equivalent. . Back to the script the shell variable name makes sense now and I can put this all together. Set the shell variable SOURCED to false, check whether the script is being sourced or not, and if it is, update the SOURCED shell variable to true (Since the || operator says to only execute the subsequent command if the previous did not return true). . Inside this block we have some commands that, as described above, will only run if the script is being run from its own subshell. The first command, set -eEu uses the set builtin to change the value of some shell options. -e forces the script to exit if almost any command in the script fails (the link provided above for set includes more details). -E let’s errors that occur within functions be trapped by the shells they inherit from. That’s confusing because it’s how I’d normally expect error handling to work, not a special case. This post explains what’s going on. Finally, -u treats unset variables and parameters as an error. Again, this is what I’d expect a sane language to do by default. I think the normal behavior is to just return an empty string in bash though. Gross. . Next up is shopt -s extdebug. The shopt builtin lets us set additional shell options. The details of extdebug are in the previous link, but basically it allows better error tracing within function calls. . The next line is a bit of error handling as well. trap catches certain signals and runs a command in response. The basic syntax is trap &lt;command to run when caught&gt; &lt;signals to catch&gt;. Looking at trap &#39;s=$?; echo &quot;$0: Error on line &quot;$LINENO&quot;: $BASH_COMMAND&quot; exit $s&#39; ERR that means if/when an ERR signal occurs in the script we’ll set the shell variable s to the exit status of the last task (that’s what $? is), print the filename of the shell script ($0), the line number of the error and it’s command, along with its exit status. Because of the options set above there’s no need to explicitly tell the trap to exit. . The final line in the block changes the internal field separator from the default of &lt;space&gt;&lt;tab&gt;&lt;newline&gt; to &lt;newline&gt;&lt;tab&gt;. The link above explains in general what that’s for. We’ll have to get a bit farther along in the script to see why it’s being used here. . Text formatting . That part was dense. The next few lines are easier: . # Text modifiers Bold=&quot; 033[1m&quot; Reset=&quot; 033[0m&quot; # Colors Red=&quot; 033[31m&quot; Green=&quot; 033[32m&quot; Yellow=&quot; 033[33m&quot; . Text enclosed within $Bold and $Reset will be bolded. Similarly, enclosing within one of the colours and $Reset will set the text to that colour. . Setup paths . The next section sets up a log file: . WORKING_DIR=$(pwd) LOG=&quot;${WORKING_DIR}/arch-install.log&quot; [[ -f ${LOG} ]] &amp;&amp; rm -f &quot;${LOG}&quot; echo &quot;Start log...&quot; &gt;&gt;&quot;${LOG}&quot; . pwd stands for “print working directory”. When you enclose a command in $() it means to take the result of the command. To quickly illustrate, EXAMPLE=pwd would set EXAMPLE to “pwd”, whereas EXAMPLE=$(pwd) would set EXAMPLE to something like /root. The first two lines therefor set the LOG variable to point to a file in the current directory named arch-install.log. . The next line checks if the logfile exists, and deletes it if it does. . The final line writes “Start log…” into the logfile. &gt;&gt; redirects the output of the previous command to the end of the file on the right hand side. Since we know this is a brand new file (because of the line above) this will be the first line of the logfile. . Flags and variables . The next section sets up some system based flags . SYS_ARCH=$(uname -m) # Architecture (x86_64) UEFI=0 KEYMAP=&quot;us&quot; WIFI=0 . uname returns system information, and the -m flag specifies to return the machine hardware. As the comment above describes this will likely return x86_64. Later in the script we’ll check if the system is UEFI or BIOS. . User provided variables . Here we just provide some defaults to variables that the user will set later in the script. . # User provided variables HOST_NAME=&quot;computer&quot; KERNEL_VERSION=&quot;default&quot; MAIN_DISK=&quot;/dev/sda&quot; ROOT_PWD=&quot;&quot; ANSIBLE_PWD=&quot;&quot; . Common helper functions . The next section has a series of small functions that will be used throughout the larger script. Let’s see what they do. The first one is: . print_line() { printf &quot;%$(tput cols)s n&quot; | tr &#39; &#39; &#39;-&#39; |&amp; tee -a &quot;${LOG}&quot; } . The function name gives a pretty solid hint what it does. printf Allows you to print a combination of strings and variables along with specified formatting for the variables. There’s some good docs here. tput provides information about the current terminal, in this case cols says the number of columns that make up a row in the terminal. In my VM tput cols returns 100 so the printf would resolve to printf &quot;%100s n&quot; which means we’ll print spaces across the width of the terminal. |&amp; means to take the output (both from standard error and standard output, as opposed to just | which only return standard output) of the previous command and pass it as an input to the following command. tr in turn replaces the first string with the second, so we turn spaces into dashes, creating a line of dashes across the screen. Finally, that line of dashes is piped to tee -a which sends its input both to standard output and a file. The -a flag means to append the output to the file rather than overwriting it. All of that to say this function prints a line of dashes across your screen and into the logfile we defined above. . Next up we have blank_line, which based on the explanation above is pretty self explanatory. . blank_line() { echo -e &quot; n&quot; |&amp; tee -a &quot;${LOG}&quot; } . Next up is print_title . print_title() { clear print_line echo -e &quot;# ${Bold}$1${Reset}&quot; |&amp; tee -a &quot;${LOG}&quot; print_line echo &quot;&quot; |&amp; tee -a &quot;${LOG}&quot; } . clear clears the screen. Everything else has been explained except $1 which is just the first argument passed to the function. This means calling print_title &quot;This is the title&quot; would clear the screen, print a line of dashes to the screen and log file, print This is the title to the screen and log file, another line, and then start at the beginning of a new line for whatever text follows. . After that is print_title_info . print_title_info() { T_COLS=$(tput cols) echo -e &quot;${Bold}$1${Reset} n&quot; | fold -sw $((T_COLS - 18)) | sed &#39;s/^/ t/&#39; |&amp; tee -a &quot;${LOG}&quot; } . echo just prints some text, the -e flag tells it to interpret escaped characters, so t will show a tab rather than the literal t. That gets piped to fold, which wraps the text at 18 characters less than the width of the terminal. -s tells it to wrap at the last whitespace before the column limit (so don’t wrap in the middle of a word) and w is how you specify the column width to wrap on. After that we pipe the output to sed which I frankly find intimidating. This one’s not so bad though. &#39;s/&lt;pattern&gt;/&lt;other pattern&gt;/&#39; just performs a find replace of &lt;pattern&gt; for &lt;other pattern&gt; in each line of text that’s passed in. The patterns can be regular expressions, which I also find intimidating to work with, but this one is just saying to replace the beginning of the line (that’s what ^) means with a tab. Not so terrible. . The next several functions repeat the general concepts above, just with different formatting (red font for errors for example) so I won’t reproduce them here. . Next up is pause_function . pause_function() { print_line read -re -sn 1 -p &quot;Press enter to continue...&quot; } . In the original code the read line was in an if block that was based on a variable that wasn’t set anywhere in the script. I assume that was planned to build fully unattended builds eventually, but I took it out for now at least. read receives input from the user. -re specifies not to allow backslashes to escape characters and to use Readline to obtain the line in an interactive shell. -s tells read not to echo input to the terminal, n 1 tells it how many characters of input to wait for. -p tells it what text to prompt with. . Next up is arch-chroot . arch_chroot() { arch-chroot /mnt /bin/bash -c &quot;${1}&quot; |&amp; tee -a &quot;${LOG}&quot; } . This is cool, when I’ve tried to build my own version of this in the past I broke things up into a pre and post chroot because I couldn’t figure out how to get my script to change contexts. This one does it by just sending the commands one at a time into the chrooted environment. Neat! . The final helper is is_package_installed . is_package_installed() { #check if a package is already installed for PKG in $1; do pacman -Q &quot;$PKG&quot; &amp;&gt;/dev/null &amp;&amp; return 0 done return 1 } . pacman -Q searches for a package matching the subsequent argument. If it finds it it will return its full name and version. If it can’t it will return an error. So this function will return 0 if any packages are found, and 1 if none of them are. . Verification functions . These are also helper functions, but they’re specifically designed to make sure the script is being run from the correct environment. . First up is check_root . check_root() { print_info &quot;Checking root permissions...&quot; if [[ &quot;$(id -u)&quot; != &quot;0&quot; ]]; then error_msg &quot;ERROR! You must execute the script as the &#39;root&#39; user.&quot; fi } . id -u returns the user id. Since Root is always user 0 on a system we know this isn’t being run as root and the script will fail. . Next up is check_archlinux . check_archlinux() { if [[ ! -e /etc/arch-release ]]; then error_msg &quot;ERROR! You must execute the script on Arch Linux.&quot; fi } . -e &lt;file&gt; checks if a file exists, so if /etc/arch-release does not exist (it’s an empty file on the USB boot system) then we’re not on Arch and had better exit. . Next up is check_boot_system . check_boot_system() { if [[ &quot;$(cat /sys/class/dmi/id/sys_vendor)&quot; == &#39;Apple Inc.&#39; ]] || [[ &quot;$(cat /sys/class/dmi/id/sys_vendor)&quot; == &#39;Apple Computer, Inc.&#39; ]]; then modprobe -r -q efivars || true # if MAC else modprobe -q efivarfs # all others fi if [[ -d &quot;/sys/firmware/efi/&quot; ]]; then ## Mount efivarfs if it is not already mounted # shellcheck disable=SC2143 if [[ -z $(mount | grep /sys/firmware/efi/efivars) ]]; then mount -t efivarfs efivarfs /sys/firmware/efi/efivars fi UEFI=1 else UEFI=0 fi } . The purpose of this section is to verify the boot mode. Pretty much any system I can imagine installing on these days will be UEFI, but it doesn’t hurt to check. I’m not totally sure what the first little bit is doing, and I don’t have a mac to test. The -r flag is to remove a module from the kernel, rather than add it like the normal command. modprobe -q efivarfs will add the efivarfs module to the kernel, and fail quietly if it can’t find that module (that’s the -q flag). As described in the install guide, if you have a /sys/firmware/efi/ directory, which is what the first block of the second if statement is checking, then your system is EFI. The next part describes what it’s going to do (mount efivarfs if it’s not already mounted), but let’s dig into how it does that. -z returns true if the length of an evaluated string is zero. mount without any arguments returns all mountpoints in the system. We pipe that into grep which will return /sys/firmwar/efi/efivars if it’s mounted and an empty string if not, which accomplishes the goal. The last part of the script sets the variable UEFI to identify if the system is EFI or BIOS. . Next up is check_wifi . check_wifi() { has_wifi=($(ls /sys/class/net | grep wlan)) if [ -n &quot;$has_wifi&quot; ]; then WIFI=1 fi } . As per the Arch Wikie /sys/class/net lists all network devices. So if I list that directory and match on wlan then I know there’s a wireless device. I’ll use this to determine whether or not to install wireless tools when loading software. . Prompts / User interaction . The first prompt asks for a hostname for the system. It had some code for auto naming that I trimmed out. The rest of it is pretty self explanatory: . ask_for_hostname() { print_title &quot;Hostname&quot; print_title_info &quot;Pick a hostname for this machine. Press enter to have a random hostname selected.&quot; read -rp &quot;Hostname [ex: archlinux]: &quot; HOST_NAME if [[ $HOST_NAME == &quot;&quot; ]]; then HOST_NAME=&quot;arch-$((1 + RANDOM % 1000)).tts.lan&quot; fi } . The read command takes inputs, the -r flag prevents special characters from being included, and -p displays the text that follows as a prompt without a newline before taking the input. . The last block says if the input is empty to give a hostname like arch-[random number 1-1000].tts.lan. . Next up we’re determine which hard disk to install on. Note that this isn’t handling partitioning or anything yet. . ask_for_main_disk() { print_info &quot;Determining main disk...&quot; devices_list=($(lsblk --nodeps --noheading --list --exclude 1,11,7 | awk &#39;{print &quot;/dev/&quot; $1}&#39;)) if [[ ${#devices_list[@]} == 1 ]]; then device=${devices_list[0]} else print_title &quot;Main Disk Selection&quot; print_title_info &quot;Select which disk to use for the main installation (where root and boot will go).&quot; lsblk --nodeps --list --exclude 1,11,7 --output &quot;name,size,type&quot; blank_line PS3=&quot;Enter your option: &quot; echo -e &quot;Select main drive: n&quot; select device in &quot;${devices_list[@]}&quot;; do if contains_element &quot;${device}&quot; &quot;${devices_list[@]}&quot;; then break else invalid_option fi done fi MAIN_DISK=$device } . The first line calls lsblk to list all available block devices. --nodeps tells it not to show holder devices, so for example if I have an sda device with two partitions - sda1 and sda2 it will only show sda, which is what we want since we’re just picking the disk itself at this stage. --noheading drops column headers, which we want since we’re just going to parse this list. --list produces the output as a list, which we want in order to make a list of potential disks to select. --exclude 1,11,17 tells it not to list RAM or optical drive devices. The output of that list is piped into awk so that /dev/ can be prepended to it. . The next line says that if the array has only one entry (there’s only one disk available) then we just use that device. If we have more than one device the script prints out a list of them using the same command used to build the list of available disks but showing column headers and including some details to help select the correct disk. . PS3 sets the prompt used by the select command. The select statement has you pick a device and loops if you haven’t selected one of the options in the list until you do. Finally we set MAIN_DISK to the device we want to install on. . The original script has some similar functions to pick a second disk, but I’m not going to be using that option so I’m omitting it. . There are a few other selection scripts (to set a root password and kernel version for example), but there’s nothing new in terms of BASH in them so I’ll omit them from this post. . Installation/configuration options . Now we get to functions that actually help with the installation and configuration of the system. . First is configure_mirrorlist . configure_mirrorlist() { print_info &quot;Configuring repository mirrorlist&quot; pacman -Syy |&amp; tee -a &quot;${LOG}&quot; # Install reflector pacman -S --noconfirm reflector |&amp; tee -a &quot;${LOG}&quot; print_status &quot; Backing up the original mirrorlist...&quot; rm -f &quot;/etc/pacman.d/mirrorlist.orig&quot; |&amp; tee -a &quot;${LOG}&quot; mv -i &quot;/etc/pacman.d/mirrorlist&quot; &quot;/etc/pacman.d/mirrorlist.orig&quot; |&amp; tee -a &quot;${LOG}&quot; print_status &quot; Rotating the new list into place...&quot; # Run reflector /usr/bin/reflector --score 100 --fastest 20 --age 12 --sort rate --protocol https --save /etc/pacman.d/mirrorlist |&amp; tee -a &quot;${LOG}&quot; # Allow global read access (required for non-root yaourt execution) chmod +r /etc/pacman.d/mirrorlist |&amp; tee -a &quot;${LOG}&quot; # Update one more time pacman -Syy |&amp; tee -a &quot;${LOG}&quot; } . pacman -Syy says to sync all available packages from the master repository. The -S flag is for sync, and yy forces a refresh even if the list appears to be up to date. . Next the script installs reflector in order to update and optimize the list of mirrors that will be used for downloading packages. . The rest of the script is pretty well commented and straightforward. . Partitioning . The script I’m templating off of is designed to wipe an entire disk. I generally dual boot Windows so I definitely don’t want that option. In light of that I had to tweak this section a fair bit. Rather than wiping the whole disk and creating new partitions like the template script, I want to identify an existing boot and linux partition and install to them. See the TLDR section for setting up a fresh disk. If you’ve already installed Arch on the disk you’re targeting you should also clear out the partition with the LVMs on it and start with a blank slate. That’s not done by the script, check the TLDR section for how to manage that. . find_install_partition() { print_title &quot;Installation partition selection&quot; print_title_info &quot;Select the partition to install Arch. This should be an already existing boot partition. If you don&#39;t see what you expect here STOP and run cfdisk or something to figure it out.&quot; partition_list=($(lsblk $MAIN_DISK --noheading --list --output NAME | awk &#39;{print &quot;/dev/&quot; $1}&#39; | grep &quot;[0-9]$&quot;)) blank_line PS3=&quot;Enter your option&quot;: lsblk $MAIN_DISK --output NAME,FSTYPE,LABEL,SIZE echo -e &quot;select a partition&quot; select partition in &quot;${partition_list[@]}&quot;; do if contains_element &quot;$partition&quot; &quot;${partition_list[@]}&quot;; then break else invalid_option fi done INSTALL_PARTITION=$partition } . This works similarly to the main disk selection, except I formatted the output slightly differently. The one completely new command I added was the last | grep &quot;[0-9]$&quot;. That filters the output to only show entries that end with a number ($ means end of line). I couldn’t figure out a way to have lsblk not list the block device (the opposite of what we wanted in the main disk selection) so I filter it out. As an example if I have a disk /dev/sda with partitions sda1, sda2, sda3 before the grep I’d get: . /dev/sda /dev/sda1 /dev/sda2 /dev/sda3 . I really don’t want to accidentally try and make a partition on the whole device, so I filter that out so the list is just: . /dev/sda1 /dev/sda2 /dev/sda3 . There’s a practically identical function called find_boot_partition that does the same thing but identifies the boot partition for installation. . The next step is to create a physical and logical volume for the operating system using LVM . setup_lvm() { print_info &quot;Setting up LVM&quot; pvcreate $INSTALL_PARTITION vgcreate &quot;vg_main&quot; $INSTALL_PARTITION lvcreate -l 5%VG &quot;vg_main&quot; -n lv_var lvcreate -l 45%VG &quot;vg_main&quot; -n lv_root lvcreate -l 40%VG &quot;vg_main&quot; -n lv_home } . This one’s actually pretty readable. We create a physical volume on the install partition identified in the previous section, create a virtual group on it, and then create logical volumes within that. . Next we format and mount the partitions: . format_partitions() { print_info &quot;Formatting partitions&quot; mkfs.ext4 &quot;/dev/mapper/vg_main-lv_var&quot; mkfs.ext4 &quot;/dev/mapper/vg_main-lv_root&quot; mkfs.ext4 &quot;/dev/mapper/vg_main-lv_home&quot; } mount_partitions() { print_info &quot;Mounting partitions&quot; # First load the root mount -t ext4 -o defaults,rw,relatime,errors=remount-ro /dev/mapper/vg_main-lv_root /mnt # Create the paths for the other mounts mkdir -p &quot;/mnt/boot/efi&quot; mkdir -p &quot;/mnt/var&quot; mkdir -p &quot;/mnt/home&quot; if [[ $UEFI == 1 ]]; then mount -t vfat -o defaults,rw,noatime,utf8,errors=remount-ro &quot;${MAIN_DISK}1&quot; &quot;/mnt/boot/efi&quot; fi # Mount others mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_var /mnt/var mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_home /mnt/home } . Again, most of this is pretty readable. The one that I didn’t know about was the noatime option. In the original script it was set to relatime, but after reading this post it seems like I want noatime to improve the life of my SSD. . Installation . The install_base_system function doesn’t really introduce any new bash stuff, which was my main goal in writing out how this all worked line by line. I’ll present it below without further comment. . install_base_system() { print_info &quot;Installing base system&quot; pacman -S --noconfirm archlinux-keyring |&amp; tee -a &quot;${LOG}&quot; # Install kernel case &quot;$KERNEL_VERSION&quot; in &quot;lts&quot;) pacstrap /mnt base base-devel linux-lts linux-lts-headers linux-firmware |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above.&quot; ;; &quot;hard&quot;) pacstrap /mnt base base-devel linux-hardened linux-hardened-headers linux-firmware |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above.&quot; ;; *) pacstrap /mnt base base-devel linux linux-headers linux-firmware |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above.&quot; ;; esac # Install file system tools pacstrap /mnt lvm2 dosfstools mtools gptfdisk |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above. Part 4.&quot; # Install networking tools pacstrap /mnt dialog networkmanager networkmanager-openvpn |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above. Part 5.&quot; if [[ $WIFI == 1 ]]; then pacstrap /mnt iwd |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above. Wifi&quot; fi # Remaining misc tools pacstrap /mnt reflector git gvim openssh ansible terminus-font systemd-swap |&amp; tee -a &quot;${LOG}&quot; [[ $? -ne 0 ]] &amp;&amp; error_msg &quot;Installing base system to /mnt failed. Check error messages above. Part 6.&quot; # Add the ssh group arch_chroot &quot;groupadd ssh&quot; # Set the NetworkManager &amp; ssh services to be enabled arch_chroot &quot;systemctl enable NetworkManager.service&quot; arch_chroot &quot;systemctl enable wpa_supplicant.service&quot; arch_chroot &quot;systemctl enable sshd.service&quot; } . Next up we have some user account setup and configuration for ansible, which will be used for the rest of the configuration of the machine. The original script added some public keys to the ansible user’s authorized keys file. I’d like to add some automation to handle my key management approach but the VM I’m working in makes USB passthrough a hassle. (I switched to Hyper-V part way through making this guide as Virtualbox and WSL2 were fighting on my machine). . There’s a script for updating the root user account, but it has a subset of what’s in the ansible account, so let’s just look at that one: . setup_ansible_account() { print_info &quot;Setting up Ansible account&quot; arch_chroot &quot;useradd -m -G wheel -s /bin/bash ansible&quot; arch_chroot &quot;echo -n &#39;ansible:$ANSIBLE_PWD&#39; | chpasswd -c SHA512&quot; arch_chroot &quot;chfn ansible -f Ansible&quot; mkdir -p /mnt/home/ansible/.ssh chmod 0700 /mnt/home/ansible/.ssh arch_chroot &quot;chown -R ansible:ansible /home/ansible/.ssh&quot; # Add user to the ssh arch_chroot &quot;usermod -a -G ssh ansible&quot; } . useradd does about what you’d expect. -m creates a home directory for that user if it doesn’t exist. -G is followed by a list of groups you want the user to be a part of. In this case we want ansible to be part of wheel so it can perform actions with elevated privileges. chpasswd is a pretty cool way to set a user password from a script without user interaction, man pages here. chfn is used to change user info, in this case to give the ansible user the first name Ansible. . Conclusion and next steps . After running this script you have a bare bones Arch install. The next step is to create users and install all the software and configurations you need to get a functioning system. This post is already getting really long so I’m going to break that part up into a future post. .",
            "url": "https://ianepreston.github.io/iblog/2020/10/14/arch-bootstrap.html",
            "relUrl": "/2020/10/14/arch-bootstrap.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Docker Compose",
            "content": "Notes on docker-compose . Notes on docker-compose Introduction | Different file names | .env files | Conclusion | | Introduction . This is going to be a grab bag of docker-compose tips and snippets for things that I do commonly enough that I want to write them down but not commonly enough that I remember the syntax offhand. . Different file names . By default docker-compose wants the compose file to be docker-compose.yml in the same directory as the command is being run. Generally you want to stick with that, but I did come up with a situation where I wanted to give them different names. You can do docker-compose -f &lt;file name&gt; &lt;rest of your commands&gt; to get around this. Note that the -f &lt;file name&gt; has to be at the start, you can’t just put it in like any other flag. . .env files . One of the reasons I was trying to have different file names was because I wanted a bunch of different compose files to share a .env file. My new solution is to have a .env file in the root directory of where I keep my compose files and then use symbolic links to make a link to that master file in each directory. Depending on how I created the link though I would run into a too many levels of symbolic links error. A clean way to solve this is to navigate to the subdirectory and run ln -s ../.env .env. Whatever you put after -s is literally what’s included in the link, so as long as there’s a .env file in the parent folder this will work, regardless of where in your file system you move these directories. . Conclusion . That’s it for now. I’ll come back and update this document if anything else comes up. .",
            "url": "https://ianepreston.github.io/iblog/2020/10/06/docker-compose.html",
            "relUrl": "/2020/10/06/docker-compose.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Pi Bluetooth",
            "content": "Connecting a Harmony remote to a raspberry pi . Connecting a Harmony remote to a raspberry pi Introduction | Get the Harmony Bluetooth ID | Create a device in Harmony | Connect from the pi | | Introduction . This is a little stub post for me. I have a Logitech Harmony remote that I use to control a raspberry pi running Kodi for my media center. Occasionally I have to reinstall it and I always have to google a few things to remember how to do it, so this is just to consolidate that info. . Get the Harmony Bluetooth ID . Since I don’t have a GUI on my pi beyond Kodi, I have to do all the connecting over the command line. Which means it will be much easier if I have the device ID for the remote available. To find it I pair the remote with a Windows machine, and then from Control Panel Hardware and Sound Devices and Printers I can right click on the device, bring up its properties, and from the bluetooth tab get the unique identifier. For my remote that’s 00:04:20:f8:65:d1. . Create a device in Harmony . From the Harmony app go to create a new device. Under the manufacturer choose Microsoft and for the device choose Kodi. I always forget this part and just try and set it up as a generic PC, which doesn’t work. When it gets to the pairing part of the setup we can connect from the pi . Connect from the pi . SSH into the pi: . sudo bluetoothctl agent on default-agent pair 00:04:20:f8:65:d1 connect 00:04:20:f8:65:d1 trust 00:04:20:f8:65:d1 . That should do it. Harmony will say it’s connected, you can also check with sudo bluetoothctl paired-devices. .",
            "url": "https://ianepreston.github.io/iblog/2020/07/20/pi-bluetooth.html",
            "relUrl": "/2020/07/20/pi-bluetooth.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Pypack",
            "content": "Python packaging . Python packaging Intro Background | My current packaging approach | The problem | What I’m hoping to do here | How this will progress | | The process Preliminary setup Create repository | Set up environment | Make my super sweet library | | Run into problems | Some bad ways to solve the problem Just copy the file everywhere | Add it to the path | | Get hypermodern | Turn our code into a poetry package Poetry init | src layout | poetry install | poetry build test the build | | | Automate testing Add a pytest dependency | Write tests | | Publish to pypi Set up for publishing | Publish | Pull it back down and test | | Publish to a private repository | Adding dependencies | Now do conda Sort of working build Issues with this build | Fixing the issues Adding a Makefile | | | Publish to a public channel | Publish to a private channel | | Put it all together Clean slate | Full build chain | | | Conclusion and next steps. | Resources I’ve consulted | | Intro . Probably the best way to introduce this post is to explain a bit of my background, and then describe the problem I’m trying to solve. . Background . I have been using python for data analysis work since about 2017, so around 3 years at the time of writing this post. I work on a small team, and so it’s necessary for us to be able to share code for things like implementing business logic, or connecting to internal data sources. I also maintain an open source package called stats_can that can be used to access Statistics Canada datasets in python. . My current packaging approach . The current way my team shares code is by having a repository with a lib folder in it, and adding that folder to the PYTHONPATH environment variable in Windows. . The current way I build new versions of stats_can is through a cargo cult sequence of steps that I kind of sort of understand. . The problem . For the shared team library all of our stuff is basically in one giant package, broken up into subpackages. This leads to all sorts of problems: . It’s very difficult to write tests for it. | There’s no version numbering so it’s impossible to pin code at a particular version. | We can’t share it easily with other teams, and we really can’t share just one particular subpackage of it with other teams. | The whole thing just feels very wrong to me. I knew it wasn’t the way to go when I set it up, but I was very new to python and just didn’t have the experience/capacity to find a better way and it worked for the time being. | . For stats_can my current system more or less works, it just has two problems: . I only build conda packages. I’d like to allow pip users to access it but… | Like I said, the build process is a bit of a house of cards that I barely understand, so adding in another build steps scares me. | . Both of the examples described above are for libraries. I’ve built a couple of small apps, but have even less of an idea the correct way to build/deploy them. . What I’m hoping to do here . Basically I want to figure out the current best practice way to do the following: . build a library package with versions that can be installed with pip and conda | deploy those packages to both a privately hosted repository (for work specific stuff) as well as pypi and Anaconda Cloud or conda-forge for public open source stuff | Originally I was also going to include building user facing (web or CLI) apps but this got pretty long already so I think I’m going to leave that for another post | Ditto for CI/CD, linting, extensive testing, and all the other things that go into managing a project. Too big to include in this post. | . So a library with conda and pip packages, hosted both publicly and privately means four total ways to manage the library. . How this will progress . I find that most of the packaging guides I’ve read show either how to build a completely trivial project that demonstrates one narrow feature, or some giant project that’s a lot to take in all at once. My aim is to start from a single file script and gradually build it up to the final product that I laid out in the what I’m trying to accomplish section. I’ll host the repositories for the library/app on GitHub, and use tags in order to mark the progress of the project through various stages. . The process . Preliminary setup . Create repository . The first step in any project is to make a repository. This one has the uncreative name of ianlibdemo. If you want to follow along at home you can clone it and check out the tag for the associated stage in the tutorial. The state of the repository right after being created in this case can be accessed with git checkout eg01 . Set up environment . So I have somewhere to work from, and also to make this process reproducible for others the next thing I have to do is create an isolated python environment to work in. I’m a conda user so I’ll create an environment.yml file: . name: ianlibdemo_conda_env dependencies: - python . Then I’ll create the environment with conda env create -f environment.yml. . There’s absolutely nothing to this environment, which is kind of the point. . Make my super sweet library . Enough talk! Let’s write some code! Well, actually, I’m not going to write any code. The point of this tutorial is to build a package, not write a super awesome library, so I’m just going to copy the demo project used in SciPy 2018 - the sheer joy of packaging. The original code is here. Basically what the module does is take a text file and output a copy with all the words capitalized (except a specified subset). . In the root directory of the repository I’ll copy the capital_mod.py file and cap_data.txt. I’ll also create an example_in.txt file that I can use to manually test the capitalize function. . Now I have the following files in my repository: . $ ls __pycache__/ capital_mod.py example_in.txt LICENSE cap_data.txt environment.yml README.md . I can test the “package” out from the interactive prompt: . $ python -i Python 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import capital_mod &gt;&gt;&gt; capital_mod.get_datafile_name() WindowsPath(&#39;C:/Users/ianep/Documents/ianlibdemo/cap_data.txt&#39;) &gt;&gt;&gt; capital_mod.capitalize(&quot;example_in.txt&quot;, &quot;example_out.txt&quot;) &gt;&gt;&gt; quit() . Everything looks like it ran fine, and if I check in the directory I have file example_out.txt that is indeed a capitalized version of example_in.txt. If you want to get your repository to this point run git checkout eg02. . So everything works great and we can go home, right? . Run into problems . This is all well and good, but I don’t just want to use this functionality in this folder. The idea is that this is a utility library. Presumably there are all sorts of scripts that I want to add this file capitalization capability to. Maybe I have coworkers I want to share this with, or use it in an app I’m building. As it stands how can I accomplish this? . Some bad ways to solve the problem . Just copy the file everywhere . Fine. It only works from the local directory? I’ll just put a copy of it everywhere I want it. This is pretty clearly a bad idea. It will be annoying to copy the file into every location I might want to use it, if I ever have to update the functionality I will then have to track down every instance of that file and make the change repeatedly, and it violates DRY so any experienced developer that sees me do it will make fun of me. Better not do it this way. . Add it to the path . This is already going to be a really long guide so I don’t want to add too much about the python path directly. This guide by Chris Yeh is the best I’ve found on the python path and import statements, so if you’re curious by all means check that out. Briefly though, let’s demonstrate the two ways we could directly add this “package” to the path, and therefore run it without being in the same directory. . To set the stage I’ve created a new directory separate from the package, and created a text file that I will try and capitalize: . (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ ls demo_in.txt (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ cat demo_in.txt i want to capitalize this text file, but it&#39;s in the wrong folder. oh no! . If I just try and do the same steps I did from within the folder it will fail: . &gt;&gt;&gt; import capital_mod Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; ModuleNotFoundError: No module named &#39;capital_mod&#39; . That’s because the folder with capital_mod.py is not on my path. . One way I can solve this is by adding the path to capital_mod.py to my path. Like so: . $ export PYTHONPATH=&quot;/c/Users/Ian/Documents/ianlibdemo&quot; (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ python -i Python 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:34:03) [MSC v.1916 64 bit (AMD64)] on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import capital_mod &gt;&gt;&gt; capital_mod.get_datafile_name() WindowsPath(&#39;C:/Users/Ian/Documents/ianlibdemo/cap_data.txt&#39;) &gt;&gt;&gt; capital_mod.capitalize(&quot;demo_in.txt&quot;, &quot;demo_out.txt&quot;) &gt;&gt;&gt; quit() (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ cat demo_out.txt I Want to Capitalize This Text File, But It&#39;s In the Wrong Folder. Oh No! . This worked, but I don’t want to have to run that export command every time before I run a script, and sharing this code with other people and telling them to do that every time seems like a hassle. There are ways to permanently add folders to your python path. This guide covers them nicely. But we’re not actually going to go this route so let’s move on. . The slightly less hacky way is to use sys.path from within a python script. Back in my demo directory I can write a python script that looks like this: . import sys sys.path.append(r&quot;C: Users Ian Documents ianlibdemo&quot;) import capital_mod capital_mod.capitalize(&quot;demo_in.txt&quot;, &quot;demo_out.txt&quot;) . We can see that this works as well: . (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ ls demo_in.txt syspathdemo.py (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ python syspathdemo.py (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ ls demo_in.txt demo_out.txt syspathdemo.py (ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp $ cat demo_out.txt I Want to Capitalize This Text File, But It&#39;s In the Wrong Folder. Oh No! . This also worked, but I had to import sys, and I had to know the exact path to the library. It’s going to be annoying to have to put that in every script, and if I try and share this code with anyone else they’re going to have to modify it to point to wherever they’ve saved my library code. . Get hypermodern . As I was working on this guide I discovered a series of articles by Claudio Jolowicz called Hypermodern Python. The series is an opinionated (in a good way) look at how to configure a python project in 2020. It’s excellent and well worth a read, but I can’t completely adopt its recommendations for two related reasons. The first is that it assumes you’re either using a *NIX system or can load WSL2 on your Windows machine. For my work setup neither of those assumptions hold. It also assumes you’re working in the standard python ecosystem and therefore doesn’t reference conda either for environment management or packaging. For the remainder of this guide I’m going to try and follow Claudio’s suggestions where possible, but adapt them to incorporate conda. . Turn our code into a poetry package . Poetry seems to be the current best practice for building python packages. Let’s see if we can get it working with conda. . Poetry init . After adding poetry as a dependency to my conda environment and updating the environment I run poetry init: . $ poetry init This command will guide you through creating your pyproject.toml config. Package name [ianlibdemo]: Version [0.1.0]: Description []: Python packaging - how does it work? Author [[Ian Preston] &lt;17241371+ianepreston@users.noreply.github.com&gt;, n to skip]: Ian Preston License []: GPL-3.0-or-later Compatible Python versions [^3.7]: Would you like to define your main dependencies interactively? (yes/no) [yes] no Would you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no Generated file [tool.poetry] name = &quot;ianlibdemo&quot; version = &quot;0.1.0&quot; description = &quot;Python packaging - how does it work?&quot; authors = [&quot;Ian Preston&quot;] license = &quot;GPL-3.0-or-later&quot; [tool.poetry.dependencies] python = &quot;^3.7&quot; [tool.poetry.dev-dependencies] [build-system] requires = [&quot;poetry&gt;=0.12&quot;] build-backend = &quot;poetry.masonry.api&quot; Do you confirm generation? (yes/no) [yes] yes . At the end of this process I have a pyproject.toml file in the root of my repository with the text listed above inside. . src layout . The root folder of this repository is getting crowded. I’ve got various files that either describe the project or the environment I’m supposed to work on it in intermingled with the actual source code for the package. To address this I’ll make a separate folder for the actual package files, and as recommended by hypermodern python I’ll use src layout . poetry install . The last step for a basic install is to use poetry to install the package into the environment. Since poetry 1.0 it should be able to detect conda environments and do its installation directly into them based on this PR. . $ poetry install Updating dependencies Resolving dependencies... (0.1s) Writing lock file No dependencies to install or update - Installing ianlibdemo (0.1.0) . Seems to work, let’s try that old example that wouldn’t run before: . (ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo $ ls example_in.txt (ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo $ python -i Python 3.7.7 (default, May 6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; from ianlibdemo.capital_mod import capitalize &gt;&gt;&gt; capitalize(&quot;example_in.txt&quot;, &quot;example_out.txt&quot;) &gt;&gt;&gt; quit() (ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo $ cat example_in.txt these words will all get capitalized, except the ones in that super special text file, like is, or, and a. (ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo $ cat example_out.txt These Words Will All Get Capitalized, Except the Ones In That Super Special Text File, Like Is, Or, And A. . Magic! Note that I have to do one more layer of importing from the ianlibdemo package whereas before I was directly importing the capital_mod module, but otherwise we’re gold. . Of course this hasn’t really solved my problem yet, I still don’t have an actual package that other people can install. But still, progress! . poetry build . It turns out that making it to the previous step was essentially all I needed to create a pip installable package. Just running poetry build from the root of the repository creates a dist folder containing a sdist and a wheel . test the build . Having built this package, how would I install it? . To start the test I’ll create a new empty conda environment and make sure I can’t import the ianlibdemo package. . $ conda create -n pyonly python ... $ conda activate pyonly $ python -i &gt;&gt;&gt; import ianlibdemo Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; ModuleNotFoundError: No module named &#39;ianlibdemo&#39; . This verifies that I have a clean environment without that package installed. I can use pip to install it like so: . $ pip install /c/tfs/ianlibdemo/dist/ianlibdemo-0.1.0-py3-none-any.whl $ python -i Python 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type &quot;help&quot;, &quot;copyright&quot; &gt;&gt;&gt; import ianlibdemo . The import ran successfully. I haven’t done a lot of validation that the package works the way I’d expect, but I’ll get to that when we set up testing later. Note that I installed the package using the .whl file that the build process created, but I could have also used the .tar.gz file in the same folder just as easily. . Since we’ve now built a working package this seems like another good place for a checkpoint. To see the state of the project at this point you can run git checkout eg03. . Automate testing . This is already going to be a big post so I’m definitely not going to offer extensive notes on testing, but I’d like to include enough to at least ensure it integrates with the rest of the process, and to save manually testing after each step. . Add a pytest dependency . We want to use pytest for testing, so the first step is to add it as a development dependency. Normally this would be a simple one liner, poetry add --dev pytest, but because of this bug between conda and poetry, at least at the time of this writing I had to install an update of msgpack before I could get it to run. I’ve amended the environment.yml file to include this fix, so between that and hopefully this bug being resolved in time this shouldn’t be an issue for anyone else following this guide, I just wanted to flag what I encountered and how I resolved it. . Write tests . Now in the base of the repository we add a tests folder and add an empty __init__.py file and a test_capitalize.py file. The test file looks like this: . from ianlibdemo import capital_mod def test_capitalize_file(tmp_path): in_file = tmp_path / &quot;in_file.txt&quot; in_content = &quot;this is the lowercase input sentence&quot; in_file.write_text(in_content) out_file = tmp_path / &quot;out_file.txt&quot; out_content = &quot;This is the Lowercase Input Sentence n&quot; # Output shouldn&#39;t exist before we call the function assert not out_file.exists() capital_mod.capitalize(in_file, out_file) assert out_file.exists() assert out_file.read_text() == out_content . Now from the base directory of the repository I can run pytest with poetry run pytest. . To see the project at this stage run git checkout eg04. . Publish to pypi . I’ve built a package, I can test that it works, the next step is to publish it somewhere for others to access. The defacto source for python packages is PyPi. However, since this is just a demo package I don’t want to publish it there, since it will just add clutter. Fortunately, there is a similar location designed exactly for testing out publishing packages, appropriately named Test PyPi. . Set up for publishing . In order to publish packages you need an account. The registration process is straightforward. Note that pypi and test pypi use completely separate databases, and you will need an account for each of them. For now we’re just publishing to test pypi so it’s not an issue, but just something to keep in mind. . Next I want to create an API token. You can just use your username and password to authenticate and publish packages, but tokens are the preferred method. Once you’re logged in you can click on your account, go to account settings, and under API tokens click “add API token”. Give it a descriptive name and save it somewhere secure (I put mine in a LastPass note). As they warn on the page it will only be displayed once, and if you lose it you’ll have to delete it and create a new one. . Now we need to set up the test pypi repository in poetry. From the poetry docs you can see that repositories are added to your poetry config: . poetry config repositories.testpypi https://test.pypi.org/legacy/ poetry config pypi-token.testpypi &lt;your api key&gt; . Note that these configurations are global to poetry, so they’re not saved in your repository. If you switch machines, or (I think) change conda environments since we installed poetry with conda you’ll have to redo these configurations. . Publish . Once this is set up publishing is quite straightforward. If you haven’t already built the package do so with poetry build and then run poetry publish --repository testpypi. . . Look at that! There it is! . Pull it back down and test . Let’s just make sure that all worked. . First make a clean conda environment with just pytest for testing and activate it: . conda create -n test_env pytest conda activate test_env . Navigate to the root of your package folder and try running tests. They should fail, because we don’t have the package installed in this environment: . cd ~/Documents/ianlibdemo pytest . . . tests test_capitalize.py:1: in &lt;module&gt; from ianlibdemo import capital_mod E ModuleNotFoundError: No module named &#39;ianlibdemo&#39; . Now pip install that package and try running tests again: . pip install --index-url https://test.pypi.org/simple/ ianlibdemo pytest . . . ======================== 1 passed, 1 warning in 0.09s ========================= . Looks good! . Publish to a private repository . Not all of the code we develop should be published on the public internet. Some of it you just want accessible to an internal team. I have a private package index running using this docker container - setting that up will be its own post. Once you have that all set up though the process is exactly the same as for the public pypi so I’ll leave it at that for this guide. . None of the steps used to publish this package required changes to the library repository, so you can still use git checkout eg04 to view the state of the repository at this point. . Adding dependencies . One thing I realized I should ensure is that all of this works with libraries that depend on other libraries. Let’s add a dependency on pandas and give that a shot. . Fortunately adding a dependency is easy. Since I want to require pandas I just run poetry add pandas and it’s now a dependency. I added a module called fun_pandas and a test for it in my tests suite. After that I rebuilt the package and uploaded it to a repository as described above, pulled it back down and tested it like before and everything worked! It’s nice when that happens. . To see the project at this stage you can run git checkout eg05. . Now do conda . The next thing I want to work out is how to build a conda package. The first step is to add conda-build to my environment. The next step is to define a meta.yaml file to specify how to do the build. . Sort of working build . Rather than just dump the final working file, I think it will be useful to step through from the first version I got working to the final one I’m happy with. A lot of the steps for setting this up are hacky, so seeing what doesn’t work is as important as seeing what does for people who are trying to figure out how to apply this to their own projects. . Here’s the first version of my meta.yaml that actually built: . {% set name = &quot;ianlibdemo&quot; %} {% set version = &quot;0.2.0&quot; %} package: name: &quot;{{ name|lower }}&quot; version: &quot;{{ version }}&quot; source: path: ./dist/{{ name }}-{{ version }}-py3-none-any.whl build: script: &quot;{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv &quot; requirements: host: - pip - python run: - python - pandas test: imports: - ianlibdemo . From an environment with conda-build installed I can build a package by running conda-build . from the base of the repository. It creates a conda package as a tar.bz2 file in a deeply nested directory. From there I can install it into an environment with something like: . conda install /c/Users/e975360/.conda/envs/conda_build_test/conda-bld/win-64/ianlibdemo-0.2.0-py38_0.tar.bz2 . Running pytest in an environment with that package installed resulted in one passed test and one failure for the one requiring pandas. As we’ll see below, that issue will get solved if I can load it to a package repository so I’ll leave that alone at this point. . Issues with this build . First off, note that it’s called meta.yaml not meta.yml. Despite .yml being the common and preferred extension for this file type (see this SO thread) it has to end with .yaml or conda-build can’t find it. | Also note that I’m pointing it to the .whl file that I built with poetry, rather than the .tar.gz that’s in the same folder. In theory I should be able to do either, and most examples online point to .tar.gz files, but I got errors about not having poetry in my build environment, and when I tried to add poetry I got a version conflict because apparently the main conda repository only has the python2.7 version of poetry and… it just seemed easier to use the .whl. | It makes a build that claims to be specific to windows and python 3.8 when in fact this should run on any OS and any python 3. | I have to repeat the file name in two places | I’m specifying the version number in two places now since it’s already in the pyproject.toml file. There’s a risk of them getting out of sync | Similar to the version number I have to specify dependencies in this file as well as pyproject.toml (pandas in this case). Unfortunately, since conda packages can have slightly different names than their pypi counterparts, and I have to actually specify python as a dependency here I don’t think there’s an automated way to keep these in sync. Fortunately I don’t expect dependencies to change as often as the package version so this will be less of a burden to manage. | To do anything with the created package I have to scroll up through a big install log and find the path to the file | I get a bunch of build environments and intermediate files created on my machine (maybe this is why the build guide suggests using docker). | . Fixing the issues . Setting the build to work for any OS and python is an easy fix. Under the build section you just add one line. The build section now looks like this: . build: noarch: python script: &quot;{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv &quot; . Defining the package file in once place is similarly easy. Jinja lets you concatenate variables with the ~ symbol. The updated relevant section looks like this: . {% set name = &quot;ianlibdemo&quot; %} {% set version = &quot;0.2.0&quot; %} {% set wheel = name ~ &quot;-&quot; ~ version ~ &quot;-py3-none-any.whl&quot; %} package: name: &quot;{{ name|lower }}&quot; version: &quot;{{ version }}&quot; source: path: ./dist/{{ wheel }} build: noarch: python script: &quot;{{ PYTHON }} -m pip install ./{{ wheel }} --no-deps --ignore-installed -vv &quot; . Adding a Makefile . The rest of the issues outlined above aren’t directly the result of the meta.yaml file. To resolve them I’ll need to write some scripts, and to tie that all together I’ll use my good friend Make. . To begin I add some boilerplate to the beginning of the file to handle conda environments . # Oneshell means I can run multiple lines in a recipe in the same shell, so I don&#39;t have to # chain commands together with semicolon .ONESHELL: # Need to specify bash in order for conda activate to work. SHELL=/bin/bash # Note that the extra activate is needed to ensure that the activate floats env to the front of PATH CONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate ENV_NAME = ianlibdemo_conda_env . Next I create a python script that will read the version number from pyproject.toml and update the version in meta.yaml with it. I won’t reproduce that script here but it’s in the scripts folder of the ianlibdemo repository. . Finally I add a target to sync the versions. I can then make that a pre-requisite of building the conda package. . .PHONY: versionsync versionsync: $(CONDA_ACTIVATE) $(PROJECT_NAME) python scripts/version_sync.py . .PHONY: means that target should be run each time it’s called. By default Make won’t redo a target if an output file already exists. . Now running make versionsync from the root of the repository will take the version from pyproject.toml and put it in meta.yaml. Eventually I’ll also want to ensure that the python package has been built by poetry before building the conda package. . PS: I documented how you can activate conda environments from within makefiles and bash scripts here. Since I had to refer back to it when doing this I thought it would be helpful to include a pointer. . The next issue I described above is that running conda-build generates the package in some obscure subdirectory and you have to scroll back up through the log file to find it. If I want to upload the package to a repository or install it directly that’s going to be a hassle. Fortunately conda-build comes with a --output flag that you can run to return where your package file would be saved if you actually ran conda-build. Knowing this I can write a small bash script which first builds the package and then uses the --output flag to find the generated package and copy it into my dist directory. . The new part of the Makefile looks like this: . conda: $(CONDA_ACTIVATE) $(ENV_NAME) bash scripts/conda_build.sh . And the bash script looks like this: . #!/bin/bash conda-build . CONDA_PACK=$(conda-build . --output) cp $CONDA_PACK dist/ . I’m going to make a cleanup function later to remove all build artifacts so we’ll leave that alone for now. . Publish to a public channel . To publish to an external public conda channel I have to install the anaconda-client package in my environment. The first time I do an upload I will need to log in with anaconda login and provide my username and password. . After that I can add a new recipe to my makefile to publish the package: . conda_ext_pub: conda $(CONDA_ACTIVATE) $(ENV_NAME) anaconda upload $$(conda-build . --output) . conda_ext_pub depends on the conda recipe so this will build the package first and then upload it to Anaconda.org. After running make conda_ext_pub I can see that the package was indeed published to Anaconda.org: . . As with the previous installations I can create a new blank environment with just pytest installed, install this package into it with conda install -c ian.e.preston ianlibdemo and now both my tests pass, as pandas is installed as well. . Publish to a private channel . As with the other private repository, actually setting up the repository is outside the scope of this post. This will assume that you have one created and that packages are stored on some sort of file share that you can access from your build machine. There’s no fancy way to publish conda packages to a private repository. You just drop the package file in the appropriate architecture subfolder (noarch in this case since this is a pure python package) and then run conda index on the repository folder. My server has a file watcher that detects changes and auto runs that, so all we have to do to publish a package is to make sure it’s in the right place. In this example the file share from my local machine is at r4001 finpublic FP&amp;A channel_test noarch and the web server is available at http://dml01:8081/. . To set up publishing I add the following to my makefile: . CONDA_LIB_DIR = //r4001/finpublic/FP &amp;A/channel_test/noarch . . . conda_int_pub: conda $(CONDA_ACTIVATE) $(ENV_NAME) cp $$(conda-build . --output) $(CONDA_LIB_DIR) . After that I can install the package into a library by running conda install -c http://dml01:8081 ianlibdemo. . To see the project at this stage you can run git checkout eg06. . Put it all together . All of the pieces are here, so the final thing to do is to put them all together. I started that process in the last section by creating a makefile, now I just have to finish it up by tying the pip packaging and publishing in with the conda packaging and publishing. . Clean slate . After a package file is built and published we don’t really have any further need for it locally, but it’s not automatically deleted. Let’s make a clean task in Make that will clear out any previous builds. That way any new process can start fresh. . The clean task looks like this: . clean: # remove pip packages rm -rf ./dist/* # remove conda packages and build artifacts $(CONDA_ACTIVATE) $(ENV_NAME) bash scripts/conda_clean.sh . and conda_clean.sh looks like this: . #!/bin/bash export CONDA_BLD_PATH=${CONDA_PREFIX}/conda-bld rm -rf $CONDA_BLD_PATH . Full build chain . The last step is to add make tasks to build and publish the pip packages and set them as appropriate dependencies for the conda steps. . First, add a task to build the pip installable package: . pip: clean $(CONDA_ACTIVATE) $(ENV_NAME) poetry build . Next add tasks to publish to external and internal pip sources: . pip_ext_pub: pip $(CONDA_ACTIVATE) $(ENV_NAME) poetry publish --repository testpypi pip_int_pub: pip $(CONDA_ACTIVATE) $(ENV_NAME) poetry publish --repository localpypi . Finally as an example we can make wrapper tasks that will publish pip and conda packages to external/internal sources: . all_int_pub: pip_int_pub conda_int_pub echo &quot;publishing to internal conda and pip repository&quot; all_ext_pub: pip_ext_pub conda_ext_pub echo &quot;publishing to external conda and pip repository&quot; . At this point if you want to build and publish your package you can just run make all_int_pub and it will clear out old build artifacts, build a new pip installable package, upload it to the internal pip package repository, sync the version number with conda, build a conda package and publish that to the internal conda package repository. Not bad! . This is concludes the changes I’m planning to make in this repository. If you just clone the repository as is you should see it in this state, or you can run git checkout eg07. . Conclusion and next steps. . This guide demonstrated how to turn some python code into an installable package, and distribute that package to internal and external users via pip or conda. At the end of this you should be able to reproduce this process for your own project. But there’s always more to do, so what are some next steps to think about? . First of all, a lot of what we’ve done to set this project up would be broadly applicable to any library built under similar circumstances. It’d be a shame to have to rewrite or copy paste that Makefile into every library you build with minor alterations for example. It would be a good idea to use a templating tool like cookiecutter to automate the files and folder structure that will be consistent across projects. Stay tuned, I’m working on putting that together next. . Next, there’s still lots of aspects of developing and maintaining a library that we haven’t touched. Things like linting, testing, coverage reporting… Take a look at the rest of the Hypermodern Python series for some ideas there. . Finally, I haven’t described how you actually set up an internal package repository for conda or pip packages. I’ll have a follow up post on that coming soon too. . Resources I’ve consulted . This section will serve as a link dump for things I’ve referenced while going through this process. In no particular order they are: . SciPy 2018 talk - the sheer joy of packaging | What the heck is pyproject.toml? | A tutorial on packaging up your python code for pypi | conda build docs | Python packaging in 2020 | Package python projects the proper way with poetry | poetry2conda | Hypermodern Python | Host your own index | .",
            "url": "https://ianepreston.github.io/iblog/2020/07/09/pypack.html",
            "relUrl": "/2020/07/09/pypack.html",
            "date": " • Jul 9, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Conda",
            "content": "Trouble with reproducible conda environments . Trouble with reproducible conda environments Introduction | The question | | Introduction . I’m having trouble making reproducible conda environments. I’ve posted the question below on Stack Overflow and Reddit but I’ve got nothing. I’m leaving the question here for easy future reference. If I come up with a solution I’ll update this post. . The question . Hi everyone. . I’m really struggling to create a reproducible conda environment. I’ll outline the approach I’ve taken so far and the issue I’ve encountered. I’d appreciate any tips for what I can do to troubleshoot next or resources I could check. . As background, I work on a small team, and want to be able to share a copy of an environment I’ve been using with other members of my team so I can be sure we have identical versions of all the libraries required for our work. . My current workflow is as follows: . Write out an environment file with unpinned dependencies and let conda build the environment | . name: example_env_build channels: - conda-forge - defaults dependencies: - pandas - requests . The actual environment has a lot more stuff in it, but that’s the idea . I then create the environment with conda env create -f example_env_build.yml | I export the environment so that all versions and their dependencies will be pinned with conda env export -n example_env_build --no-builds --file test_export.yml. I added --no-builds because I was finding the certain builds were getting marked as broken and causing issues and getting the version right seemed close enough for my purposes. | I edit the test_export.yml file and change the name to example_env and remove the prefix line from the bottom. | I build a new environment with this pinned file just to make sure it goes ok, and then share the file with the rest of my team. | . This has generally worked well if everyone tries to build the environment relatively quickly after the file is created. However, the whole point of being able to specify a reproducible environment is that I should be able to recreate that environment at any time. Someone on my team recently got a new computer so I was trying to help her set up her environment and ran into a series of conflicts. To troubleshoot I tried to rebuild the environment on my machine and ran into the same situation. . For troubleshooting I did the following: . Clone my environment so I have a backup while I mess around conda create --name example_env_clone --clone example_env | Export the environment conda env export -n example_env --no-builds --file example_env_rebuild.yml | Delete the example environment so I can rebuild it conda env remove --name example_env | Try and recreate the environment I just exported conda env create -f example_env_rebuild.yml | . From there I ran into all sorts of version conflicts. I don’t understand this because a) These are all versions being used in a working environment and b) a lot of the “conflicts” don’t seem to be conflicts to me. As an example, here’s one from my current attempt: . Package phik conflicts for: phik=0.9.10 pandas-profiling=2.4.0 -&gt; phik[version=&#39;&gt;=0.9.8&#39;] . I picked that one basically at random but there are tons like that. As I read it I’m trying to install phik 0.9.10, and pandas-profiling requires &gt;=0.9.8, which 0.9.10 satisfies. . I’m at my wits end here. I’ve read a million “how to manage conda environments” guides (For example this, this, and this) along with the conda environment management docs. All of them seem to indicate what I’m doing should work perfectly fine, but my team and I constantly run into issues. . Has anyone had a similar experience? Is there something I’m missing, or a resource I could consult? I’d greatly appreciate any pointers. . Thanks .",
            "url": "https://ianepreston.github.io/iblog/2020/05/17/conda.html",
            "relUrl": "/2020/05/17/conda.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Conda_envs",
            "content": "How to work with conda environments in shell scripts and Makefiles . How to work with conda environments in shell scripts and Makefiles Setting up the problem | What’s going on? | Possible solution | Possible problem and solution | Ok, how about MakeFiles? | Conclusion | The final scripts Makefile | env.yml | eg.py | eg.sh (for makefile) | eg.sh (standalone) | | | I’ve struggled with automating working with the conda python environment manager for a while. It’s a relatively small part of my work flow so I haven’t made figuring it out a top priority, but it’s really bugging me. In this post I’m going to document the problem and all the troubleshooting steps I went through to resolve it. I’m writing this post in parallel with actually resolving the issue, so it’s going to be a bit stream of consciousness. . Setting up the problem . I have conda installed. I’m using bash as my shell. I’ve run conda init bash and my version of conda is new enough that this works nicely in interactive mode. . Before I go further describing the problem, I should note which version of conda I’m running, since I know a lot has changed with how it sets itself up, and more may change in the future. This guide is built using Miniconda on Windows 10, running conda 4.8.3. . When I open a new bash terminal I can see from my prompt that I’m in the base environment and I can run conda activate example_env to activate to activate an environment called eg_env. I can get back to base with conda deactivate. Now let’s say I’ve written a python script that’s intended to be run in that environment. I might want to write a shell script to run that file, or use a Makefile to use that script as part of a larger pipeline. To demo and work through this I’m going to make a small conda environment, and a small python script that will only work if I’m in that python environment. . Here’s my env.yml: . name: eg_env dependencies: - numpy . Now I create a simple python script, which I should be able to run without issue from that environment, but not from base (I’m using Miniconda so my base env is quite sparse). . Here’s the eg.py file: . import numpy as np print(np.__version__) . Just to be sure I’ll try running it from base. I get the following error: . $ python eg.py Traceback (most recent call last): File &quot;eg.py&quot;, line 1, in &lt;module&gt; import numpy as np ModuleNotFoundError: No module named &#39;numpy&#39; . Running it from my active environment is no problem: . $ python eg.py 1.18.1 . Excellent, environment management works! But what if I want to call this from another scripts? This is clearly contrived in this example but there are certainly situations where I might want to do this, and if nothing else it will demonstrate a bit more about how conda works. . So first off, here’s a bash script that just runs the python program: . #!/bin/bash echo &quot;This is my test bash script&quot; python eg.py . If I run this script from my base environment, it behaves basically the same as before: . $ ./eg.sh This is my test bash script Traceback (most recent call last): File &quot;eg.py&quot;, line 1, in &lt;module&gt; import numpy as np ModuleNotFoundError: No module named &#39;numpy&#39; . Similarly, if I run it from my example environment it runs just fine: . $ ./eg.sh This is my test bash script 1.18.1 . Cool. OK, but say I want my script to handle activating the environment for me? Let’s modify it and see what happens: . #!/bin/bash echo &quot;This is my test bash script&quot; echo &quot;Activating conda environment&quot; conda activate eg_env echo &quot;Running python script&quot; python eg.py . Running this from my base environment I get: . $ ./eg.sh This is my test bash script Activating conda environment CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. . . . Running python script Traceback (most recent call last): File &quot;eg.py&quot;, line 1, in &lt;module&gt; import numpy as np ModuleNotFoundError: No module named &#39;numpy&#39; . What’s going on? . There’s actually some guidance in the full error message from the attempt above. The relevant section is this: . CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; . I’ve already done that for my shell, but apparently it doesn’t apply to subshells launched from that shell. I can’t just put conda init bash in the script, because you need to restart your shell for it to be applied. . As far as I know, all running conda init bash did for me was add a line to my .bash_profile file: . # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by &#39;conda init&#39; !! eval &quot;$(&#39;/C/ProgramData/Miniconda3/Scripts/conda.exe&#39; &#39;shell.bash&#39; &#39;hook&#39;)&quot; # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; . Possible solution . What happens if I just put that at the top of the script? . Bash script updated: . #!/bin/bash echo &quot;This is my test bash script&quot; echo &quot;Activating conda environment&quot; eval &quot;$(&#39;/C/ProgramData/Miniconda3/Scripts/conda.exe&#39; &#39;shell.bash&#39; &#39;hook&#39;)&quot; conda activate eg_env echo &quot;Running python script&quot; python eg.py . Output: . $ ./eg.sh This is my test bash script Activating conda environment Running python script 1.18.1 . It worked! Notably, I’m still in my base environment when the script exits. The activation only happens in the subshell. I also tested running this script from a prompt that was already in that environment and it ran fine as well. . Possible problem and solution . Ok, this works as long as I’m either the only person trying to use this script, or everyone else is also running Miniconda on Windows from a system level install. That seems pretty fragile. Can we improve this? . The which conda command returns the path to your conda install, so I should be able to replace the absolute path with what that returns to get the same result: . #!/bin/bash echo &quot;This is my test bash script&quot; echo &quot;Activating conda environment&quot; eval &quot;$($(which conda) &#39;shell.bash&#39; &#39;hook&#39;)&quot; conda activate eg_env echo &quot;Running python script&quot; python eg.py . This works too! Getting pretty close to a nice solution. . Ok, how about MakeFiles? . Because I am fancy, I like to have a MakeFile for my projects, which I can then use to run scripts or series of commands with nice convenient shortcuts. Most of what I do could definitely be accomplished with pure shell scripting, but it will be a little nicer if I can do it with Make, so let’s try. . To make this a little more realistic I’m going to say there are two things I might want to do using make in this project. One might be to format the python file with black and the other might be to run the file. I’ll add black to my example environment, but not my base, which means I’ll have to have the environment activated to format or run the script. I’d like to be able to run the python script directly from the makefile, or run it through a shell script. . In the same folder as my example python and shell scripts I’ll add a Makefile, based off the top answer on This stackoverflow question: . # Oneshell means I can run multiple lines in a recipe in the same shell, so I don&#39;t have to # chain commands together with semicolon .ONESHELL: # Need to specify bash in order for conda activate to work. SHELL=/bin/bash # Note that the extra activate is needed to ensure that the activate floats env to the front of PATH CONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate test: $(CONDA_ACTIVATE) eg_env echo $$(which python) # format the file with black lint: $(CONDA_ACTIVATE) eg_env black eg.py # Run the file directly run_py: $(CONDA_ACTIVATE) eg_env python eg.py # Run the file from a shell script run_sh: $(CONDA_ACTIVATE) eg_env bash eg.sh . I’m also going to update eg.sh to remove the environment activation I set up earlier, because the idea is that make should be handling this for me now. . All of these work! I’m a little bummed I have to put that $(CONDA_ACTIVATE) eg_env at the top of each recipe, but that’s still pretty solid. I tried adding an active recipe and setting it as a requirement for the other recipes like so: . active: $(CONDA_ACTIVATE) eg_env test: active echo $$(which python) . But the activation from active didn’t persist into running test. Maybe there’s a way to get that working but I’m going to call this close enough for my needs. . Conclusion . Getting conda activation to work from within bash and Makefiles is a finicky process. Or at least I don’t have a strong enough understanding of subshells, environment variables conda and probably some other things to make it seem otherwise. That said, the steps outlined in this guide should allow you to automate processes involving conda environments without too much hassle. . The final scripts . For reference, here’s the final form of what I used to setup and test this demo: . Makefile . # Oneshell means I can run multiple lines in a recipe in the same shell, so I don&#39;t have to # chain commands together with semicolon .ONESHELL: # Need to specify bash in order for conda activate to work. SHELL=/bin/bash # Note that the extra activate is needed to ensure that the activate floats env to the front of PATH CONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate test: $(CONDA_ACTIVATE) eg_env echo $$(which python) # format the file with black lint: $(CONDA_ACTIVATE) eg_env black eg.py # Run the file directly run_py: $(CONDA_ACTIVATE) eg_env python eg.py # Run the file from a shell script run_sh: $(CONDA_ACTIVATE) eg_env bash eg.sh . env.yml . name: eg_env dependencies: - numpy - black . eg.py . import numpy as np print(np.__version__) . eg.sh (for makefile) . #!/bin/bash echo &quot;This is my test bash script&quot; echo &quot;Running python script&quot; python eg.py . eg.sh (standalone) . #!/bin/bash echo &quot;This is my test bash script&quot; echo &quot;Activating conda environment&quot; eval &quot;$($(which conda) &#39;shell.bash&#39; &#39;hook&#39;)&quot; conda activate eg_env echo &quot;Running python script&quot; python eg.py .",
            "url": "https://ianepreston.github.io/iblog/2020/05/13/conda_envs.html",
            "relUrl": "/2020/05/13/conda_envs.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Samba",
            "content": "A basic SAMBA share for home networks . A basic SAMBA share for home networks Intro Edit October 2020 | | Password free server on a Linux box What I’m installing this on | The steps Install samba | Backup any existing smb.conf and then update | Restart and enable SAMBA, give it a test | | No Password Conclusion | | Mounting shares from a NAS Create a credentials file | Find the appropriate UID and GID to assign ownership | Create the fstab entry | NAS Conclusion | | | Intro . Edit October 2020 . The original post dealt with setting up a share from an Ubuntu server to provide read/write access without a password. Since writing that part I purchased a Synology NAS and am now including sections on setting up shares for that. . Most SAMBA guides I find online are some combination of out of date or focused on the enterprise. My objective is to provide a quick reference for setting up files shares from a Linux server to Windows clients, or to properly mount SAMBA shares from a NAS device onto Linux clients. This is only appropriate for a home network. In the case of the Linux server I’m sacrificing security/specific user permissions for being easily able to connect to my file share. On a small LAN where I can easily physically monitor the devices I think this is worth it. Clearly you should not do this for an organization or if you have more sensitive data you’re sharing. . Password free server on a Linux box . What I’m installing this on . The current server I’m running this on is an Ubuntu 18.04 machine. Hopefully most of this will translate to similar setups. I’m sure I’ll be upgrading the OS soon so I’ll edit this if I encounter any breaking changes. . The steps . Install samba . sudo apt install samba . Backup any existing smb.conf and then update . # If you have one already sudo mv /etc/samba/smb.conf /etc/samba/smb.conf.bak . Now setup the new smb.conf: . [global] map to guest = Bad User logging = systemd log level = 1 guest account = &lt;username&gt; [data] # This share allows guest read write access # without authentication, hope you trust everyone on your LAN path = /mnt/data/ # Or whatever folder you&#39;re sharing read only = no guest ok = yes guest only = yes . Where &lt;username&gt; is the user on your samba server that has appropriate access to the folder you’re sharing. . After saving the config file you can run testparm to see if there are any syntax errors. . Restart and enable SAMBA, give it a test . From the samba server: . sudo systemctl status smbd # check if it&#39;s running # If it&#39;s running do this sudo systemctl restart smbd # If it&#39;s not do this sudo systemctl start smbd . Try and connect from a Windows machine, make sure you can create and delete files. Back on the samba client you can check if the files you created have the right permissions (should be assigned to the user you created). . Assuming everything works enable the server so it will reload if you restart the machine. From the samba server: . sudo systemclt enable smbd . No Password Conclusion . That’s it! Super simple but every time I tried to get a SAMBA share going in the past I always ended up struggling. Hopefully this guide will be helpful to future me and anyone else who’s got a similar situation. . Mounting shares from a NAS . This part is pretty straightforward, but there are always a few finicky bits that I have to google, so I’m recording the process here for the next time I have to set it up. Setting up the actual share on the NAS is outside the scope of this article. In my case I’m using a Synology box so it’s pretty much just point and click. . Create a credentials file . I’m going to try and pay a reasonable amount of attention to security in this implementation. The NAS has a user set up with read and write permissions for the share that I want to access. I’ll use the suggestions from the Arch Wiki to set up a credential file. . sudo mkdir /etc/samba/credentials sudo echo &quot;username=&lt;shareusername&gt;&quot; &gt;&gt; /etc/samba/credentials/share sudo echo &quot;password=&lt;sharepassword&gt;&quot; &gt;&gt; /etc/samba/credentials/share sudo chown root:root /etc/samba/credentials/share sudo chmod 700 /etc/samba/credentials sudo chmod 600 /etc/samba/credentials/share . Find the appropriate UID and GID to assign ownership . When creating the /etc/fstab entry for the share mount we want to assign ownership to the user that will actually be accessing the files. This is done by UID and GID. For my single user systems that’s usually 1000:1000 but I like to double check and I usually have to look up the command so here it is. . id -u &lt;username&gt; id -g &lt;username&gt; . Will give the UID and GID for user . Create the fstab entry . Now we just need to create an entry in fstab to the share: . //&lt;server&gt;/&lt;share_path&gt; /mnt/&lt;share_point&gt; cifs _netdev,uid=&lt;uid&gt;,gid=&lt;gid&gt;,credentials=/etc/samba/credentials/share 0 0 . NAS Conclusion . This extension to the post just has a few code snippets that I’ve found useful. There’s not a lot of exposition on why I’ve set things up the way I did. It’s more meant as a reference for future me, but hopefully it’s useful to others. .",
            "url": "https://ianepreston.github.io/iblog/2020/05/09/samba.html",
            "relUrl": "/2020/05/09/samba.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Pfsense",
            "content": "Building pfsense . Building pfsense Rationale Who this is for | What this will cover | | Resources | Base Install | Wizard Setup | WiFi Addendum | | WebUI Overview and general setup System / General Setup | System / Advanced / Admin Access | System / Advanced / Firewall &amp; NAT | System / Advanced / Networking | System / Advanced / Miscellaneous | System / Advanced / Notifications | Interfaces / WAN (and LAN) | Dashboard | | NTP Server | DHCP Static Mappings | | DNS | Pfblocker - ad and geoblocking | Dynamic DNS | OpenVPN - secure remote access Basic Setup DNS resolution | | Create Users | Export keys to devices | | Conclusion | | Rationale . Who this is for . This guide is mostly for me. I’m trying to get better at documenting all the tech stuff I do on my home system, both to ensure I have a decent understanding of what I’m doing, and also to ensure I can reproduce it if I need to rebuild at some point in the future. This guide is heavily reliant on the guidance from Mark Furneaux in his YouTube series on the topic. The first video in that playlist provides a good overview of what pfsense is and why you might want to install it. If you watch that video and think you might like to install pfsense, but don’t want to watch about 9 hours of YouTube to figure out how to set it up, this might come in handy for you as well. . What this will cover . At a high level this will cover installing and configuring a pfsense box along with a Unifi wireless access point. Hardware selection is not covered here. The table of contents describes the sections of pfsense settings and applications that will be set up over the course of this guide. . Resources . Besides the excellent pfsense documentation I relied heavily on a series of YouTube videos from Mark Furneaux. . Base Install . The instructions on the install page are pretty good. The link for creating install media is also comprehensive. Since I didn’t deviate from their instructions in any meaningful way and there’s no reason to expect their documentation to go away I’m not going to reiterate the steps in any detail here. Basically flash the installer image to a USB key, boot that in your system and follow the wizard. . Wizard Setup . Next we go through the Wizard. I did the bare minimum of config here, preferring to leave the details for once I had a minimally working system set up. From a browser head to the default IP. You’ll get prompted that the connection is not secure. We’ll deal with certificates later. For now just hit advanced and proceed. . The first thing you should see is a login page. The default login is admin/pfsense. After that there’s a welcome page for the Wizard, followed by a page describing all the support options that are available. After that we get to the first page to actually populate. . . All that’s necessary on this page is to give your router a name (I went with behemoth because I’m doing an expanse themed naming scheme). You also need to name your local domain. localcomain seems fine so I left it at that. DNS can be configured here but I left it alone and will configure it later. . Next is time zone, left to default NTP for now but set to my timezone. . My WAN interface is DHCP so I just leave all default. . . The next step is to configure your LAN interface. You can leave it on 192.168.1.1 but I prefer to change it. The reason is that 192.168.[1/0].1 is a super common address range. If I use that range on my LAN, and then try and VPN back into it from someone else’s network using the same address range it tends to cause issues. The pfsense book has a good chapter on this. The 24 subnet mask is equivalent to 255.255.255.0. If you haven’t seen that notation before, or need to brush up the pfsense book also has a decent high level introduction. . Next step is to update the admin password - make it something secure and save it somewhere. I use LastPass for everything password related. . Wizard over! You’ll be prompted to reload, and if you changed the LAN IP you might need to release and renew your IP again before it will load up. . Navigate to https://behemoth (or whatever you picked for your hostname)- connected! . WiFi . So getting WiFi going was a bit of an adventure. This might be too niche for anyone else to benefit from, but I’m sure going to appreciate having it written down somewhere if I need to do it again. My WAP is a Ubiquiti Unifi AP AC Lite. All Ubiquiti hardware is managed through a controller. You can buy a physical device from them to serve this function, but they’re at least $130 so nuts to that. When I was planning out this build I read that you can install the controller software right on your pfsense box. That seemed ideal… until I tried it. After much weeping and gnashing of teeth I ended up burning my pfsense install to the ground and starting over. Given I actually hadn’t done much on the setup yet and messing around with getting the controller working involved installing a bunch of software I didn’t want any more and wasn’t sure I could get rid of that seemed like the safest bet. On to plan B. Eventually I’ll probably get a Raspberry pi as a dedicated host for the controller, but for now I don’t have a spare one so I decided to throw it on my server. . The saints over at LinuxServer have a container made so I went with that. After resolving a port conflict with one of my other services the controller launched just fine. . After navigating to my server’s IP on port 8443 I got a login prompt where I had to name my controller and then sign into (or create) my ubiquiti account. I set everything to auto optimize and to create one SSID for both 5GHz and 2.4GHz networks. . Up to here everything is super slick, but then I hit problems. . . My WAP showed up on the home page but it was just stuck on “Adopting (Update Required)”. Initial googling suggested that I would have to manually push a firmware update before I could adopt. This didn’t actually solve the issue, but just for reference here’s how you do that: . Check your router to determine what IP the WAP has been assigned. Google for the firmware for your hardware, on the Unifi page you’ll get a download icon and after accepting the terms you’ll get a direct download link. SSH into the WAP, the default username and password are both ubnt. . curl &lt;firmware_binary_link_location&gt; -o /tmp/fwupdate.bin syswrapper.sh upgrade2 &amp; . The prompt on the WAP should now show the latest firmware version when you SSH back in. Again, this didn’t fix my issue, but knowing how to push firmware updates might come in handy later so I wanted it recorded. . The actual solution was to hard code where the controller should be broadcasting from. Here’s the forum post describing it. I had to put in the IP of my server for some reason. I could resolve it by hostname from my other PC, but the WAP itself could only ping it based on the IP. I’ll revisit this after I mess with DNS more later on pfsense. . First click settings in the lower left of the controller page: . . Then on the controller page hard code the IP and host and set “Override host with controller hostname/IP”. I also set “Make controller discoverable on the L2 network” since it’s a locally hosted controller, but I’m not sure that actually made a difference. Doesn’t hurt. . . For now I’m online! . Addendum . I have an old Kobo reader. After switching to the Unifi AP I couldn’t get it on the WiFi. An even older Kindle would connect, and my newer Kobo connected after manually entering the SSID, but no luck with the older one. Eventually I found this reddit thread with the solution. From the Unifi control panel I went to settings -&gt; wireless networks -&gt; &lt;my SSID&gt; -&gt; advanced and unchecked “enable minimum data rate control” for the 2G network. After applying that and trying to connect a few more times I made it online with the Kobo. . WebUI Overview and general setup . This section of the guide will follow along with part 5.1 of Mark Furneaux’s guide. The video is about 4 years old and based on pfsense 2.3 whereas I’m using 2.4.5, so there may be some minor cosmetic differences. I also want a text representation of his guide, as it will be easier to refer back to later. . System / General Setup . Only two minor changes here: . System / Advanced / Admin Access . Turn off https since we don’t have a certificate authority (maybe I’ll figure that part out later) . . Change browser tab text so I know what page each tab is on. . . Enable SSH and change the default port to 2222 (security through obscurity). I’m going to set up keys later, for now let it work with a password but come back and update this later. . . Fun piece of trivia, right as I enabled this my web front-end froze. The actual router was still functioning, but I couldn’t get any web admin. Part of it was that I had to switch from https to http in the URL, but even after that I would hit the login, it would accept my password, and just take me back to the login page. I found this reddit thread which had the same issue. For me I could connect in using a private browsing window, which led me to try restarting my browser, which fixed it. Computers… . System / Advanced / Firewall &amp; NAT . Set Firewall optimization to conservative. I have plenty of CPU and RAM for the size of my network, so why not give idle connections a little longer to hang out? Apparently this can improve VOIP performance. . . System / Advanced / Networking . For now I’m just going to disable IPv6. I don’t think my ISP supports it and I don’t see the need for it on my LAN. Maybe I’ll revisit that later, but for now turning it off seems like the safer approach. . . Set everything possible to work on hardware. Given that I have Intel NICs in my router I think I can safely run all of these things. I ran iperf3 between two wired connections as well as speedtest before enabling the settings and then again after. Note that you have to reboot after changing these settings. After the reboot performance was essentially unchanged so I took that as a good sign and kept the settings. . . System / Advanced / Miscellaneous . I don’t expect a lot of super heavy CPU needs on this system, so I might as well save some power and heat. I’ll put it on Adaptive to start, but I’ll check out Hiadaptive if some services seem to chug. . . My CPU supports AES-NI and I’ll want that enabled for better VPN performance later. . . System / Advanced / Notifications . I want email notifications if something gets borked on my router. I don’t want to use my actual gmail address to send these notifications though, as I’d like to keep the security on it a lot more locked down. I created a new gmail account just for the router, and then in my account settings (for the router email do not do this for your actual email) I set “less secure apps” to on so that I could enable sending emails with the following settings: . . After all that I hit “test SMTP settings” and received an email informing me it worked. Hurray! . Interfaces / WAN (and LAN) . I’m going to disable IPv6 here since as I discussed I don’t want to use it. . For the LAN I first have to disable the DHCPv6 service: . . And then on each respective interface’s page I can disable IPv6 . . Dashboard . Let’s add some widgets! Everyone loves widgets. . Add S.M.A.R.T status so I can see if my hard drive is failing. Services Status to see what’s running. Interface statistics to see if I’m getting any errors, and finally traffic graphs because who doesn’t like a nice live graph? . NTP Server . YouTube guide here . In addition to the pool server that’s automatically configured I’ll add a couple extras to improve synchronization: . . I’ll also enable RDD graphs because it might be fun to see a chart of time drift and recalibration for my system, why not? . . Now that I’ve got a nice fancy NTP server it will be nice to have all the machines on my network in sync for time by using the local pfsense NTP server. . On Windows (I always forget where to find this on Windows) you go to control panel -&gt; date and time -&gt; Internet Time -&gt; Change settings -&gt; change the server to your pfsense router. In my case that’s behemoth.localdomain. On windows at least it seems like you have to include the .localdomain part. . On Linux, following this digitalocean guide first I need to check if I’m using timesyncd (probably) or ntp: . . Since in this case I’m on timesyncd I’ll have to turn that service off, install ntp, enable it, and then point it to my pfsense box: . sudo timedatectl set-ntp no # Confirm it was disabled timedatectl # Install ntp (this is for Ubuntu, similar for Arch or whatever) sudo apt update &amp;&amp; sudo apt install ntp -y # Check that it loaded ntpq -p # check that timedatectl picked it up correctly timedatectl . After that I have to edit /etc/ntp.conf and replace the default pools with just one for my router. Finally I restart the NTP service with sudo service ntp restart and I can run ntpq -p one more time to make sure it’s pointing to my server. Everything looks good and now I’ve spent way more time than is reasonable making sure my clocks are perfectly in sync. . DHCP . YouTube guide here . There are two basic settings I want to change for DHCP. They’re both fairly minor. The first just sets it so that the logs for DHCP activity show my local time zone instead of UTC, which will make them easier to interpret. The second just lets me make pretty graphs if I want. . . Since I went to all the trouble of setting up NTP, I’ll set up DHCP to push that server. Note that I had to use the IP of my router here, not the hostname. This doesn’t guarantee DHCP clients will use the NTP server, but it provides the option, so why not? . . Static Mappings . For most of the devices on my network I’d like to give them easy to type and remember names, even if they don’t have the functionality to specify a hostname (or even if they do just to be safe). For one thing this makes it easy to connect to devices, but even for things I don’t want to connect to (like an e-reader) it’s nice to give them an obvious name. That way if I’m looking at the DHCP leases on my network it will be easier to notice a new device. To do this there are two steps. The first is adding a DHCP static mapping for each device, and the second is enabling DNS to resolve those names (skipping ahead a bit since DNS is next). While you can do static mappings from the DHCP services page, it’s actually easier to do from the DHCP status page. Beside each device that’s connected there’s an “add static mask icon” which you can click. After that you just have to fill in an IP, the hostname you want, and an optional description: . . Since I’m going to set up most of my network this way I’m also going to narrow the range for regular DHCP leases to be handed out back on the DHCP Server service page: . . After this I’ll have to trigger a release/renew cycle for all devices on the network. The easiest way to do this is probably just reboot the router. . To set a proper hostname on a device here are the instructions relevant to me: . On Linux: edit /etc/hostname to be whatever you want the hostname to be. . On Android: Who knows? I can’t seem to make this give a proper name. Fortunately I can rely on the name from pfsense. . On Windows: Hit start, type “pc name” and select the entry that comes up, click “Rename this PC” and change it to whatever you want. . The last step is to set the DNS resolver to resolve these names. In services -&gt; DNS resolver I check these two boxes: . . I think I only need the second one since I gave every client on my network a static lease, but the other one seems nice to have as well, so I’ll enable it. . DNS . Relevant YouTube section . To speed up name resolution, I’m going to run namebench, wait about an hour for it to complete, and then add the three DNS servers that it finds are the fastest, along with 8.8.8.8 which is Google’s DNS sever to my DNS server settings under System -&gt; General setup: . . Respect to my ISP, all the fastest DNS servers are from them, and the two fastest are the ones I was assigned by DHCP. So this probably didn’t really do anything for me. Oh well. . I’m going to add a couple extra settings in services -&gt; DNS resolver: . . These should make response times for DNS queries a little faster at the cost of trivial CPU increase. . . Why not store more names? I have tons of RAM. . Pfblocker - ad and geoblocking . Now we get to the fun stuff. This blocks ads/spam/etc at the network level, which is awesome. . I’m using this guide from Lawrence systems as the basis for my setup. . First thing’s first is to install the package, I went with the development version since that’s what was installed in the tutorial I was following: . . Head over to the settings page: . . Then go through the wizard (this may come up automatically, it didn’t for me): . . You get some warnings about how this will wipe your setup. That’s fine, we’re installing fresh. Then you get the first actual input prompt. The defaults are fine for me so I leave them: . . . And that’s the wizard done! It should auto run an update to download all the blocklists and other goodness. Wait for that to finish. . Set up a MaxMind account for geoblocking and add in my license key: . . Enable floating rules (see the video for a discussion on why) . . Enable kill states . . On geoblocking I’m going to deny both for top spammers, and then deny inbound from anywhere except North America. I’m not hosting anything, the only inbound traffic should be me connecting in on VPN (set up later). I’ll have to remember to change this if I go on vacation somewhere off continent. I’m also going to disable logging for now as I don’t really need to look at when this happens. If there’s an issue I’ll turn it back on. . . And that’s it! I can wait an hour or manually have it reload the settings: . . Dynamic DNS . Looks like VPN certs will want to be associated with a domain so I guess I should buy a domain and set up dynamic DNS first. . I bought my domain through namecheap. I also repointed this blog to use the domain, so if that all went smoothly this is being read at blog.ianpreston.ca. . Namecheap has good docs on setting up dynamic DNS. I enabled it following the instructions above (just hit enable under advanced DNS from your domain page). . Then I created an A+ DNS record . Then from pfsense I went to services -&gt; Dynamic DNS and filled in the info for namecheap. Note that the domain part has to be your whole domain. At first I had just the ca part in there with my name and the subdomain in hostname and that didn’t work. . Refreshing the namecheap page confirmed that the dummy IP I’d entered had updated to the correct one, but a ping didn’t work right away. Presumably it takes a while for the DNS records to propagate. . OpenVPN - secure remote access . I’m following the docs and the book for this part. For now I’m going to use OpenVPN. At some point in the future I might switch to WireGuard as it seems to be the new hotness and is integrated into the latest Linux kernel, but I’m going to wait for official pfsense adoption on that one. . Basic Setup . Under VPN -&gt; OpenVPN head to the wizard tab. | Leave authentication backend as Local User Access | Fill in details for the CA. I left most things the default | Same deal for the server cert, most options should persist from the CA | Set up general server information I set hardware crypto on. | Tunnel network has to be an IP range not used by your LAN, and it shouldn’t be a common one. Since my LAN uses 172.17.1.0/24 I went with 172.17.2.0/24 | I left redirect gateway off because I’m just trying to set up LAN access remotely, I don’t need all my traffic tunneled through here if I’m remote | Local network points to my LAN so I can access it, so as described above that’s 172.17.1.0/24 | I set concurrent connections to 10. Probably more than I’ll need but why not? | I enabled Inter-Client Communication | I set the DNS default domain to localdomain | I set DNS Server 1 to 172.17.1.1. These two options should allow me to access my servers by hostname even remotely | I set the NTP server to 172.17.1.1 since I went to all the trouble of configuring it | I don’t think I need NetBIOS or WINS so I left that blank | . | Check boxes for both firewall rules | Wizard complete, now I want to be able to export settings to clients, so over to System -&gt; package manager and install openvpn-client-export | . DNS resolution . I’m not sure why I had to set this, but in order to get my client machines to be able to resolve hostnames of servers I had to go into Services -&gt; DNS resolver -&gt; General settings and check “Register connected OpenVPN clients in the DNS resolver”. The way they describe it really sounds like it should allow me to resolve client names, but it fixed my issue so whatever. . Create Users . System -&gt; User Manager -&gt; Add User. For users I’m going with the &lt;person&gt;_&lt;device&gt;_vpn naming convention. So for example my phone’s certificate will be ian_phone_vpn. I don’t add the user to admins, and I create and store a nice secure password with LastPass. Make a certificate for the user, all the default’s should be fine, just fill in an appropriately descriptive name. . Export keys to devices . Back to VPN -&gt; OpenVPN -&gt; Client Export Utility. Since I have dynamic DNS configured I changed over Host Name Resolution to my DynamicDNS name rather than my IP which is the default. . I didn’t mess with any of the defaults. I have the OpenVPN Connect app on my phone so I scrolled down to the user I just created for my phone and clicked “OpenVPN Connect (iOS/Android). Downloaded the file, transferred it to my phone, imported it in the app and was good to go. . Conclusion . That’s it for now! Everything seems to work well. I’ll update this guide if I make any further changes to my setup. . Last thing to do is go to Diagnostics -&gt; Backup &amp; Restore and save my config, just in case something bad happens. .",
            "url": "https://ianepreston.github.io/iblog/2020/05/06/pfsense.html",
            "relUrl": "/2020/05/06/pfsense.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Ssh",
            "content": "Managing SSH keys for a home network . Managing SSH keys for a home network Introduction | Who this is for | The actual process Get the drive ready | Generate CA keys Generate known_hosts file | | Generate the host key and sign it WARNING: DO NOT PUT A PASSPHRASE ON HOST KEYS | | Configure SSH to use host certificates | Create user key | Give it a test drive | Set up an ssh config file | | External servers | Android | Auto config scripts Set up host authentication | Setup user authentication | | Conclusion | | Introduction . This guide covers how I set up and manage ssh keys on my home network. It’s not meant as an explainer on what ssh is or why you should use it, more a recipe for how I do things. . Edit October 16 2020 - Added scripts to automate this setup | . Who this is for . Mostly me so I remember how to do this if I have to rebuild my network, or want to add new clients. I’m not an expert in security, so definitely do your own research before implementing any of this, and I would certainly say that this is a hobbyist implementation in general and not suitable for an organization. . I got the inspiration for this from How to SSH properly. It’s a nice and thorough guide, but it’s a bit enterprise oriented for my purposes. The idea here will be to make something that’s more maintainable for a local setup. . The actual process . The idea is to have my certificate authority keys live on a USB drive. That way when I set up a new machine I can plug in the drive and sign the keys, and then the rest of the time the keys are completely removed from network access. As long as I’m smart and don’t lose/destroy the USB drive it seems like a great idea. The main advantage from just using regular keys rather than signing them is that I don’t have to add a line to authorized_keys on every host every time I add a client, and I can keep a nice clean known_hosts on each of my clients. For this example my host machine will be named mars and the client machine will be named luna. I’ll do the preliminary setup from luna, but any machine should be fine. . Get the drive ready . I have a 64GB USB key that I’m going to store the CAs on. That’s way too big for what I need, and I wouldn’t mind having it handy to shuttle other files around (I’m not going to be taking it anywhere, on account of it has CAs for my network on it, but still). So I created a big exfat partition that took up most of the disk for files and such, along with a 200MB ntfs partition right at the end for storing keys. I went with exfat for the storage partition because it’s readable in both Windows and Linux (once you install exfat-utils and exfat-fuse), is designed for flash media, and can handle large files. I went with NTFS for the secrets partition because I have to have file permissions on the private keys or it won’t work, and windows machines won’t read EXT4. It does mean I’ll have to install NTFS support on any Linux boxes I want to run it from, which is a hassle, but I can live with that. . Generate CA keys . From the usb drive: . ssh-keygen -t ed25519 -f user_ca -C user_ca ssh-keygen -t ed25519 -f host_ca -C host_ca . Generate a host and user signing key using ed25519 encryption. Generates public private key pairs named host/user_ca and host/user_ca.pub for the public keys. RSA is the default, but all of my systems support ed25519 and I understand it’s better in terms of security and performance so I might as well take this opportunity to update. . Generate known_hosts file . This will go in the ~/.ssh folder of clients in order to validate access. . echo -n &quot;@cert-authority * &quot; &gt; known_hosts cat host_ca.pub &gt;&gt; known_hosts . Generate the host key and sign it . For any machines that I want to be able to ssh into I need to generate and sign a host key. From the same folder as the CA keys were generated: . ssh-keygen -t ed25519 -f mars_host_ed25519_key ssh-keygen -s host_ca -I mars -h mars_host_ed25519_key.pub . WARNING: DO NOT PUT A PASSPHRASE ON HOST KEYS . The first line operates the same as before. The second line signs the public key. -I mars is the certificate’s identity, apparently it can be used to revoke a certificate in the future although I’m not totally clear on how at this point. You can use the -n flag to specify which hostname in particular this key is valid for but I don’t really see the need in as small a setup as I’m doing. -h identifies this as a host key. . At the end of this, three new files are generated: mars_host_ed25519_key, mars_host_ed25519_key-cert.pub, and mars_host_ed25519_key.pub. . Configure SSH to use host certificates . Move the three generated files from the last section and copy user_ca.pub to /etc/ssh on your host and set the permissions to match the other files there. In this example I’ve physically moved the key over to the host machine and mounted it. If your host currently has ssh enabled with password based authentication you could scp it over instead: . sudo mv mars_host_ed25519_key* /etc/ssh/ sudo cp user_ca.pub /etc/ssh/ cd /etc/ssh/ sudo chown root:root mars_host* sudo chown root:root user_ca.pub sudo chmod 644 mars_host* sudo chmod 600 mars_host_ed25519_key # stricter private key permissions sudo chmod 644 user_ca.pub . Next, edit /etc/ssh/sshd_config to have the lines . HostKey /etc/ssh/mars_host_ed25519_key HostCertificate /etc/ssh/mars_host_ed25519_key-cert.pub TrustedUserCAKeys /etc/ssh/user_ca.pub . There was a commented out line in the file for HostKey so I put it below there. Placement shouldn’t really matter but it will hopefully be easier to find if I have to track this file down later. . sudo systemctl restart sshd . Once you’re sure everything is working you’ll want to disable password authentication. Be aware that if you screw up keys and have the second line set you’ll have to physically connect to the machine to resolve it. For my home network this is no big deal, but just be aware. Go into /etc/ssh/sshd_config and set the following lines and restart ssh again: . PubkeyAuthentication yes PasswordAuthentication no . Create user key . Still in the folder with the ca key: . ssh-keygen -f luna_ed25519 -t ed25519 ssh-keygen -s user_ca -I ian@luna -n admin,ansible,ipreston,pi luna_ed25519.pub mv luna* ~/.ssh/ cp known_hosts ~/.ssh/ . The only new flag in this is -n ipreston,admin,pi which is the comma separated list of users I want this to be valid for. In addition to the username I set up on my server I want this key to be able to connect to my pfsense admin user and my raspberry pi, which I generally just leave with the default pi user. . Give it a test drive . At this point if everything is configured correctly you should be able to ssh from your client to your host and only be prompted for the passkey you set (or without any prompting if you didn’t set a passkey) . ssh -i ~/.ssh/luna_ed25519 ipreston@mars . Set up an ssh config file . If that all worked the next step will be to set a nice alias to save some typing in the future. . touch ~/.ssh/config chmod 600 ~/.ssh/config . In config: . Host mars User ipreston IdentityFile ~/.ssh/luna_ed25519 . There are lots of other options you can set in that config but that’s all I need for my setup. After that all you should need to type is ssh mars to be connected to your host with the right user and using the correct authentication. . External servers . There’s no real sense in signing keys for external services. For example, I use GitHub with SSH, but they only allow CA authentication for enterprise (fair enough). There’s also no particular reason to re-use my LAN keys for GitHub, so I created a new key and set ~/.ssh/config to use the correct identity automatically: . Use the same commands as the user key setup stage, except don’t bother signing the key. Add the public key to GitHub as you normally would (GitHub docs) and then edit ~/.ssh/config to add something like the following: . Host github.com IdentityFile ~/.ssh/luna_github_ed25519 HostName github.com User git . Android . I don’t ssh a ton from Android, but every so often it’s handy to be able to do. My former go-to app was Connectbot, but I never really liked how it managed keys, and I couldn’t get it to work with the CA. I ended up going with termux. To set it up: . Generate and sign keys as normal. | copy the key pairs, config and known_hosts onto your android device | Install termux (it’s in the play store) | Open termux | Install openssh with pkg install openssh | Allow access to internal storage with termux-setup-storage and accepting the request | Navigate to where you copied the files (somewhere in ~/storage/shared) and copy them to ~/.ssh using cp, it’s just regular bash in termux. | Everything should work, try connecting to a host. | . Auto config scripts . These have been tested to set up the host and user authentication on Arch Linux. Presumably they’ll work on other distros as well. Android will still need to be done manually probably. . To set them up save them both in a directory, and then in a CA subfolder create user and host cert keys as well as a known_hosts file and config file as described above. In the config where you’d normally have the hostname associated with the key file just put HOST and the script will replace it with whatever your hostname is, which is also how it creates keys. . Set up host authentication . This script when run as root will generate a signed host key, move it to the correct directory, modify SSH configuration to authenticate using it and then restart ssh. . #!/usr/bin/env bash # Bash &quot;strict&quot; mode, -o pipefail removed SOURCED=false &amp;&amp; [ &quot;${0}&quot; = &quot;${BASH_SOURCE[0]}&quot; ] || SOURCED=true if ! $SOURCED; then set -eEu shopt -s extdebug trap &#39;s=$?; echo &quot;$0: Error on line &quot;$LINENO&quot;: $BASH_COMMAND&quot;; exit $s&#39; ERR IFS=$&#39; n t&#39; fi # Text modifiers Bold=&quot; 033[1m&quot; Reset=&quot; 033[0m&quot; # Colors Red=&quot; 033[31m&quot; Green=&quot; 033[32m&quot; Yellow=&quot; 033[33m&quot; error_msg() { T_COLS=$(tput cols) echo -e &quot;${Red}$1${Reset} n&quot; | fold -sw $((T_COLS - 1)) exit 1 } check_root() { echo &quot;Checking root permissions...&quot; if [[ &quot;$(id -u)&quot; != &quot;0&quot; ]]; then error_msg &quot;ERROR! You must execute the script as the &#39;root&#39; user.&quot; fi } generate_and_sign_host_key() { private_suffix=&quot;_host_ed25519_key&quot; private_key=&quot;$HOSTNAME$private_suffix&quot; public_key=&quot;$private_key.pub&quot; ssh-keygen -t ed25519 -f $private_key -N &quot;&quot; chown root:root $private_key* # Have to lock it down before signing chmod 600 $private_key ssh-keygen -s CA/host_ca -I $HOSTNAME -h $public_key cp CA/user_ca.pub /etc/ssh/ chown root:root /etc/ssh/user_ca.pub chmod 644 /etc/ssh/user_ca.pub chmod 644 $private_key* # Set back to stricter private key access chmod 600 $private_key mv $private_key* /etc/ssh/ } update_sshd_config() { private_suffix=&quot;_host_ed25519_key&quot; private_key=&quot;$HOSTNAME$private_suffix&quot; hostkey=&quot;HostKey /etc/ssh/$private_key&quot; cert=&quot;HostCertificate /etc/ssh/$private_key-cert.pub&quot; ca=&quot;TrustedUserCAKeys /etc/ssh/user_ca.pub&quot; sshd=&quot;/etc/ssh/sshd_config&quot; sed -i &quot;/^#HostKey s /etc /ssh /ssh_host_ed25519_key/a $ca&quot; $sshd sed -i &quot;/^#HostKey s /etc /ssh /ssh_host_ed25519_key/a $cert&quot; $sshd sed -i &quot;/^#HostKey s /etc /ssh /ssh_host_ed25519_key/a $hostkey&quot; $sshd sed -i &#39;s/^#PasswordAuthentication syes/PasswordAuthentication no/g&#39; $sshd } check_root generate_and_sign_host_key update_sshd_config systemctl restart sshd . Setup user authentication . Whatever user your run this as should end up with a configured SSH setup that will allow access into any similarly configured hosts. . #!/usr/bin/env bash # Bash &quot;strict&quot; mode, -o pipefail removed SOURCED=false &amp;&amp; [ &quot;${0}&quot; = &quot;${BASH_SOURCE[0]}&quot; ] || SOURCED=true if ! $SOURCED; then set -eEu shopt -s extdebug trap &#39;s=$?; echo &quot;$0: Error on line &quot;$LINENO&quot;: $BASH_COMMAND&quot;; exit $s&#39; ERR IFS=$&#39; n t&#39; fi # Text modifiers Bold=&quot; 033[1m&quot; Reset=&quot; 033[0m&quot; # Colors Red=&quot; 033[31m&quot; Green=&quot; 033[32m&quot; Yellow=&quot; 033[33m&quot; error_msg() { T_COLS=$(tput cols) echo -e &quot;${Red}$1${Reset} n&quot; | fold -sw $((T_COLS - 1)) exit 1 } check_root() { echo &quot;Checking root permissions...&quot; if [[ &quot;$(id -u)&quot; == &quot;0&quot; ]]; then error_msg &quot;ERROR! Don&#39;t run this as root.&quot; fi } update_known_hosts() { cp CA/known_hosts $HOME/.ssh/known_hosts } gen_keys() { cp CA/user_ca . chown $USER user_ca chmod 600 user_ca private_suffix=&quot;_ed25519&quot; private_key=$HOME/.ssh/$HOSTNAME$private_suffix public_key=&quot;$private_key.pub&quot; ssh-keygen -f $private_key -t ed25519 -N &quot;&quot; ssh-keygen -s user_ca -I $USER@$HOSTNAME -n admin,ipreston,cornucrapia,pi,ansible $public_key rm user_ca } update_config() { user_config=&quot;$HOME/.ssh/config&quot; cp CA/config $user_config chown $USER $user_config chmod 600 $user_config sed -i -e &quot;s/HOST/$HOSTNAME/g&quot; $user_config } check_root mkdir -p $HOME/.ssh update_known_hosts gen_keys update_config . Conclusion . That’s basically it, whenever I add a new client or host I can connect the USB key, run setup_host.sh as root and then setup_user.sh as any users. All previously configured clients or hosts will automatically have the correct permissions. .",
            "url": "https://ianepreston.github.io/iblog/2020/05/03/ssh.html",
            "relUrl": "/2020/05/03/ssh.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Windows Ds Software",
            "content": "Setting up for data science in python on Windows . There are lots of great guides for setting up an environment to do data science. For my purposes though they generally lack two things: . They’re designed for Mac/Linux and I run Windows at work | They just don’t match my exact personal taste/environment | . This guide is intended to be useful for anyone trying to get set up for data science in python on windows. I think most of the steps are fairly generic, and I’ll make an effort to highlight the parts that are more opinionated. The sections below will go over the core software that will need to be installed, and some handy customizations. . edit 2020-02-17 - I realized I wanted make as well, so I added a section on that. Also made note of an issue with launching interactive python from git bash, as well as solutions. . edit 2020-08-04 - I started using pylance for a python language server, discovered SQL formatter, and switched to the regular Vim plugin. . edit 2020-10-28 - Add instructions for clearing out an old install, do everything as a user level install. I cleaned up some other instructions as I went through as well. . edit 2020-11-02 - Remove reference to pyenv-win. It’s too much of a hassle. Unfortunately my best advice there is to get access to a *NIX environment somehow. . Setting up for data science in python on Windows Install VS Code App installation | Extensions | Finish up later | | Install git and bash Customizing git | Set up ssh SSH note | | Additional utilities | Further reading | | Install Miniconda Actually building environments | | Back to VS code User settings | Further Resources | | Cleaning up an old install Uninstall Apps | Remove miscellaneous crud | Remove config files | | Wrapping up | | Install VS Code . First we need an editor to actually work in. I’m a huge fan of VS code for this. It’s lightweight, extensible, free and open source, and very actively developed. Every month there’s a new release with some fancy thing that makes my life easier. . App installation . Head to the VS code download page and pick the Win64 user installer. . Go through the install process, I think all the defaults are fine, so just keep hitting next. You can optionally add in things like “Add open with code action to Windows Explorer”. It won’t directly impact the rest of what we’re doing, just integrates code with the rest of your desktop a little more fully. . You can sign in with either a GitHub or Microsoft account to enable setting sync. If you use VS code on multiple machines, or just want to easily restore all your settings if you get a new computer I recommend enabling it. See the person icon in the lower left corner. . . On the work machine I’m testing this guide on I’m getting a settings sync error, even though the same install is syncing on my home machine. I love computers. Oh well, it means I’ll get to really walk through a fresh install for this guide. . Extensions . What makes VS code so great is all its extensions. Here are some that are great: . Bracket Pair Colorizer 2 colour codes your brackets. Super helpful for debugging. | Dracula Official is a nice dark theme. The Material themes are also nice. | GitLens integrates a lot of git functionality into VS code, things like showing who made what changes inline in your code. | indent-rainbow colour codes your indentation level, similar to the brackets | markdownlint gives style suggestions when writing markdown files (like this guide!) | Material icon theme makes the icons in the file explorer a little nicer | Python is pretty core for this for obvious reasons | Pylance is a new language server for Visual Studio, it offers nicer autocomplete and seems worth using. | Better TOML handles the config format of python packages | The Remote Development extension pack lets you run VS code on a local machine while developing on a remote system, Docker container, or WSL install. | SQL Formatter will clean up any sql queries you write | SQL Server (mssql) is very handy if you interact with SQL server. The first time you open this it will run a bunch of installers in the background. | Vim - Only install this if you know what Vim is and you want to use its keybindings. If you do this extension will make you very happy, if you don’t it will make you very sad. | Editorconfig - Lets VS Code parse editorconfig files. | . Finish up later . There’s more configuration to do in VS code, but prior to that let’s set up the actual applications we’ll use with it. . Install git and bash . Like any sort of coding work, data science is done best under version control, and git is the defacto standard for that. Bash is not as obviously essential to coding/data science, but it comes with git, and using it for everything rather than the Windows shells will make it easier to apply instructions from other guides in the future, since most of them will assume you’re using bash. Plus I like bash way more than the command prompt or powershell. . git does not require any special admin privileges to install. Go to the Git download page and Choose 64-bit Git for Windows Setup and run the installer. First part is to read (if you want) and accept the license: . . The default path should be fine, but take note of where it’s being installed because we’ll need to point VS code to it later . . On the components selection screen I deselect git gui because I only ever want the terminal. If you like or want to try the gui you could leave that checked. I also check “Check daily for Git for Windows updates” because I don’t want to have to remember to update. . . The next thing you’ll be prompted for is for the default editor. If you’re comfortable with vim/nano/whatever you can change it to that. I’m going to use VS code because it’s the editor I’ll be using for everything else, and I’m going to add vim bindings to it anyway: . . Next we get to pick the default branch name for new repositories. Historically this has been “master” but most organizations are moving away from that. I’ll pick “main”. . . At the next prompt leave it on the default, we want VS code and other tools to know git exists. . . For the SSH executable we’ll use openssh. Later we’ll configure remote development with VS code and also have to use openssh there. If you use and like putty you could swap this out, but note that VS code (at least at time of this writing) doesn’t support putty, so you’ll have to set things up separately there. . . Next up is the https transport backend. If this is a personal machine you can probably just leave it on OpenSSL. If it’s a work computer you should probably switch to “Use the native Windows Secure Channel library”. For example, at work my git repos are hosted on an on prem TFS server, so I definitely want my AD Domain service to validate me. . . At the next prompt we again want the default. Windows and *NIX systems use different symbols to denote line endings. This setting will automatically convert to the Windows format when you pull down changes, but leave them *NIX style when you push them up. This will ensure everyone is getting the correct format of text file when pulling down changes. . . For the default behaviour for git pull I’m fine with fast-forward or merge. If you have a different preference by all means go for it. . . Leave the console on MinTTY, it works nicer than the other. I thought you might need it to be the Windows default console to integrate with VS code but that is not the case. In fact it seems to break console integration with VS code. Go figure. . . Using a credential manager means that once you’ve authenticated yourself for a repository you won’t have to do it every time. Unless you’re super paranoid leave this turned on. . . I just left the extra options on default . . I’m going to try enabling this feature, not being able to open a python console in bash was annoying. . . Customizing git . There are a couple handy things that are useful to customize about git. For one, VS Code is going to make a .vscode folder anywhere we open a folder with it. We’re never going to want to commit that file to version control, so we’ll create a global gitignore file (as opposed to the more standard repository specific ones) and exclude that file. I got this idea from this blog, so credit there. In your %UserProfile% .config git folder, create a file named ignore. Mine is quite basic at this point: . *.vscode . If you find your setup generates other files you’d like to ignore put them here, but don’t use this file for language specific stuff like .ipynb-checkpoints, leave that to project specific .gitignore files. . The other thing that’s nice to do is clean up your default prompt. Notably, git bash by default includes MINGW64 in the prompt. I guess this is the $MSYSTEM$ environment variable, but I can’t imagine why I’d care to see that in my prompt. The other stuff it includes by default are pretty handy, but if you don’t like them you can modify the same file I’m going to point to to update your setup. . The file that contains your prompt information should be in either %UserProfile% AppData Local Programs Git etc profile.d git-prompt.sh. This is a standard bash script, so if you’re familiar with bash scripting, or modifying your bashrc in Linux or Mac this will be familiar. If not, it’s generally pretty readable. Make a backup of it and fiddle. To get rid of the MINGW64 we just have to find the lines that say . PS1=&quot;$PS1&quot;&#39; [ 033[35m ]&#39; # change to purple PS1=&quot;$PS1&quot;&#39;$MSYSTEM &#39; # show MSYSTEM . and delete them or comment them out. I also got rid of . PS1=&quot;$PS1&quot;&#39; n&#39; # new line . so that my conda environment would be on the same line as the rest of my setup info. . Set up ssh . This will actually work exactly the same as Linux, which is nice. GitHub has nice docs on how to generate keys and associate them with your GitHub account, so I won’t reiterate that here. Anything you can connect to via SSH rather than password you should though. It’s more secure, and more convenient. . SSH note . I had an old install of putty when I first set up git bash. Even though I told it to use OpenSSH I guess I still had putty set somewhere in my environment. I had to modify the GIT_SSH environment variable for my system to point to the git ssh utility, which in my case was at C: Program Files Git usr bin ssh. Most people shouldn’t have to do this. . Additional utilities . Git bash has decent functionality out of the box, but there may be additional utilities you want to install. In my particular case, I’d like to be able to use make in my projects. Thanks to this gist I found that it’s pretty easy to do. I’ll reproduce the make install instructions here, but all credit for this part goes to the original author. . Wherever you installed git bash there should be a mingw64 folder. My home machine did a system install, so I found it in C: Program Files Git mingw64, but my work one was a user level install, so that one ended up in %UserProfile% AppData Local Programs Git mingw64. You can always find where it is by right clicking the shortcut to git bash in your start menu and hitting properties, that will show you the path. . Keep in mind you can easy add make, but it doesn’t come packaged with all the standard UNIX build toolchain–so you will have to ensure those are installed and on your PATH, or you will encounter endless error messages. . Go to ezwinports. | Download make-4.1-2-without-guile-w32-bin.zip (get the version without guile). | Extract zip. | Copy the contents to your Git mingw64 merging the folders, but do NOT overwrite/replace any existing files. | . That was all I had to do to make the basic makefiles that I wanted to use. As noted above, if you want to actually build c packages or something your process will likely be more complex. For a great beginner friendly intro to makefiles in the context of python projects, check out calm code. . Further reading . The main git page has tons of resources. I’ve also collected a few that I found useful under my Tagpacker page. . Install Miniconda . Next up we install Miniconda to handle python and all its libraries for data science. You could go with Anaconda over Miniconda for a complete setup out of the box, but I want to do all my actual work in custom environments, so having all that stuff in the base environment is just bloat. Open up the link to the Win64 MiniConda installer, download and run the installer. . We’re avoiding using admin access so you can do this with even a locked down system so leave the option on “Just Me” . . Default install path should be fine. . . Check the box to add Anaconda to the system PATH environment variable, this will allow you to use conda from git bash. I’m unchecking the box that sets conda as the default system python because I want to have a pure python install available through pyenv (more on that later). . . Finally, open up anaconda prompt and run conda init bash. This will allow you to run conda commands from within bash. By default this will also activate the conda base environment whenever you open a terminal. We don’t actually want that, so restart git bash to allow the first command to take effect and then run conda config --set auto_activate_base false. . There’s one last thing we’ll have to do. The conda init bash command added some code that runs whenever you start a git bash terminal in order to allow you to run conda commands. That code is saved in ~/.bash_profile. It will look something like this: . # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by &#39;conda init&#39; !! eval &quot;$(&#39;/c/Users/&lt;your username&gt;/Miniconda3/Scripts/conda.exe&#39; &#39;shell.bash&#39; &#39;hook&#39;)&quot; # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; . What happens there is that whenever you open bash it runs all the commands in .bash_profile first. VS code for some reason does not run this script when you open a bash prompt in the built in terminal. Instead it runs ~/.bashrc because why should anything be easy? Here’s the fix I’m using, thanks to This StackOverflow post. . move .bash_profile to .bashrc with mv ~/.bash_profile ~/.bashrc. Create a new ~/.bash_profile file that just points to ~/.bashrc so they’ll stay in sync whether you’re using regular git bash or VS code git bash. If you want to add anything else to your bash settings later just edit .bashrc. So now in your home directory (%UserProfile%) create a .bash_profile file (either using touch .bash_profile from bash or in VS code, you won’t be able to from Explorer) and put in the following lines: . if [ -f ~/.bashrc ]; then source ~/.bashrc fi . That should take care of it for conda installation . Actually building environments . Conda environment management is a big separate topic. Their documentation is really good, and I refer to it regularly. . Back to VS code . User settings . VS code comes mostly with sensible defaults, but there are a few things I like to change. In VS code hit F1 and type Open settings (JSON). If you don’t see that option (it didn’t pop up for me the first time I tried it) just open settings and look for a setting that tells you to update it in settings.json. Below are my settings, minus the stuff that just got added through configuring the extensions described above: . { &quot;diffEditor.renderSideBySide&quot;: true, &quot;workbench.editor.enablePreviewFromQuickOpen&quot;: false, &quot;workbench.editor.enablePreview&quot;: false, &quot;workbench.colorTheme&quot;: &quot;Dracula&quot;, &quot;workbench.iconTheme&quot;: &quot;material-icon-theme&quot;, &quot;editor.rulers&quot;: [88], &quot;editor.suggestSelection&quot;: &quot;first&quot;, &quot;editor.acceptSuggestionOnEnter&quot;: &quot;off&quot;, &quot;editor.minimap.enabled&quot;: false, &quot;editor.lineNumbers&quot;: &quot;relative&quot;, &quot;explorer.confirmDelete&quot;: false, &quot;editor.wordWrap&quot;: &quot;on&quot;, &quot;python.formatting.provider&quot;: &quot;black&quot;, &quot;python.linting.enabled&quot;: true, &quot;python.linting.flake8Enabled&quot;: true, &quot;python.languageServer&quot;: &quot;Pylance&quot;, &quot;git.confirmSync&quot;: false, &quot;git.autofetch&quot;: true, &quot;editor.codeActionsOnSave&quot;: null, &quot;search.exclude&quot;: { &quot;**/node_modules&quot;: true, &quot;**/bower_components&quot;: true, &quot;**/env&quot;: true, &quot;**/venv&quot;: true }, &quot;files.watcherExclude&quot;: { &quot;**/.ipynb_checkpoints/**&quot;: true, &quot;**/$tf/**&quot;: true, &quot;**/.git/objects/**&quot;: true, &quot;**/.git/subtree-cache/**&quot;: true, &quot;**/node_modules/**&quot;: true, &quot;**/env/**&quot;: true, &quot;**/venv/**&quot;: true, &quot;**/.hypothesis/**&quot;: true, }, &quot;files.exclude&quot;: { &quot;*.sublime-*&quot;: true, &quot;**/__pycache__&quot;: true, &quot;**/.DS_Store&quot;: true, &quot;**/.git&quot;: true, &quot;**/.hypothesis/**&quot;: true, &quot;**/.ipynb_checkpoints&quot;: true, &quot;**/.pytest_cache&quot;: true, &quot;**/.vscode&quot;: true, &quot;**/*.log&quot;: true, &quot;**/*.lst&quot;: true, &quot;**/$tf&quot;: true, &quot;**/node_modules&quot;: true, &quot;venv&quot;: true } } . Most of the settings have fairly clear names, and if you put them in your settings.json you’ll get a little mouseover tip that will tell you what they do. . While you can hand code in the option to have VS code use git bash it’s probably easier to hit F1 again and select “Terminal: Select Default Shell” and choose it there. That will add a line to your settings that tells VS code to use git bash. . Further Resources . There’s tons of stuff to learn about VS code to make it super handy. At a minimum, check out their keyboard shortcuts. I’m collecting other useful resources (with a decent amount of overlap with vim stuff) on my tagpacker. . Cleaning up an old install . I wanted to test drive my instructions before passing them along to a colleague. In order to be sure they’d actually work the way I think they should I needed to clear out my old environment. I’ll document my steps here. Hopefully this will be useful for anyone else looking to start a fresh environment on Windows. . Uninstall Apps . Even if you installed a program as a user level install, you’ll still be prompted for an Administrator password if you try and remove it from “Add/Remove Programs” in the control panel because… Windows. Running the actual uninstall exe file works fine though. . For git this should be located at %UserProfile% AppData Local Programs Git unins000.exe. The uninstaller won’t remove that folder itself so delete it after you’ve run it. . VS code is in a similar location and follows a similar process %UserProfile% AppData Local Programs Microsoft VS Code unins000.exe. . Miniconda as a user level install is in a slightly different place: %UserProfile% Miniconda3 Uninstall-Miniconda3.exe You’ll be prompted for an admin password a couple times, but if you click “no” it still seems to work, which is cool I guess. It will prompt you to reboot and when it comes back up you should be clear. . Remove miscellaneous crud . Depending on how long you’ve been using your machine and how many different installs you’ve done you may or may not have these files. Here’s a list of the things I checked for and then removed from my machine: . Orphan start menu entries in %UserProfile AppData Roaming Microsoft Windows Start Menu Programs . | Renmants in C: ProgramData. In my case I had a folder for jupyter . | . Remove config files . If you like how your configuration is set up there’s no major harm in keeping these things. I’m just removing them to make sure I have a fair comparison to how a fresh install would go. All paths in the list of stuff I’ve removed are relative to %UserProfile%. . .conda | .config/configstore | .continuum | .cookiecutter_replay | .cookiecutters | .ipynb_checkpoints | .ipython | .jupyter | .matplotlib | .poetry | .software | .vscode | AppData Local Black | AppData Local ContinuumIO | AppData Local MikTeX | AppData Local nvim | AppData Local nvim-data | AppData Local pip | AppData Local pypa | AppData Local pypoetry | AppData Local Python Tools | AppData Local Spyder | AppData Local VSCodeVim | AppData Local Yarn | AppData Roaming Code | AppData Roaming jupyter | AppData Roaming pypoetry | AppData Roaming Python | .bash_history | .bash_profile | .flake8 | .gitconfig | .git-credentials | .gitignore_global | .python_history | . As you can see, managing a python development environment creates a lot of crud on your system. . Wrapping up . That’s about it! After following this guide you should have a very comfy software setup in Windows to do data science from. .",
            "url": "https://ianepreston.github.io/iblog/2020/02/15/windows-ds-software.html",
            "relUrl": "/2020/02/15/windows-ds-software.html",
            "date": " • Feb 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ianepreston.github.io/iblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ianepreston.github.io/iblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}